* MySQL 性能调整



* 索引

为表添加索引是提高查询性能的一个非常强大的方法。索引允许 MySQL 快速找到查询所需的数据。如果在表中添加正确的索引，查
询性能可能会提高几个数量级。诀窍在于知道要添加哪些索引。为什么不在所有列上添加索引呢？索引也会产生开销，因此在添加随机
索引之前需要分析自己的需求。

本章首先讨论什么是索引、索引的一些概念以及添加索引可能带来的弊端。然后介绍MySQL支持的各种索引类型和功能。本章的下
一部分开始讨论 InnoDB 如何使用索引，特别是与索引组织表相关的索引。最后，将讨论如何选择应该向表中添加哪些索引以及何时
添加。

** 索引是什么

为了能够使用索引来正确提高性能，了解什么是索引非常重要。本节将不讨论不同的索引类型（将在本章后面的 "索引类型 "一节
中讨论），而是介绍索引的高层次概念。

索引的概念并不新鲜，早在计算机数据库出现之前就已经存在。举个简单的例子，看看这本书。在本书的末尾，有一个索引，收录
了一些单词和术语，这些单词和术语被选为与本书文本最相关的检索词。图书索引的工作方式在概念上与数据库索引的工作方式类
似。它将数据库中的 "术语 "组织起来，这样你的查询就能比读取所有数据并检查其是否符合搜索条件更快地找到相关数据。这里
引用术语一词，是因为索引并不一定是由人类可读的单词组成的。也可以为二进制数据（如空间数据）建立索引。

简而言之，索引可以组织数据，从而缩小查询需要检查的行数。选择得当的索引可以极大地提高速度--几个数量级。再看这本书：
如果你想阅读有关 B 树索引的内容，可以从第 1 页开始阅读全书，或者在本书的索引中查找 "B 树索引 "一词，然后直接跳转
到相关页面。在查询 MySQL 数据库时，也有类似的改进，不同的是，查询可能比在书中查找相关信息要复杂得多，因此索引的重
要性也随之增加。

那么，你只需要添加所有可能的索引，对吗？不。除了添加索引的管理复杂性外，索引本身在正确使用时不仅能提高性能，还会增加
开销。因此，您需要谨慎选择索引。

另一个问题是，即使可以使用索引，它也并不总是比扫描整个表格更有效率。如果你想阅读这本书的大部分内容，那么在索引中查
找每个感兴趣的术语，找出讨论该主题的地方，然后再去阅读，最终会比从头到尾阅读整本书要慢。同样，如果您的查询无论如何
都需要访问表中的大部分数据，那么从头到尾读取整个表就会变得更快。扫描整个表会变得更便宜的阈值到底是多少，这取决于多
个因素。这些因素包括磁盘类型、顺序 I/O 与随机 I/O 相比的性能、数据是否适合内存等。

在深入探讨索引的细节之前，我们不妨先快速了解一些关键的索引概念。

** 索引概念

鉴于索引是个大话题，有几个术语用来描述索引也就不足为奇了。当然，还有一些索引类型的名称，如 B 树、全文、空间等，但
还有一些更笼统的术语需要注意。本章稍后将介绍索引类型，因此这里将讨论更一般的术语。

*** key和index

您可能已经注意到，有时使用 "index"一词，有时使用 "key"一词。这两者有什么区别呢？索引是键的列表。不过，在 MySQL
语句中，这两个词经常可以互换。

在 "主键 "中，"key "就很重要--在这种情况下，必须使用 "key"。另一方面，在添加索引时，可以根据需要编写 ALTER
TABLE table_name ADD INDEX ...或 ALTER TABLE table_name ADD KEY ...。手册在这种情况下使用 "索引"，因
此为了保持一致，建议使用索引。

有几个术语可以描述你正在使用的索引类型。首先要讨论的是唯一索引。

*** 唯一索引

唯一索引是指索引中的每个值只能有一条记录。考虑一个包含人员数据的表。表中可能包含个人的社会保险号或类似标识符。没有
两个人会共享社会保障号，因此在存储社会保障号的列上定义唯一索引是合理的。

从这个意义上说，"unique "更多的是指一种约束，而不是索引功能。不过，索引部分对于 MySQL 快速确定新值是否已经存在至
关重要。

在 MySQL 中使用唯一索引时的一个重要考虑因素是如何处理 NULL 值。比较两个 NULL 值是未定义的（或者换句话说，NULL
不等于 NULL），因此，允许 NULL 值的列上的唯一索引不会对列上 NULL 的行数设置任何限制。如果想限制唯一约束只允许一
个 NULL 值，可以使用触发器检查是否已经存在 NULL 值，并使用 SIGNAL 语句引发错误。触发器示例见清单

#+begin_src sql
  create table my_table(
    Id int unsigned not null,
    Name varchar(50),
    primary  key (Id),
    unique index(Name)
  );
#+end_src

#+begin_src sql
  delimiter $$
  create trigger befins_my_table
  before insert on my_table
    for each row
  begin
    declare v_errmsg, v_value text;
    if exists(select 1 from my_table where Name <=> NEW.Name) then
       if NEW.Name is null then
         set v_value='NULL';
       else
         set v_value=concat('''', NEW.Name, '''');
       end if;
         set v_errmsg=concat('Duplicate entry ',
                             v_value,
                             ' For key ''Name''');
       SIGNAL SQLSTATE '23000'
         set MESSAGE_TEXT=v_errmsg,
             MYSQL_ERRNO=1062;
     end if;
  end $$
  delimiter;
#+end_src

这将处理 Name 列的任何重复值。它使用 NULL 安全等号运算符 (<=>) 来确定 Name 的新值是否已存在于表中。如果存在，
如果值不是 NULL，则会加注引号，否则不会加注引号，因此可以区分字符串 "NULL "和 NULL 值。最后，会发出一个 SQL 状
态为 23000、MySQL 错误编号为 1062 的信号。错误信息、SQL 状态和错误编号与正常的重复键约束错误相同。

主键是一种特殊的唯一索引。

*** 主键

表的主键是唯一定义记录的索引。主键绝不允许出现 NULL 值。如果表中有多个 NOT NULL 唯一索引，其中任何一个都可以作为
主键。在讨论聚类索引时，我们将解释其中的原因，因此应选择一个或多个具有不可变值的列作为主键。也就是说，以永远不更改
给定记录的主键为目标。

主键对于 InnoDB 来说是非常特殊的，而对于其他存储引擎来说，它可能只是一个约定俗成的问题。不过，在所有情况下，最好
总是有一些值可以唯一标识一条记录，例如，这样可以让复制快速确定要修改的记录（第 26 章中有更多相关内容），而且组复制
功能明确要求所有表都必须有一个主键或一个非 NULL 唯一索引。在 MySQL 8.0.13 及更高版本中，可以启用
sql _require _primary _key 选项，要求所有新表必须有主键。如果更改现有表的结构，该限制也同样适用。

#+begin_comment
  启用 sql_require_primary_key 选项（默认已禁用）。没有主键的表可能会导致性能问题，有时会以意想不到的微妙方式
  出现。
#+end_comment

如果有主键，是否还有次键？

*** 二级索引

二级索引 "一词用于指不是主键的索引。它没有任何特殊含义，因此使用这个名称只是为了明确说明，无论该索引是唯一索引还是
非唯一索引，它都不是主键。

如前所述，主键对 InnoDB 有特殊意义，因为它用于聚类索引。

*** 聚类索引

聚类索引是 InnoDB 特有的，是 InnoDB 组织数据的术语。如果你熟悉 Oracle DB，你可能知道索引组织表；这描述的是同样
的事情。

InnoDB 中的一切都是索引。行数据位于 B 树索引的叶页中（B 树索引稍后会介绍）。该索引称为聚类索引。这个名称源于索引
值被聚类在一起。聚类索引使用主键。如果没有指定明确的主键，InnoDB 将查找不允许 NULL 值的唯一索引。如果该索引不存在，
InnoDB 将添加一个隐藏的 6 字节整数列，使用全局（适用于所有 InnoDB 表）自动递增值来生成唯一值。

主键的选择对性能也有影响。本章后面的 "索引策略 "一节将讨论这些问题。聚类索引也可以看作是覆盖索引的一种特例。这是什
么呢？你马上就会知道。

*** 覆盖索引

如果一个索引包含了特定查询所需的索引表中的所有列，那么这个索引就是覆盖索引。也就是说，索引是否覆盖取决于使用索引进
行的查询。一个索引可能对一个查询是覆盖索引，但对另一个查询却不是。考虑一个索引（a, b）和一个选择这两列的查询：

#+begin_src sql
  select a, b
    from my_table
   where a=10;
#+end_src

在这种情况下，查询只需要 a 列和 b 列，因此不需要查找行的其他部分--索引足以检索到所有需要的数据。另一方面，如果查
询还需要列 c，索引就不再起作用了。当使用 EXPLAIN 语句分析查询时（第 20 章将介绍），如果表使用了覆盖索引，那么
EXPLAIN 输出中的 Extra 列将包含 "Using Index"。

InnoDB 的聚类索引是覆盖索引的一个特例（尽管 EXPLAIN 不会显示 "Using Index"）。聚类索引包括叶节点中的所有行数
据（尽管一般情况下只有列的子集被实际索引），因此索引将始终包括所有需要的数据。某些数据库在创建索引时支持 include
子句，可用于模拟聚类索引的工作方式。

巧妙地创建索引，将其用作执行次数最多的查询的覆盖索引，可以大大提高性能，"索引策略 "部分将对此进行讨论。

添加索引时，需要遵守一些限制。接下来要介绍的就是这些限制。

** 索引限制

InnoDB 索引有一些限制。这些限制从索引大小到表允许的索引数量不等。最重要的限制如下：

+ B 树索引的最大宽度为 3072 字节或 767 字节，具体取决于 InnoDB 行格式。最大大小基于 16 kiB InnoDB 页面，对
  于较小的页面大小，限制会更低。

+ 只有在指定了前缀长度的情况下，Blob 和文本类型列才能在索引（全文本索引除外）中使用。本章稍后将在 "索引功能 "一节
  中讨论前缀索引。

+ 功能键部分计入表格 1017 列的限制内。

+ 每个表最多可以有 64 个二级索引。

+ 多列索引最多可包括 16 列和功能键部分。


您可能会遇到的限制是 B 树索引的最大索引宽度。使用 DYNAMIC（默认）或 COMPRESSED 行格式时，索引宽度不能超过 3072
字节；使用 REDUNDANT 和 COMPACT 行格式时，索引宽度不能超过 767 字节。使用 DYNAMIC 和 COMPRESSED 行格式的表，
8 KiB 页面的限制减少到一半（1536 字节），4 KiB 页面的限制减少到四分之一（768 字节）。这对字符串和二进制列索引的
限制尤为明显，因为这些值不仅在本质上通常较大，而且在计算大小时使用的也是可能需要的最大存储量。这意味着，使用
utf8mb4 字符集的 varchar(10) 会导致 40 字节的限制，即使您从未在列中以单字节字符存储任何内容。

在为文本或 blob 类型的列添加 B 树索引时，必须始终提供一个键长度，指定要在索引中包含多少列的前缀。这甚至适用于只支
持 256 字节数据的 tinytext 和 tinyblob。对于 char、varchar、二进制和 varbinary 列，只有在值的最大字节数超
过表允许的最大索引宽度时，才需要指定前缀长度。

#+begin_comment
  对于文本和 blob 类型的列，与其使用前缀索引，不如使用全文索引（稍后详述），或者添加一个包含 blob 哈希值的生成
  列，或者以其他方式优化访问。
#+end_comment

如果为表添加功能索引，则每个功能键部分都会计入表的列限制。如果创建的索引有两个功能部分，那么这两个功能部分将作为两
列计算到表的限制中。对于 InnoDB，一个表最多只能有 1017 列。

最后两个限制与表中可包含的索引数量以及单个索引中可包含的列和功能键部分的数量有关。一个表最多可以有 64 个二级索引。
在实际操作中，如果您已经接近这个限制，那么您可能需要重新考虑您的索引策略。本章后面的 "索引的缺点是什么？"中将讨论
索引的开销，因此在任何情况下，最好将索引的数量限制在真正有利于查询的范围内。同样，向索引添加的部分越多，索引的规模
就越大。InnoDB 的限制是最多只能添加 16 个部分。

如果需要为表添加索引或删除多余的索引，该怎么办？索引可以与表一起创建，也可以稍后创建，还可以删除索引，这将在下文中
讨论。

*** SQL语法

在首次创建模式时，您一般会有一些添加索引的想法。随着时间的推移，您的监控可能会发现某些索引已不再需要，而应添加其他
索引。这些对索引的更改可能是由于对所需索引的误解；数据可能已经更改，或者查询可能已经更改。

在更改表的索引时，有三种不同的操作：在创建表时创建索引、为现有表添加索引或从表中删除索引。无论是与表一起添加索引，
还是作为后续操作添加索引，索引定义都是一样的。在删除索引时，只需要索引名称。

本节将介绍添加和删除索引的一般语法。在本章的其余部分，将根据特定的索引类型和功能进一步举例说明。

*** 创建表时创建索引

创建表格时，可以在 CREATE TABLE 语句中添加索引定义。索引定义在列之后。您可以选择指定索引的名称；如果不指定，索
引将以索引中的第一列命名。

清单 14-2 显示了一个创建了多个索引的表的示例。如果不知道所有索引类型的作用，也不必担心，本章稍后将讨论这个问题。

#+begin_src sql
  create table db1.person(
    Id int unsigned not null,
    Nmae varchar(50),
    Birthdate date not null,
    Location point not null srid 4326,
    Description text,
    primary key(Id),
    index (Nmae),
    spatial index (Location),
    fulltext index(Description)
  );
#+end_src

这样就在 db1 模式（必须事先存在）中创建了带有四个索引的表 person。第一个是主键，它是一个 B 树索引（稍后将详细介
绍），指向 Id 列。第二个也是 B 树索引，但它是所谓的二级索引，索引 Name 列。第三个索引是位置列上的空间索引。第四个
索引是描述列上的全文索引。

您还可以创建一个包含多列的索引。如果需要在不止一列上设置条件，在第一列上设置条件并按第二列排序，等等，这将非常有用。
要创建多列索引，请用逗号分隔的列表指定列名：

#+begin_src sql
  INDEX(Name, Birthdate)
#+end_src

列的顺序非常重要，这将在 "索引策略 "中解释。简而言之，MySQL 只能使用从左边开始的索引，也就是说，只有同时使用 Name
时，才能使用索引的 Birthdate 部分。这意味着索引（Nmae，Birthdate）与（Birthdate，Name）不是同一个索引。

一般来说，表上的索引不会一成不变，那么如果要为现有表添加索引，该怎么办呢？


*** 新增索引

如果确定需要，可以为现有表添加索引。为此，需要使用 ALTER TABLE 或 CREATE INDEX 语句。由于 ALTER TABLE 语句
可用于表的所有修改，因此您可能希望坚持使用该语句；不过，无论使用哪种语句，所做的工作都是一样的。

清单 14-3 显示了两个如何使用 ALTER TABLE 创建索引的示例。第一个示例添加了一个索引；第二个示例在一条语句中添加了
两个索引。

#+begin_src sql
  alter table db1.person
    add index (Birthdate);

  alter table db1.person
    drop index Birthdate;

  alter table db1.person
    add index (Nmae, Birthdate),
    add index (Birthdate);
#+end_src

第一条和最后一条 ALTER TABLE 语句使用 ADD INDEX 子句告诉 MySQL 应该向表中添加索引。第三条语句添加了两个这样
的子句，中间用逗号隔开，以便在一条语句中添加两个索引。在这两条语句之间，索引会被删除，因为拥有重复索引是不好的做法，
MySQL 也会对此提出警告。

用两条语句添加两个索引还是用一条语句添加两个索引有区别吗？是的，可能会有很大区别。添加索引时，有必要执行一次全表扫
描，以读取索引所需的所有值。对于大型表来说，全表扫描是一项昂贵的操作，因此从这个意义上说，最好在一条语句中同时添加
两个索引。另一方面，只要索引能完全保存在 InnoDB 缓冲池中，创建索引的速度就会快很多。将两个索引的创建分成两条语句，
可以减少缓冲池的压力，从而提高索引创建性能。

最后一项操作是删除不再需要的索引。

*** 删除索引

删除索引的操作与添加索引类似。可以使用 ALTER TABLE 或 DROP INDEX 语句。使用 ALTER 表时，可以将删除索引与表的
其他数据定义操作结合起来。

#+begin_src sql
  show create table db1.person\G
#+end_src

#+begin_src sql
  select INDEX_NAME, INDEX_TYPE,
    GROUP_CONCAT(COLUMN_NAME ORDER BY SEQ_IN_INDEX) as Columns
  from information_schema.STATISTICS
  where TABLE_SCHEMA='db1'
    and TABLE_NAME='person'
  group by INDEX_NAME, INDEX_TYPE;
#+end_src

在您的情况下，索引可能以不同的顺序列出。第一个查询使用 SHOW CREATE TABLE 语句获取完整的表定义，其中也包括索引及
其名称。第二个查询是查询 information_schema。STATISTICS 视图。该视图对于获取索引信息非常有用，将在下一章详细
讨论。一旦决定要删除哪个索引，就可以使用清单 14-5 所示的 ALTER TABLE。

#+begin_src sql
  alter table db1.person drop index name_2;
#+end_src

这将删除名为 name_2 的索引，即（Name、Birthdate）列上的索引。

本章其余部分将详细介绍什么是索引，在本章末尾，"索引策略 "一节将讨论如何选择要索引的数据。首先，必须了解索引为什么会
产生开销。

** 指数有哪些缺点？

生活中很少有免费的东西，索引也不例外。虽然索引对提高查询性能很有帮助，但它们也需要存储并保持更新。此外，在执行查询
时，索引越多，优化器需要做的工作就越多，这是一个不太明显的开销。本节将介绍索引的这三个缺点。

*** 存储

添加索引最明显的代价之一是需要存储索引，以便在需要时随时可用。 存储开销有两方面：索引需要存储在磁盘上以保持其持久性，
同时还需要 InnoDB 缓冲池中的内存以供查询使用。

磁盘存储意味着你可能需要在系统中添加磁盘或块存储。 如果使用 MySQL Enterprise Backup (MEB) 等复制原始表空间文
件的备份解决方案，备份也会变大，完成时间也会变长。

InnoDB 总是使用缓冲池来读取查询所需的数据。 如果缓冲池中还不存在数据，则会先将其读入缓冲池，然后用于查询。因此，
使用索引时，索引和行数据一般都会被读入缓冲池（使用覆盖索引时例外）。需要放入缓冲池的数据越多，其他索引和数据的空间
就越小--除非你把缓冲池的容量增大。当然，情况要比这复杂得多，因为避免全表扫描还能防止将整个表读入缓冲池，从而减轻缓
冲池的压力。总体收益与开销之间的关系取决于使用索引可以避免检查多少表，以及其他查询是否会读取索引避免访问的数据。

总而言之，在添加索引时，你将需要额外的磁盘，而且一般来说，你将需要更大的 InnoDB 缓冲池来保持相同的缓冲池命中率。
另一个开销是，索引只有在保持更新时才有用。这就增加了更新数据时的工作量。

*** 更新索引

无论何时对数据进行更改，都必须更新索引。 这包括在插入或删除数据时添加或删除行链接，以及在更新值时修改索引。你可能
不会太在意这些，但这可能会造成很大的开销。事实上，在恢复逻辑备份（通常包括用于创建数据的 SQL 语句的文件，例如使用
mysqlpump 程序创建的文件）等批量数据加载过程中，保持索引更新的开销往往会限制插入率。

#+begin_comment
  更新索引的开销可能会非常大，因此一般建议在向空表进行大型导入时删除二级索引，然后在导入完成后重新创建索引。
#+end_comment

对于 InnoDB 来说，开销还取决于二级索引是否适合缓冲池。只要整个索引都在缓冲池中，那么更新索引的成本就相对较低，而且
也不太可能成为严重的瓶颈。如果索引无法容纳，InnoDB 就不得不在表空间文件和缓冲池之间不停地洗页，这时开销就会成为主
要瓶颈，导致严重的性能问题。

还有一个不太明显的性能开销。索引越多，优化器确定最佳查询计划的工作量就越大。

*** 优化器

当优化器分析查询以确定它认为的最优查询执行计划时，它需要评估每个表上的索引，以确定是否应该使用索引，以及是否对两
个索引进行索引合并。我们的目标当然是尽可能快地评估查询。但是，在优化器中花费的时间通常是不可忽略的，在某些情况下
甚至会成为瓶颈。

请看一个非常简单的查询示例，从一个表中选择一些行：

#+begin_src sql
  select ID, Nmae District, Population
    from world.city
  where CountryCode='AUS';
#+end_src

在这种情况下，如果表 city 上没有索引，显然需要进行表扫描。如果有一个索引，还需要评估使用索引的查询成本，等等。
如果您有一个复杂的查询，涉及许多表，每个表都有十几个可能的索引，那么就会产生许多组合，这将反映在查询的执行时间上。

#+begin_comment
  如果在优化器中花费的时间成为问题，可以添加第 17 章和第 24 章中讨论的优化器和连接顺序提示来帮助优化器，这样它
  就不需要评估所有可能的查询计划。
#+end_comment

虽然这些描述添加索引的开销的页面会让人觉得索引不好，但不要回避索引。对于频繁执行的查询，选择性强的索引会带来很大
的好处。但是，不要为了添加索引而添加索引。在本章末尾的 "索引策略 "一节中，我们将讨论选择索引的一些思路，本书的其
他部分也会有讨论索引的示例。在此之前，值得讨论一下 MySQL 支持的各种索引类型以及其他索引功能。

** 索引类型

索引的最佳类型并非适用于所有用途。为查找给定值范围（例如 2019 年的所有日期）内的行而优化的索引，与为给定单词或短
语搜索大量文本的索引，需要有很大的不同。这意味着在选择添加索引时，必须决定需要哪种索引类型。MySQL 目前支持五种不
同的索引类型：

+ B树索引
+ 全文索引
+ 地理索引(R树索引)
+ 多值索引
+ hash索引

本节将介绍这五种索引类型，并讨论它们可用于加速哪些类型的问题。

*** B树索引

到目前为止，B 树索引是 MySQL 中最常用的索引类型。事实上，所有 InnoDB 表都包含至少一个 B 树索引，因为数据是在
B 树索引（聚类索引）中组织的。

B 树索引是一种有序索引，因此它善于查找以下情况的记录：正在查找等于某个值的列，大于或小于给定值的列，或者介于两个
值之间的列。这使它成为许多查询中非常有用的索引。

B 树索引的另一个优点是性能可预测。顾名思义，索引就像一棵树，从根页面开始，到叶子页面结束。InnoDB 使用 B 树索引
的扩展，称为 B+ 树。+"的意思是同级的节点是链接的，因此在扫描索引时很方便，无需在到达某个节点的最后一条记录时再返
回到父节点。

#+begin_comment
  在 MysQL 中，B-树和 B+ 树这两个术语可以互换使用。
#+end_comment

城市名称索引的索引树示例见图 14-1。(图中的索引级别从左到右排列，与其他一些 B 树索引图示从上到下的排列方式不同
）。这样做主要是为了节省空间）。

[[./images/1BmL8f.png]]

在图中，文档形状代表一个 InnoDB 页面，多个文档堆叠在一起的形状（如第 0 层中标注为 "Christchurch "的文档）代
表多个页面。从左到右的箭头从根页面指向叶页面。根页面是索引搜索开始的地方，而叶页面则是索引记录存在的地方。介于两
者之间的页面通常称为内部页面或分支页面。页面也可称为节点。连接同级页面的双箭头是 B 树和 B+ 树索引的区别所在，它
允许 InnoDB 快速移动到上一个或下一个同级页面，而无需通过父页面。

对于小型索引，可能只有一个页面既是根页面又是叶页面。在更一般的情况下，索引有一个根页面，如图的最左边所示。图中最
右边的部分是叶页。对于大型索引，中间可能还有更多级。叶节点为 0 级，其父页为 1 级，依此类推，直到根页。

在图中，页面上标注的值，例如 "A Coruña"，表示树的该部分所涵盖的第一个值。因此，如果您在第 1 层查找值
"Adelaide"，您就会知道它在叶页的最顶层，因为该页包含了从 "A Coruña "开始到最后一个值 "Beijing "之前的所有
值。这就是上一章讨论的整理方法发挥作用的一个例子。

一个主要特点是，无论你遍历哪个分支，层级的数量总是相同的。例如，在图中，这意味着无论查找哪个值，都将读取四个页面，
四个层级各一个（如果有几行具有相同的值，对于范围扫描，可能会读取叶子层级的更多页面）。因此，可以说这棵树是平衡的。
正是这一特性带来了可预测的性能，而且层级数量扩展良好，也就是说，层级数量会随着索引记录数量的增加而缓慢增长。当需
要从磁盘等相对较慢的存储设备中访问数据时，这一特性尤为重要。

#+begin_comment
  您可能也听说过 T树索引。B 树索引针对磁盘访问进行了优化，而 T树索引与 B 树索引类似，只是针对内存访问进行了优
  化。因此，将所有索引数据存储在内存中的 NDBCluster 存储引擎使用 T 树索引，即使它们在 SQL 级别被称为 B 树索引。
#+end_comment

本节开头提到，B 树索引是迄今为止 MySQL 中最常用的索引类型。事实上，如果你有任何 InnoDB 表，即使你从未自己添加
过任何索引，你也在使用 B 树索引。InnoDB 使用聚类索引组织存储数据，这实际上意味着行存储在 B+ 树索引中。B树索引
也不只用于关系数据库，例如，一些文件系统就以B树结构组织元数据。

需要注意的是，B 树索引的一个特性是只能用于比较索引列的整个值或左前缀。这意味着，如果要检查索引日期的月份是否为
5 月，则不能使用索引。如果要检查索引字符串是否包含给定的短语，也是同样的道理。

在索引中包含多列时，同样的原则也适用。考虑索引（Name, Birthdate）：在这种情况下，您可以使用该索引搜索给定的姓
名或姓名和生日的组合。但是，在不知道姓名的情况下，您不能使用该索引搜索具有给定生日的人。

有几种方法可以处理这种限制。在某些情况下，可以使用功能索引，或者将列的信息提取到可以编制索引的生成列中。在其他情
况下，可以使用另一种索引类型。例如，全文索引可用于搜索字符串中包含 "查询性能调整 "短语的列。

*** 全文索引

全文索引专门用于回答 "哪个文档包含这个字符串？"这样的问题。也就是说，全文索引在查找列与字符串完全匹配的行时并没有
进行优化，为此，B 树索引是更好的选择。

全文索引的工作原理是对被索引的文本进行标记化。具体方法取决于所使用的解析器。InnoDB 支持使用自定义解析器，但通常使
用内置解析器。默认解析器假定文本使用空白作为分隔符。MySQL 包含两个可选的解析器：支持中文、日文和韩文的 ngram 解
析器和支持日文的 MeCab 解析器。

InnoDB 使用名为 FTS_DOC_ID 的特殊列将全文索引链接到记录，该列是一个 bigint 无符号 NOT NULL 列。如果添加了全
文索引，而该列还不存在，InnoDB 会将其添加为隐藏列。添加隐藏列需要重建表，因此如果要为大型表添加全文索引，需要考虑
到这一点。如果知道要在表中使用全文索引，可以事先自己添加列，并为列添加唯一索引 FTS_DOC_ID_INDEX。您也可以选择使
用 FTS_DOC_ID 列作为主键，但要注意 FTS_DOC_ID 值不允许重复使用。自己准备表的示例如下：

#+begin_src sql
  drop table if exists db1.person;

  create table db1.person(
    FTS_DOC_ID bigint unsigned not null auto_increment,
    Nmae varchar(50),
    Description text,
    primary key (FTS_DOC_ID),
    fulltext index(Description)
  );
#+end_src

如果没有 FTS_DOC_ID 列，并且在现有表中添加了一个全文本列，MySQL 将返回一个警告，告知该表已被重建以添加该列：

Warning (code 124): InnoDB rebuilding table to add column FTS_DOC_ID

如果计划使用全文索引，建议从性能角度明确添加 FTS_DOC_ID 列，并将其设置为表的主键或为其创建辅助唯一索引。自己创建
列的缺点是必须自己管理值。

另一种专门的索引类型是空间数据索引。全文索引适用于文本文档（或字符串），而空间索引则适用于空间数据类型。

*** 空间索引（R 树）

从历史上看，空间特性在 MySQL 中的使用并不多。不过，随着 InnoDB 在 5.7 版中支持空间索引，以及 MySQL 8 中支持为
空间数据指定空间参考系统标识符 (SRID) 等其他改进，您有可能在某些时候需要空间索引。

空间索引的一个典型用例是包含兴趣点的表格，每个兴趣点的位置与其他信息一起存储。例如，用户可能会要求获取其当前位置 50
公里范围内的所有电动汽车充电站。要尽可能高效地回答这样的问题，就需要空间索引。

MySQL 以 R 树的形式实现空间索引。R 代表矩形，暗示了索引的用途。R 树索引对数据进行组织，使空间上相近的点在索引中
存储得很近。这样就能有效确定空间值是否满足某些边界条件（如矩形）。

只有在列声明为 NOT NULL 且空间参照系统标识符已设置的情况下，才能使用空间索引。空间条件是通过 MBRContains() 等
函数之一指定的，该函数接收两个空间值，并返回第一个值是否包含另一个值。除此之外，使用空间索引没有特殊要求。清单 14-6
显示了一个带有空间索引的表和一个可以使用该索引的查询的示例。

#+begin_src sql
  create table db1.city(
     id int unsigned not null,
     Nmae varchar(50) not null,
     Location point SRID 4326 not null,
     primary key (id),
     spatial index(Location));

  insert into db1.cty
  values (1, 'Sydney',
          ST_GeomFromText('Point(-33.8650 151.2094)', 4326));

  set @boundary=ST_GeomFromText('Polygon((-9 112, -45 112, -45 160,
      -9 160, -9 113))', 4326);

  select id, Name
    from db1.city
  where MBRContains(@boundary, Location);
#+end_src

在示例中，一个包含城市位置的表在位置列上有一个空间索引。空间参照系统标识符 (SRID) 设置为 4326，以表示地球。在这
个示例中，插入了一条记录，并定义了一个边界（如果你很好奇，那么边界包含澳大利亚）。您也可以在 MBRContains() 函数
中直接指定多边形，但这里分两步进行，以便查询的各个部分更加清晰。

因此，空间索引有助于回答某个几何形状是否在某个边界内。同样，多值索引也能帮助回答给定值是否在值列表中。

*** 多值索引

MySQL 在 MySQL 5.7 中引入了对 JSON 数据类型的支持，并在 MySQL 8 中通过 MySQL 文档存储扩展了该功能。您可以使
用生成列上的索引或功能索引来创建 JSON 文档上的索引；但是，迄今为止讨论的索引类型未涵盖的一种用例是搜索 JSON 数组
包含某些值的文档。例如，一个城市集合，每个城市都有一个郊区数组。上一章中的 JSON 文档示例就是这样：

#+begin_src js
  {
      "name":
      "Sydney",
      "demographics": { "population": 5500000 },
      "geography": { "country": "Australia", "state": "NSW" },
      "suburbs": [ "The Rocks", "Surry Hills", "Paramatta" ]
  }
#+end_src

如果您想搜索城市集合中的所有城市，并返回那些拥有名为 "Surry Hills "的郊区的城市，那么您需要一个多值索引。
MySQL 8.0.17 增加了对多值索引的支持。

解释多值索引如何有用的最简单方法是看一个示例。清单 14-7 从 world_x 示例数据库中提取了 countryinfo 表，将其复
制到 mvalue_index 表中，然后对其进行修改，使每个 JSON 文档都包含一个城市数组，其中包含城市人口和所在地区。最后，
还包含一个查询，以显示检索澳大利亚所有城市名称（_id = 'AUS'）的示例。本书 GitHub 代码库中的 listing_14_7.sql
文件也提供了这些查询，可以使用 \source listing_14_7.sql 命令在 MySQL Shell 中执行。

#+begin_src sql
  \use world_x

  drop table if exists mvalue_index;

  create table mvalue_index like countryinfo;

  insert into mvalue_index(doc)
    select doc
      from countryinfo;

  update mvalue_index
    set doc=JSON_INSERT(
    doc,
    '$.cities',
    (select JSON_ARRAYAGG(
             JSON_OBJECT(
             'district', district,
             'name', name,
             'population',
                  Info->'$.Population'
           )
          )
         from city
        where CountryCode=mvalue_index.doc->>'$.Code'
       )
  );
#+end_src

#+begin_src sql
  select JSON_PRETTY(doc->>'$.cities[*].name')
    from mvalue_index
    where doc->>'$.Code'='AUS'\G
#+end_src

列表首先将 world_x 模式设为默认模式，然后删除存在的 mvalue_index 表，并使用与 countryinfo 表相同的定义和相同
的数据再次创建该表。您也可以直接修改 countryinfo 表，但通过处理 mvalue_index 副本，您可以通过删除
mvalue_index 表轻松重置 world_x 模式。该表由名为 doc 的 JSON 文档列和名为 _id 的生成列（主键）组成：

#+begin_src sql
  show create table mvalue_index\G
#+end_src

UPDATE 语句使用 JSON_ARRAYAGG() 函数为每个国家创建一个包含三个 JSON 对象（地区、名称和人口）的 JSON 数组。
最后，执行 SELECT 语句返回澳大利亚城市的名称。

现在能为城市名新增多值索引

#+begin_src sql
  alter table mvalue_index
    add index (((cast(doc->>'$.cities[*].name' as char(35) array))));
#+end_src

该索引从文档根部的城市数组的所有元素中提取名称对象。生成的数据被转换为 char(35) 值数组。之所以选择这种数据类型，
是因为城市名称所在的城市表是 char(35)。在 CAST() 函数中，char 和 varchar 数据类型都使用 char。

新索引可以使用 MEMBER OF 操作符和 JSON_CONTAINS() 与 JSON_OVERLAPS() 函数用于 WHERE 子句。MEMBER OF 操
作符询问给定值是否是数组的成员。JSON_CONTAINS() 与此非常相似，但与 MEMBER OF 的引用搜索相比，它需要进行范围搜
索。JSON_OVERLAPS() 可用于查找包含多个值中至少一个值的文档。清单 14-8 展示了使用操作符和每个函数的示例。

#+begin_src sql
  select doc->>'$.Code' as Code, doc->>'$.Nmae'
  from mvalue_index
  where 'Sydeney' member of (doc->'$.cities[*].name');

  select doc->>'$.Code' as Code, doc->>'$.Name'
  from mvalue_index
  where JSON_CONTAINS(
    doc->'$.cities[*].name',
    '"Sydeney"'
  );

  select doc->>'$.Code' as Code, doc->>'$.Name'
  from mvalue_index
  where JSON_OVERLAPS(
    doc->'$.cities[*].name',
    '["Sydeney", "New York"']'
  );
#+end_src

使用 MEMBER OF 和 JSON_CONTAINS() 的两个查询都是查找有一个名为悉尼的城市的国家。最后一个查询使用
JSON_OVERLAPS()，查找城市名为悉尼或纽约或两者都有的国家。

MySQL 还剩下一种索引类型：散列索引。

*** hash索引

如果要搜索某列与某个值完全相等的记录，可以使用本章前面讨论过的 B 树索引。不过还有另一种方法：为每一列的值创建一个
哈希值，然后使用哈希值搜索匹配的记录。为什么要这样做呢？答案是，这是一种查找记录的快速方法。

散列索引在 MySQL 中使用不多。一个值得注意的例外是 NDBCluster 存储引擎，它使用散列索引来确保主键和唯一索引的唯一
性，还使用散列索引来提供使用这些索引的快速查找。就 InnoDB 而言，它并不直接支持散列索引；不过，InnoDB 有一种称为
自适应散列索引的功能，值得我们多加考虑。

自适应哈希索引功能在 InnoDB 中自动运行。如果 InnoDB 检测到你正在频繁使用二级索引，并且自适应散列索引已启用，它
就会根据最频繁使用的值建立一个散列索引。哈希索引只存储在缓冲池中，因此在重启 MySQL 时不会持久化。如果 InnoDB 发
现内存可以更好地用于向缓冲池加载更多页面，它就会丢弃部分哈希索引。这就是所谓自适应索引的意思： InnoDB 会尝试调整
它，使其最适合你的查询。你可以使用 innodb_adaptive_hash_index 选项启用或禁用该功能。

从理论上讲，自适应散列索引是一种双赢方案。你无需考虑需要为哪些列添加哈希索引，就能获得哈希索引的优势，而且内存使用
情况也会自动得到处理。不过，启用哈希索引也会产生开销，而且并非所有工作负载都能从中受益。事实上，对于某些工作负载来
说，这种开销可能会变得非常大，以至于出现严重的性能问题。

监控自适应散列索引有两种方法：信息模式中的 INNODB_METRICS 表和 InnoDB 监控器。INNODB_METRICS 表包含自适应散
列索引的八个指标，其中两个指标默认已启用。清单 14-9 显示了 INNODB_METRICS 中包含的八个指标。

#+begin_src sql
  select NAME, COUNT, STATUS, COMMENT
  from information_schema.INNODB_METRICS
  where SUBSYSTEM='adaptive_hash_index'\G
#+end_src

使用自适应哈希索引的成功搜索次数（adaptive_hash_searches）和使用 B 树索引完成的搜索次数
（adaptive_hash_searches_btree）默认是启用的。你可以使用这些指标来确定 InnoDB 使用哈希索引解决查询的频率，
而不是底层 B 树索引。其他指标不太常用，因此默认禁用。不过，如果你想更详细地了解自适应哈希索引的用处，可以放心地启
用这六个指标。

监控自适应哈希索引的另一种方法是使用 InnoDB 监控，如清单 14-10 所示。输出中的数据会有所不同。

#+begin_src sql
  show engine inoodb status\G
#+end_src

首先要检查的是 Semaphores 部分。如果自适应散列索引是竞争的主要来源，那么 btr0sea.ic 文件（源代码中实现自适应散
列索引的地方）周围就会出现 Semaphores。如果偶尔（但很少）会出现 Semaphores，这不一定是个问题，但如果频繁出现长
时间的 Semaphores，则最好禁用自适应散列索引。

另一个值得关注的部分是插入缓冲区和自适应哈希索引部分。其中包括散列索引使用的内存量，以及使用散列和非散列搜索回答查
询的速率。请注意，这些速率针对的是监视器输出顶部附近列出的时间段--在示例中，针对的是 2019-05-05 17:22:14 之前
的最后 16 秒。

关于支持的索引类型的讨论到此结束。关于索引，还有更多的内容，因为有几项功能值得你去熟悉。

** 索引特性

知道存在哪些类型的索引是一回事，但能够充分利用这些索引则是另一回事。为此，你需要更多地了解 MySQL 中与索引相关的
功能。这些功能包括按相反顺序对索引中的值进行排序、功能索引和自动生成索引。本节将介绍这些功能，以便您在日常工作中使
用它们。

*** 函数索引

到目前为止，索引已直接应用于列。这是最常见的添加索引方式，但也有需要使用派生值的情况。例如，查询所有生日在 5 月份
的人：

#+begin_src sql
  drop table if exists db1.person;

  create table db1.person(
    Id int unsigned not null,
    Name varchar(50),
    Birthdate date not null,
    primary key (id)
  );

  select *
    from db1.person
    where month(Birthdate) = 5;
#+end_src

如果在 "出生日期 "列上添加索引，则不能用于回答该查询，因为日期是根据其完整值存储的，而不是根据列的最左部分进行匹配
（另一方面，搜索所有 1970 年出生的人可以使用 "出生日期 "列上的 B 树索引）。(另一方面，要搜索所有 1970 年出生的
人，可以在 Birthdate 列上使用 B 树索引）。

其中一种方法是用派生值生成一列。例如，在 MySQL 5.7 及更高版本中，您可以告诉 MySQL 自动更新列：

#+begin_src sql
  create table db1.person(
    Id int unsigned not null,
    Name varchar(50) not null,
    Birthdate date not null,
    BirthMonth tinyint unsigned
              generated always as (month(Birthdate))
              virtual not null,
    primary key(Id),
    index(BirthMonth)
  );
#+end_src

在 MySQL 8.0.13 中，有一种更直接的方法可以实现这一目标。您可以直接索引函数的结果：

#+begin_src sql
  create table db1.person(
    Id int unsigned not null,
    Name varchar(50) not null,
    Birthdate date not null,
    primary key (Id),
    index((month(Birthdate)))
  );
#+end_src

使用功能索引的好处是可以更明确地显示要索引的内容，而且不会多出 BirthMonth 列。除此之外，添加函数索引的两种方法工
作原理相同。

*** 前缀索引

表的索引部分大于表数据本身的情况并不少见。如果你索引的是大字符串值，情况尤其如此。B 树索引的索引数据最大长度也有限
制--使用动态或压缩行格式的 InnoDB 表为 3072 字节，其他表则更小。这实际上意味着不能为文本列建立索引，更不用说长
文本列了。减少大型字符串索引的一种方法是只索引值的第一部分。这就是所谓的前缀索引。

创建前缀索引时，需要指定要索引的字符串的字符数或二进制对象的字节数。如果要为城市表（来自世界数据库）中 Name 列的前
十个字符创建索引，可以这样做

#+begin_src sql
  alter table world.city add index(Name(10));
#+end_src

请注意括号中添加的索引字符数。只要选择足够多的字符来提供良好的选择性，该索引的效果几乎与索引整个名称一样好，而且它
使用的存储空间和内存也更少。需要包含多少字符？这完全取决于您要索引的数据。您可以通过查询数据来了解前缀的唯一性。清
单 14-11 举例说明了有多少城市名称共享前十个字符。

#+begin_src sql
  select left(Name, 10), count(*),
    count(distinct Name) as 'Distinct'
  from world.city
  group by left(Name, 10)
  order by count(*) desc, left(Name, 10)
  limit 10;
#+end_src

这表明，使用这个索引前缀，最多只能读取 6 个城市来查找匹配。虽然这比完全匹配要多，但仍比扫描所有表格要好得多。当然，
在这种比较中，您还需要验证前缀匹配的数量是由于前缀碰撞造成的，还是由于城市名称相同造成的。例如，对于 "剑桥 "来说，
有三个城市都叫这个名字，所以索引前十个字符还是整个名字并没有什么区别。您可以针对不同的前缀长度进行此类分析，以了解
增加索引大小会带来微小回报的临界值。在很多情况下，索引并不需要那么多字符就能很好地工作。

如果您认为可以删除索引，或者您想推出一个索引，但又不想让它立即生效，该怎么办呢？
答案就是隐形索引。

*** 隐形索引

MySQL 8 引入了一项名为 "隐形索引 "的新功能。它允许你拥有一个已维护并可随时使用的索引，但优化器会忽略该索引，直到
你决定让它可见。这样，您就可以在复制拓扑中推出新索引，或禁用您认为不需要或类似的索引。您可以快速启用或禁用索引，因
为这只需要更新表的元数据，所以变化是 "即时 "的。

例如，如果您认为不需要某个索引，那么在告诉 MySQL 删除索引之前，先将其隐藏起来，这样就可以监控数据库在没有索引的情
况下是如何运行的。如果发现某些查询（例如，在您监控期间未执行的月度报告查询）确实需要索引，您可以快速重新启用它。

使用 INVISIBLE 关键字可将索引标记为不可见，使用 VISIBLE 关键字可将不可见索引恢复为可见。例如，要在 world.city
表的 Name 列上创建一个不可见索引，并在以后使其可见，可以使用

#+begin_src sql
  alter table world.city add index(Name) invisible;
#+end_src

#+begin_src sql
  alter table world.city alter index Name visible;
#+end_src

如果禁用了某个索引，而查询使用了指向隐藏索引的索引提示，查询将返回错误：

#+begin_comment
  ERROR: 1176: Key 'Name' doesn't exist in table 'city'
#+end_comment

通过启用优化器开关 use_invisible_indexes（默认为关闭），可以覆盖索引的不可见性。如果由于索引不可见而遇到问题，
且无法立即重新启用，或者想在新索引普遍可用之前对其进行测试，这将非常有用。为连接临时启用不可见索引的示例如下

#+begin_src sql
  SET SESSION optimizer_switch = 'use_invisible_indexes=on';
#+end_src

即使启用了 use_invisible_indexes 优化开关，也不允许在索引提示中引用该索引。

MySQL 8 的另一项新功能是降序索引。

*** 降序索引

在 MySQL 5.7 及更早版本中，当您添加 B 树索引时，它总是按升序排序。这非常适合查找精确匹配、按索引升序检索行等。
不过，虽然升序索引可以加快按降序查找行的查询速度，但效果并不理想。MySQL 8 增加了降序索引来帮助解决这些用例。

要利用降序索引，并不需要做什么特别的事情。例如，只需在索引中使用 DESC 关键字即可：

#+begin_src sql
  alter table world.city add index(Name DESC);
#+end_src

如果索引中有多个列，则这些列不必都按升序或降序排列。您可以根据查询的最佳效果，混合使用升序和降序列。

*** 分区和索引

如果创建分区表，分区列必须是主键和所有唯一键的一部分。这样做的原因是 MySQL 没有全局索引的概念，因此必须确保唯一性
检查只需考虑单个分区。

在性能调整方面，分区可以有效地使用两个索引来解决查询问题，而无需使用索引合并。当用于分区的列在查询的条件中使用时，
MySQL 会剪切分区，因此只搜索条件可以匹配的分区。然后就可以使用索引来解决查询的其余部分。

考虑一个表 t_part，该表根据 Created 列（时间戳）进行分区，每个月一个分区。如果查询 2019 年 3 月所有 val 列值
小于 2 的记录，那么查询将首先剪切 Created 值上的分区，然后使用 val 上的索引。清单 14-12 显示了这样一个示例。

#+begin_src sql
  create table t_part(
     id int unsigned not null auto_increment,
     Created timestamp not null,
     val int unsigned not null,
     primary key (id, Created),
     index(val)
  )engine=InnoDB 
  partition by range(unix_timestamp(Created))
  (partition p201901 values less than (1548939600),
  partition p201902 values less than (1551358800),
  partition p201903 values less than (1554037200),
  partition p201904 values less than (1556632800),
  partition p201905 values less than (1559311200),
  partition p201906 values less than (1561903200),
  partition p201907 values less than (1564581600),
  partition p201908 values less than (1567260000),
  partition pmax values less than maxvalue);
#+end_src

#+begin_src sql
  insert into t_part(Created, val)
  with recursive counter(i) as (
   select 1 
   union select i+1
   from counter 
   where i<1000)
  select FROM_UNIXTIME(
   floor(rand()*(1567260000-1546261200))
   +1546261200) , floor(rand()*10) from counter;
#+end_src

#+begin_src sql
  explain select id, Created, val 
  from t_part tp 
  where Created between '2019-03-01 00:00:00' and '2019-03-31 23:59:59' and val<2
#+end_src

t_part 表使用创建列的 Unix 时间戳按范围分区。EXPLAIN 输出（第 20 章将详细介绍 EXPLAIN）显示，查询中将只包含
p201903 分区，并使用 val 索引作为索引。由于示例使用的是随机数据，因此 EXPLAIN 的确切输出可能会有所不同。

到目前为止，关于索引的所有讨论都是针对明确创建的索引。对于某些查询，MySQL 还可以自动生成索引。这是我们要讨论的最后
一项索引功能。

*** 自动生成索引

对于包含连接到其他表或子查询的子查询的查询，由于子查询不能包含显式索引，因此连接可能会很昂贵。为了避免在这些由子查
询生成的临时表上进行全表扫描，MySQL 可以在连接条件上添加一个自动生成的索引。

以 sakila 示例数据库中的电影表为例。表中有一列名为 release_year，表示电影上映的年份。如果要查询在有数据的年份中
每年有多少部电影上映，可以使用下面的查询（是的，如果不使用子查询，这个查询可以写得更好，但这样写是为了演示自动生成
索引的功能）：

#+begin_src sql
  SELECT release_year, COUNT(*)
  FROM sakila.film
  INNER JOIN
  (SELECT DISTINCT release_year FROM sakila.film ) release_years USING (release_year)
  GROUP BY release_year;
#+end_src

MySQL 选择对电影表进行全表扫描，并在子查询中添加自动生成索引。当 MySQL 添加自动生成索引时，EXPLAIN 输出将包括
<auto_key0>（或用不同值替换的 0）作为可能的键和使用的键。

自动生成的索引可以大幅提高包含子查询的查询性能，而优化程序无法将这些子查询重写为普通连接。最重要的是，它是自动生成
的。

关于索引功能的讨论到此结束。在讨论如何使用索引之前，还需要了解 InnoDB 如何使用索引。

** InnoDB和索引

自 20 世纪 90 年代中期推出第一个版本以来，InnoDB 组织表的方式一直是使用聚类索引来组织数据。因此，人们常说
InnoDB 中的一切都是索引。从字面上看，数据的组织就是索引。默认情况下，InnoDB 使用主键来建立聚类索引。如果没有主
键，它会寻找不允许 NULL 值的唯一索引。在万不得已的情况下，InnoDB 会使用自动递增计数器为表添加一个隐藏列。

对于索引组织的表，InnoDB 中的所有内容都是索引。聚类索引本身是作为 B+ 树索引组织的，实际行数据位于叶子页中。这对
查询性能和索引有一些影响。接下来的章节将介绍InnoDB如何使用主键，以及这对次键意味着什么，提供一些建议，并探讨索
引组织表的最佳用例。

*** 聚集索引

由于数据是根据聚类索引（主键或其替代物）组织的，因此主键的选择非常重要。如果在现有值之间插入一条带有主键值的新记录，
InnoDB 就必须重新组织数据，为新记录腾出空间。在最坏的情况下，InnoDB 将不得不把现有页面一分为二，因为页面的大小是
固定的。页面拆分会导致叶页面在底层存储上失去顺序，造成更多随机 I/O，进而导致查询性能下降。页面拆分将作为第25章
中DDL和批量数据加载的一部分进行讨论。

*** 二级索引

二级索引的叶子页存储对行本身的引用。由于行是根据聚类索引存储在 B+ 树索引中的，因此所有二级索引都必须包含聚类索引的
值。如果您选择的列的值需要很多字节，例如具有长字符串且可能是多字节字符串的列，这将大大增加二级索引的大小。

这也意味着，在使用二级索引执行查找时，实际上要进行两次索引查找：首先是预期的二级键查找，然后从叶子页获取主键值，并
用于主键查找以获取实际数据。

对于非唯一二级索引，如果您有一个显式主键或一个 NOT NULL 唯一索引，用于主键的列就会被添加到索引中。MySQL 知道这
些额外的列，即使它们没有显式地成为索引的一部分，如果能改善查询计划，MySQL 就会使用它们。

*** 建议

由于 InnoDB 使用主键的方式，以及将主键添加到二级索引的方式，因此最好使用单调递增的主键，使用的字节数越少越好。自
动递增整数符合这些特性，因此是很好的主键。

如果表没有任何合适的索引，用于聚类索引的隐藏列会使用类似于自动递增的计数器来生成新值。但是，由于该计数器对 MySQL
实例中带有隐藏主键的所有 InnoDB 表都是全局的，因此会成为一个争用点。在复制过程中，隐藏键也不能用于定位受事件影响
的行，而分组复制需要主键或 NOT NULL 唯一索引来进行冲突检测。因此，建议始终为所有表明确选择一个主键。

另一方面，UUID 并不是单调递增的，因此不是一个好的选择。MySQL 8 中的一个选项是使用 UUID_TO_BIN()函数，并将第二
个参数设置为 1，这将使 MySQL 交换第一组和第三组十六进制数字。第三组是 UUID 时间戳部分的高字段，因此将其提升到
UUID 的开头有助于确保 ID 不断增加，而且将其存储为二进制数据所需的存储空间不到十六进制值的一半。

*** 最佳使用案例

索引组织的表对于使用该索引的查询特别有用。正如 "聚类索引 "这个名字所暗示的，聚类索引中具有相似值的记录会被存储在彼
此附近。由于 InnoDB 总是将整个页面读入内存，这也意味着主键值相似的两条记录很可能被一起读入。如果你在查询中或相隔
不久执行的查询中同时需要这两条记录，那么缓冲池中已经有了第二条记录。

现在，您应该对 MySQL 中的索引以及 InnoDB 如何使用索引（包括其数据组织）有了很好的背景知识。现在是将这些知识综合
起来讨论索引策略的时候了。

** 索引策略

说到索引，最大的问题是索引什么，其次是使用哪种索引和索引的哪些功能。我们不可能一步一步地创建终极指南来确保最佳索引；
为此，我们需要经验以及对模式、数据和查询的充分理解。不过，我们可以给出一些一般性指导原则，本节将对此进行讨论。

首先要考虑的是何时添加索引；是在最初创建表格时添加，还是稍后再添加。然后是主键的选择以及如何选择主键。最后是二级索
引，包括向索引中添加多少列，以及索引是否可以用作覆盖索引。

*** 什么时候应该新增索引或删除索引

索引维护是一项永无止境的任务。它从创建表格时就开始了，并贯穿整个表的生命周期。不要对索引工作掉以轻心--如前所述，好
的索引和差的索引之间的差别可能是几个数量级。如果索引效果不佳，就无法通过投入更多硬件资源来摆脱困境。索引不仅会影响
原始查询性能，还会影响锁定（将在第 18 章中进一步讨论）、内存使用率和 CPU 使用率。

创建表格时，尤其要花时间选择一个好的主键。在表的生命周期内，主键通常不会发生变化，如果你决定更改主键，对于索引组织
的表来说，必然需要对表进行全面重建。二级索引在更大程度上可以随着时间的推移而调整。事实上，如果计划在表的初始数据量
中导入大量数据，最好等到数据加载完成后再添加二级索引。唯一索引可能是一个例外，因为数据验证需要唯一索引。

创建表格并填充初始数据后，需要监控表格的使用情况。在 sys 模式中有两个视图可用于查找表和进行全表扫描的语句：

+ schema_tables_with_full_table_scans 该视图显示所有未使用索引读取记录的表，并按该数字降序排列。如果某个表
  在未使用索引的情况下读取了大量记录，则可以查找使用该表的查询，看看索引是否能提供帮助。该视图基于
  table_io_waits_summary_by_index_usage 性能模式表，也可以直接使用，例如，如果要进行更高级的分析，如查找未
  使用索引读取的行的百分比。

+ statements_with_full_table_scans 该视图显示了完全不使用索引或不使用良好索引的语句的规范化版本。这些语句按
  完全不使用索引的执行次数排序，然后按不使用良好索引的次数排序--均以降序排列。该视图基于
  events_ statements_summary_by_digest 性能模式表。


第 19 章和第 20 章将详细介绍这些视图和底层性能模式表的使用。

如果发现查询可以从额外的索引中获益，那么就需要评估在执行查询时，额外获益的成本是否值得。

同时，还需要留意是否有不再使用的索引。performance schema和sys schema式对查找未使用或不常用的索引特别有用。
以下三个系统模式视图非常有用

- schema_index_statistics 该视图统计了使用给定索引读取、插入、更新和删除行的频率。与
  schema_tables_with_full_table_scan 视图一样，schema_index_statistics 也是基于
  table_io_waits_ summary_by_index_usage 性能模式表。

- schema_unused_indexes 该视图将返回自上次重置数据以来（不长于上次重启时间）尚未使用的索引名称。该视图也基于
  table_io_waits_summary_by_index_usage 性能模式表。

- schema_redundant_indexes 如果有两个索引覆盖相同的列，那么 InnoDB 为保持索引最新而付出的努力就会加倍，优化
  器的负担也会加重，但却不会有任何收获。顾名思义，schema_redundant_indexes视图可以用来查找冗余索引。该视图基
  于 STATISTICS 信息模式表。

使用前两个视图时，必须记住数据来自performance schema中的内存表。如果您有一些查询只是偶尔执行，那么统计数据可
能无法反映您的总体索引需求。在这种情况下，隐形索引功能就会派上用场，因为它可以让你禁用索引，同时保留索引，直到你确
定可以安全地放弃它。如果发现一些很少执行的查询需要索引，你可以很容易地再次启用索引。

如前所述，首先要考虑的是选择什么作为主键。应该包括哪些列？这是接下来要讨论的问题。

*** 主键的选择

在使用索引组织的表时，主索引的选择非常重要。主键会影响随机 I/O 和顺序 I/O 的比例、二级索引的大小以及需要读入缓冲
池的页数。InnoDB 表的主键始终是 B+ 树索引。

与聚类索引相关的最佳主键应尽可能小（以字节为单位），保持单调递增，并在短时间内对频繁查询的记录进行分组。在实践中，
可能无法满足所有这些要求，在这种情况下，你需要做出最好的妥协。对于许多工作负载来说，自动递增的无符号整数（取决于表
的预期行数，可以是 int 也可以是 bigint）是一个不错的选择；不过，也可能有一些特殊的考虑因素，比如对跨多个 MySQL
实例的唯一性的要求。主键最重要的特点是，它应尽可能具有顺序性和不可更改性。如果更改了某一行的主键值，则需要将整行移
动到聚类索引中的新位置。

#+begin_comment
  自动递增的无符号整数通常是主键的最佳选择。它可以单调递增，不需要太多存储空间，还能在聚类索引中将最近的记录分组。
#+end_comment

你可能会认为，隐藏主键和其他列一样，是聚类索引的最佳选择。毕竟，它是一个自动递增的整数。然而，隐藏主键有两个主要缺
点：它只能识别本地 MySQL 实例中的行，而且计数器对（实例中的）所有 InnoDB 表都是全局的，没有用户定义的主键。隐藏
键只在本地有用，这意味着在复制过程中，隐藏值不能用来识别要在副本中更新的记录。计数器是全局性的，这意味着在插入数据
时，它可能会成为一个争夺点，并导致性能下降。

总之，你应该明确定义你想要的主键。对于二级索引，有更多的选择，我们接下来会看到。

*** 新增二级索引

二级索引指的是主键之外的所有索引。它们可以是唯一的，也可以是非唯一的，你可以在所有支持的索引类型和功能中进行选择。
如何选择添加哪些索引？本节将帮助你更轻松地做出决定。

注意不要在表中添加过多索引。索引会产生开销，因此如果添加的索引最终没有被使用，查询和系统的整体性能就会变差。这并不
意味着在创建表格时不应该添加任何二级索引。只是你需要花点心思在这上面。

在执行查询时，可以通过多种方式使用二级索引。其中一些方法如下：

+ 减少检查的记录： 当使用 WHERE 子句或连接条件时，可以在不扫描整个表的情况下找到所需的记录。
+ 排序数据： B 树索引可用于按照查询需要的顺序读取行，从而让 MySQL 绕过排序步骤。
+ 验证数据： 这就是唯一索引中唯一性的用途。
+ 避免读取整行： 覆盖索引可以在不读取整行的情况下返回所有需要的数据。
+ 查找 MIN() 和 MAX() 值： 对于 GROUP BY 查询，只需检查索引中的第一条记录和最后一条记录，即可找到索引列的最小
  值和最大值。

主键显然也可以用于所有这些目的。从查询的角度看，主键和次键没有区别。

在决定是否添加索引时，您需要问问自己需要索引的目的是什么，以及索引是否能够实现这些目的。一旦确认了这一点，就可以考
虑多列索引应该按照什么顺序添加列，以及是否应该添加额外的列。接下来的两个小节将对此进行更详细的讨论。

*** 多列索引

只要不超过索引的最大宽度，就可以在索引中添加多达 16 个列或功能部分。这既适用于主键，也适用于二级索引。InnoDB 对
每个索引的限制是 3072 字节。如果包含使用可变宽度字符集的字符串，那么计算索引宽度的是可能的最大宽度。

在索引中添加多个列的一个好处是，可以将索引用于多个条件。这是提高查询性能的有效方法。举例来说，如果要查询某个国家的
城市，并要求该城市的最低人口数： 

#+begin_src sql
  select ID, Name, District, Population
  from world.city
  where CountryCode='AUS'
  and Population>1000000;
#+end_src

您可以在 CountryCode 列上使用索引来查找国家代码设置为 AUS 的城市，也可以在 Population 列上使用索引来查找人口
超过 100 万的城市。更妙的是，您可以将其合并为一个包含两列的索引。

如何做到这一点很重要。国家代码使用的是等量引用，而人口则是范围搜索。索引中的列一旦用于范围搜索或排序，除了作为覆盖
索引的一部分外，就不能再使用索引中的其他列。在本例中，需要在 "人口 "列之前添加 "国家代码 "列，以便将索引用于两个
条件：

#+begin_src sql
  alter table world.city
  add index (CountryCode, Population);
#+end_src

在这个例子中，索引甚至可以用来对结果进行排序。

如果需要添加多个列，且所有列都用于相等条件，那么需要考虑两点：哪些列最常用，以及列过滤数据的效果如何。当索引中有多
个列时，MySQL 将只使用索引的左前缀。例如，如果您有一个索引（col_a、col_b、col_c），那么只有同时对 col_a 进行
过滤（而且必须是相等条件），才能使用该索引对 col_b 进行过滤。因此需要谨慎选择顺序。在某些情况下，可能需要为同一列
添加多个索引，而索引之间的列顺序是不同的。

如果无法根据使用情况决定包含列的顺序，则先添加选择性最强的列。下一章将讨论索引的选择性，但简而言之，列的独特值越多，
选择性就越强。通过先添加选择性最强的列，可以更快地缩小索引部分包含的行数。

您可能还想包含不用于筛选的列。为什么要这样做呢？答案是这有助于形成覆盖索引。

*** 覆盖索引

覆盖索引是一个表上的索引，其中给定查询的索引包括该表所需的所有列。这意味着，当 InnoDB 到达索引的叶子页时，它已经
掌握了所需的全部信息，而不需要读取整条记录。根据表的具体情况，这可能会大大提高查询性能，尤其是如果你能用它来排除行
中的大部分内容，比如大文本或 blob 列。

还可以使用覆盖索引来模拟二级聚类索引。请记住，聚类索引只是一个 B+ 树索引，整个行都包含在叶子页中。覆盖索引在叶子
页中包含行的一个完整子集，因此可以模拟该子集列的聚类索引。与聚类索引一样，任何 B 树索引都会将相似的值组合在一起，
因此可以用来减少读入缓冲池的页数，并有助于在执行索引扫描时进行顺序 I/O。

不过，与聚类索引相比，覆盖索引有一些限制。覆盖索引只能在读取时模拟聚类索引。如果你需要写入数据，那么更改始终必须访
问聚类索引。另外，由于 InnoDB 的多版本并发控制 (MVCC)，即使使用覆盖索引，也必须检查聚簇索引，以验证是否存在该行
的另一个版本。

添加索引时，值得考虑的是索引的查询需要哪些列。即使索引不用于对这些列进行筛选或排序，也值得添加选择部分中使用的任何
额外列。您需要在覆盖索引的好处和索引增加的大小之间取得平衡。因此，如果只是漏掉一两个小列，这种策略就很有用。覆盖索
引带来的查询效益越大，您可以接受添加到索引中的额外数据也就越多。

** 总结

本章是一次索引世界之旅。一个好的索引策略意味着数据库是停滞不前还是运转良好。索引有助于减少查询中检查的记录数量，此
外，覆盖索引可以避免读取整条记录。另一方面，索引在存储和持续维护方面也会产生开销。因此，有必要平衡对索引的需求和建
立索引的成本。

MySQL 支持几种不同的索引类型。最重要的是 B 树索引，这也是 InnoDB 使用聚类索引在其索引组织表中组织行时使用的索
引。其他索引类型包括全文索引、空间（R 树）索引、多值索引和哈希索引。后一种类型在 InnoDB 中比较特殊，因为它只支
持自适应哈希索引功能，该功能会自动决定添加哪些哈希索引。

我们已经讨论过一系列索引功能。功能索引可用于对表达式中使用列的结果进行索引。前缀索引可用于减少文本和二进制数据类型
索引的大小。在推出新索引或软删除现有索引时，可以使用隐形索引。降序索引可提高按降序遍历索引值的效率。索引还与分区相
关联，您可以使用分区有效地实现支持在查询中对单个表使用两个索引。最后，MySQL 能够自动生成与子查询相关的索引。

本章的最后一部分首先介绍了 InnoDB 的特性以及使用索引组织表的注意事项。对于与主键相关的查询，这些表是最佳选择，但
对于以随机主键顺序插入的数据和通过二级索引查询数据，这些表的效果就没那么好了。

本章的最后一部分首先介绍了 InnoDB 的特性以及使用索引组织表的注意事项。对于与主键相关的查询，这些表是最佳选择，但
对于以随机主键顺序插入的数据和通过二级索引查询数据，这些表的效果就没那么好了。

关于什么是索引以及何时使用索引的讨论到此结束。在下一章讨论索引统计时，我们将看到更多关于索引的内容。




* 索引统计

在上一章中，我们学习了索引。其中提到，优化器会评估每个索引，以决定是否使用该索引。它是如何做到这一点的呢？这主要是本
章的主题，包括索引统计、如何查看索引统计信息以及如何维护统计。

** 什么是索引统计？

当 MySQL 决定是否使用索引时，归根结底要看 MySQL 认为索引对查询有多有效。请记住，当你使用二级索引时，实际上会有一
个额外的主键查询来获取数据。二级索引的排序方式也与行的排序方式不同，因此使用索引一般意味着随机 I/O（使用覆盖索引可
以帮助解决这个问题）。另一方面，表扫描在很大程度上是顺序 I/O。因此，逐行进行表扫描要比使用二级索引查找同一行便宜。

这意味着，要使索引有效，必须过滤掉表的大部分内容。具体要过滤掉多少，取决于硬件的性能特性、缓冲池中表的容量、表的定
义等。在老式旋转磁盘时代，经验法则是如果需要的行超过 30%，则首选表扫描。内存中的行越多，磁盘的随机 I/O 性能越好，
这个阈值就越高。

#+begin_comment
  覆盖索引可以减少从跳转到实际行数据所需的随机 I/O 量，从而改变这种情况。
#+end_comment

这就是索引统计的作用所在。优化器（MySQL 中决定使用哪种查询计划的部分）需要一些简单的方法来确定给定查询计划的索引
好坏。优化器显然知道索引包含哪些列，但它还需要知道索引过滤行的效果如何。这些信息正是索引统计信息所提供的。因此，索
引统计信息是索引选择性的衡量标准。主要有两种统计数据：唯一值的数量和某个范围内的值的数量。

在讨论索引统计时，人们最常想到的是唯一值的数量。这就是索引的卡入度。卡片性越高，唯一值越多。对于主键和其他不允许
NULL 值的唯一索引来说，由于所有值都必须是唯一的，因此卡片性就是表中的行数。

优化程序会根据逐个查询请求给定范围内的行数。这对范围条件（如 WHERE val > 5）、IN()条件或一系列 OR 条件都很有
用。为单个查询临时收集这些信息的一个例外是直方图，MySQL 8 支持直方图。下一章将讨论直方图。

简而言之，索引统计是关于索引中数据分布的近似信息。在 MySQL 中，负责提供索引统计信息的是存储引擎。因此，InnoDB
如何处理索引统计是值得深入研究的。

** InnoDB和索引统计

正是存储引擎向服务器层和优化器提供了索引统计数据。因此，了解 InnoDB 如何确定其统计数据非常重要。InnoDB 支持两种
存储统计数据的方式：持久性和瞬时性。无论哪种方式，统计信息的确定方法都是一样的。本节将首先讨论如何收集统计数据，然
后介绍持久和瞬时统计数据的具体细节。

** 统计是怎样收集的

InnoDB 通过分析索引的随机叶页来计算索引统计数据。例如，可能会随机抽取 20 个索引页（这也被称为 20 个索引潜入），
并检查这些页包含哪些索引值。然后，InnoDB 会根据索引的总大小进行缩放。

这意味着 InnoDB 索引统计数据并不精确。当你看到某个查询条件意味着将读取 100 条记录时，这只是根据分析样本得出的估
计值。这甚至包括主键和其他唯一索引，以及在 information_schema.TABLES视图。表中的估计行数与主键的估计卡长度相
同。

另一个考虑因素是如何处理 NULL 值，因为 NULL 具有不等于 NULL 的属性。因此，在收集统计数据时，是将所有 NULL 值归
入一个数据桶，还是将它们分开处理？最佳解决方案取决于您的查询。将所有 NULL 值视为不同的值会增加索引的卡入度，尤其是
在索引列有很多 NULL 行的情况下。这对查找非 NULL 值的查询很有帮助。另一方面，如果将所有 NULL 都视为相同值，则会降
低索引的核心数量，这对于包含 NULL 的查询来说是有意义的。你可以使用 innodb_stats_method 选项来选择 InnoDB 处理
NULL 值的方式。该选项有三种取值：

+ nulls_equal 在这种情况下，所有 NULL 值都被视为相同。这是默认值。如果不确定选择哪个值，请选择 nulls_equal。
+ nulls_unequal 在这种情况下，NULL 值被视为不同的值。
+ nulls_ignored 在这种情况下，收集统计数据时将忽略 NULL 值。

为什么使用估算值而不是精确统计值（即全索引扫描）？原因在于性能。对于大型索引，执行完整的索引扫描需要很长时间。一般
还包括磁盘 I/O，这使得性能问题更加严重。为了避免计算索引统计量对查询性能产生不利影响，我们选择将扫描限制在相对较少
的页数上。

** 样本页

使用近似统计量的缺点是，它们并不总是能很好地代表值的实际分布。出现这种情况时，优化器可能会选择错误的索引或错误的连
接顺序，导致查询速度慢于所需。不过，也可以调整随机索引下潜的次数。如何调整取决于使用的是持久统计还是瞬时统计：

+ 持久性统计使用 innodb_stats_persistent_sample_ pages 选项作为默认的采样页数。表选项 STATS_SAMPLE_PAGES
  可以用来指定特定表的页数。

+ 暂态统计使用 innodb_ stats_transient_sample_pages 选项为所有表指定的页数。

关于持久统计和瞬时统计的两个小节详细介绍了处理索引统计的两种方法。

将样本页数设置为给定值是什么意思？这取决于索引中的列数。如果只有一列，该值的字面意思就是采样该数量的叶页。但是，对
于多列索引，页数是按列计算的。例如，如果将样本页数设置为 20，而索引中有四列，则总共采样 4*20=80 页。

#+begin_comment
  实际上，索引统计采样比本章描述的更为复杂。例如，并不总是需要一直向下到叶页。如果两个相邻的非叶子节点具有相同的
  值，那么可以得出结论：最左边（按照排序）的所有叶子页都具有相同的值。如果你有兴趣了解更多，源代码中
  storage/innobase/ dict/dict0stats.cc 文件顶部的注释是一个很好的起点：
  https://github.com/mysql/mysql-server/blob/8.0/ storage/innobase/dict/dict0stats.cc。
#+end_comment

必须检查多少页才能得到准确的估计？这取决于表格。如果数据是均匀的，即每个索引值的行数大致相同，那么只需要检查相对较
少的页数，默认页数通常就足够了。另一方面，如果数据分布很不规则，则可能需要增加采样页数。队列中任务的状态就是非常不
规则的数据的一个例子。随着时间的推移，大多数任务都会处于完成状态。在最糟糕的情况下，你可能会遇到所有随机潜水都看到
相同状态的情况，这使得 InnoDB 得出结论：只有一个值，索引作为过滤器毫无价值。

#+begin_comment
  对于仅有几行数据的过滤值，下一章将讨论的直方图对于改进查询计划非常有用。
#+end_comment

表格的大小也是一个需要考虑的因素。一般来说，表越大，必须检查的页数就越多，才能获得良好的估算结果。原因是表越大，整
个叶页就越有可能指向具有相同索引值的行。这就降低了每个取样页的值，因此为了弥补这一缺陷，有必要对更多页面进行取样。

一种特殊情况是 InnoDB 被配置为索引下潜次数多于叶子页的数量。在这种情况下，InnoDB 会检查所有叶子页，并在此时停止。
这将提供尽可能准确的统计数据。如果在分析过程中没有活动事务，那么该时间点的统计数据将是准确的。这包括表中的页数。您
将在本章稍后部分学习如何查找使用持久性统计的索引和表中的页数。

实际上，不可能使用精确的值。InnoDB 支持多版本，允许事务的高并发性，即使这些事务涉及写入。由于每个事务都有自己的数
据视图，精确的统计数据意味着每个事务都有自己的索引统计数据。这是不可行的，那么InnoDB是如何处理的呢？这是下一个
要考虑的问题。

** 事务隔离级别

与此相关的一个问题是，收集统计数据时使用的是哪种事务隔离级别。InnoDB 支持四种隔离级别：已读未提交、已读已提交、可
重复读（默认）和可序列化。在收集索引统计数据时，我们选择使用未提交读取。这是有道理的，因为可以很好地假定大多数事务
最终都会提交，或者如果提交失败会重试。这些统计信息将用于未来的查询，因此没有必要在收集统计信息时增加维护读视图的开
销。

不过，这对于对表进行较大更改的事务确实有影响。一个极端（但并非不可能）的例子是，考虑一个缓存表，其中的数据由一个包
含两个步骤的事务刷新：

1. 删除表中的所有现有数据。
2. 用更新的数据重建表格。

默认情况下，当表的 "大部分 "内容发生变化时，会更新索引统计信息。(本章后面的 "持久索引统计 "和 "瞬时索引统计 "部
分将介绍什么是 "大部分"）。这意味着步骤 1 完成后，InnoDB 将重新计算统计数据。这很简单--表是空的，所以没有统计数
据。如果查询就在此时执行，优化器会认为表是空的。不过，除非查询是在 "读取未提交事务 "隔离级别下执行的，否则查询仍会
读取所有旧行，而且查询计划很可能会导致查询执行效率低下。   

对于像刚才讨论的问题，你需要持久统计数据，因为有更好的配置选项来处理特殊情况。在讨论持久统计数据的细节之前，值得学
习的是如何在持久统计数据和瞬时统计数据之间做出选择。

** 配置统计类型

如前所述，InnoDB 有两种方式来存储索引统计信息。它可以使用持久存储，也可以使用暂存存储。你可以使用
innodb_stats_persistent 选项为表设置默认方法。设置为 1 或 ON（默认值）时，将使用持久性统计；设置为 0 或
OFF 时，将改为瞬时统计。还可以使用 STATS_ PERSISTENT 表选项为每个表配置统计方法。例如，要为 world.city 表启
用持久统计，可以使用 ALTER TABLE 这样的命令

#+begin_src sql
  alter table world.city STATS_PREPARED=1;
#+end_src

使用 CREATE TABLE 语句创建新表时，也可以设置 STATS_PERSISTENT 选项。对于 STATS_PERSISTENT，只能使用 0 和
1 作为值。

持久索引统计自推出以来一直是默认设置，也是推荐选择，除非遇到测试表明瞬态统计可以解决的问题。持久统计与瞬态统计之间
存在一些差异，必须加以了解。接下来将讨论这些差异。

** 持久性索引统计

持久索引统计是在 MySQL 5.6 中引入的，目的是使查询计划比旧的瞬时索引统计更稳定。顾名思义，启用持久索引统计后，统
计信息将被保存，因此在重启 MySQL 时不会丢失。不过，除了持久性之外，还有更多不同之处，我们将一一道来。

除了稳定的查询计划外，持久统计数据还允许对采样页数进行详细配置，并具有良好的监控功能，甚至可以直接查询保存统计数据
的表。由于监控与瞬态统计有很大的重叠，因此将在本章稍后部分讨论，本节将重点讨论持久统计的配置和存储统计的表。

** 配置

可以对持久统计数据进行配置，以便在收集统计数据的成本和统计数据的准确性之间取得良好的平衡。与瞬时统计不同的是，可以
在全局级别和每个表上配置行为。在未设置表特定选项时，全局配置将作为默认设置。

有三个全局选项专门针对持久性统计。它们是

+ innodb_stats_persistent_sample_pages： 要采样的页面数。页数越多，统计数据越准确，但成本也越高。如果该值大
  于索引的叶子页数，则对整个索引进行采样。默认值为 20。
+ innodb_stats_auto_recalc： 当表中超过 10%的行被更改时，是否自动更新统计信息。默认为启用 (ON)。
+ innodb_stats_include_delete_marked： 是否在统计中包含已标记为删除但尚未提交的记录。稍后将详细讨论这个选
  项。默认为禁用（OFF）。

innodb_stats_persistent_sample_pages 和 innodb_stats_auto_recalc 选项也可以按表设置。这样就可以根据特
定表的大小、数据分布和工作量来微调需求。虽然不建议进行微调，但可以用来处理前面讨论过的缓存表等情况，以及一般默认值
无法覆盖的其他表。

我们的建议是，尽量为 innodb_stats_ persistent_sample_pages 找到一个折中方案，既能提供足够好的统计数据，让优
化器确定最佳查询计划，又能避免为计算统计数据而进行过多的扫描。如果你发现由于索引统计数据不准确导致优化器选择了低效
计划，从而导致查询性能下降，那么你就需要增加采样页面的数量。另一方面，如果 ANALYZE TABLE 耗时过长，可以考虑减少
采样页数。然后，可以使用即将介绍的特定表选项，根据需要减少或增加特定表的采样页数。

对于大多数表，建议启用 innodb_stats_auto_recalc。这将有助于确保统计数据不会因为大量变更而过时。自动校验在后台
进行，因此不会延迟触发更新的应用程序的响应。当表中超过 10% 的内容发生变化时，表将排队等待索引统计更新。为避免不断
重新计算小表的统计数据，还要求每次索引统计数据更新之间至少间隔 10 秒。

当然，也有不希望自动重新计算统计数据的例外情况，例如，如果您有一个缓存表来加快报告查询的执行速度，而缓存表中的数据
会不时完全重建，但其他方面不会发生变化。在这种情况下，禁用统计数据的自动重新计算，并在重建完成后显式地重新计算统计
数据，可能会有好处。另一种方法是在统计中包含删除标记的行。

请记住，索引统计数据是使用读取未提交事务隔离级别计算的。虽然这在大多数情况下都能提供最佳统计数据，但也有例外。当事
务暂时完全改变了数据分布时，可能会导致不正确的统计数据。完全重建表是最极端的情况，也是最常见的问题所在。
innodb_stats_ include_delete_marked选项就是为这种情况而引入的。InnoDB 不会将未提交的已删除记录视为已删除，
而是仍将其纳入统计。该选项仅作为全局选项存在，因此即使只有一个表存在这个问题，它也会影响所有表。如前所述，另一种方
法是禁用受影响表的统计自动重新计算，然后自行处理。

#+begin_comment
  如果有事务会对表进行较大的更改，比如删除所有行然后重建表，那么可以考虑禁用表的索引统计自动重新计算功能，或者启用
  innodb_stats_include_delete_marked。
#+end_comment

到目前为止，只提到了全局选项。如何更改表的索引统计设置？由于可以使用 STATS_PERSISTENT 表选项来覆盖表的
innodb_stats_persistent 全局值，所以还有一些选项可以控制表的持久化统计行为。表选项包括

+ stats_auto_recalc： 重写表是否启用了自动重新计算索引统计量的功能。
+ stats_sample_pages： 重写表格的采样页数。

可以在使用 CREATE TABLE 创建表格时设置这些选项，也可以在之后使用 ALTER TABLE 设置这些选项，如清单 15-1 所
示。

#+begin_src sql
  create schema if not exists chapter_15;
  use chapter_15
  create table city(
    City_ID int unsigned not null auto_increment,
    City_Name varchar(40) not null,
    State_ID int unsigned not null,
    primary key(City_ID),
    index(City_Name, State_ID, City_ID)
  )STATS_AUTO_RECALC=0,
  STATS_SAMPLE_PAGES=10;
#+end_src

#+begin_src sql
  alter table city STATS_AUTO_RECALC=1,
                   STATS_SAMPLE_PAGES=20;
#+end_src

首先，创建表格城市，禁用自动校正功能，并设置 10 个样本页。然后更改设置，启用自动重复计算，并将样本页数增加到 20
页。请注意 ALTER TABLE 返回的受影响行数为 0。更改持久性统计选项只会更改表的元数据，因此会立即发生，不会影响数据。
这意味着你可以根据需要更改设置，而不必担心执行昂贵的操作。例如，您可能希望在批量操作中禁用自动重新计算。

有了调整指数统计的机会，对收集到的数据进行分析就显得尤为重要。在 "监控 "部分讨论瞬时统计数据后，将讨论一些一般方法。
不过，持久统计数据之所以持久，是因为它们存储在表中，这些表也提供了有价值的信息。

** 索引统计表

InnoDB 在 mysql 模式中使用两个表来存储与持久统计数据相关的数据。这些表不仅有助于研究统计数据和采样数据，还有助于
了解索引的总体情况。

最常用的表是 innodb_index_stats 表。该表为每个 B 树索引提供了几行信息，包括索引各部分的唯一值数量
（cardinality）、索引中的叶子页数量以及索引的总大小。表 15-1 总结了表中的列。

| 列名             | 数据类型          | 描述                                       |
|-----------------+-----------------+-------------------------------------------|
| database_name    | varchar(64)     | 模式中带有索引的表的位置。                     |
| table_name       | varchar(199)    | 带索引的表名                                |
| index_name       | varchar(64)     | 索引名                                     |
| last_update      | timestamp       | 上一次索引统计更新                           |
| stat_name        | varchar(64)     | 统计量的名称。另请参阅本表后的内容。            |
| stat_value       | bigint unsigned | 统计值                                     |
| sample_size      | bigint unsigned | 采样多个页                                  |
| stat_description | varchar(1024)   | 统计量的描述。对于卡入度，它是计算卡入度时包含的列 |

主键由列 database_name、table_name、index_name 和 stat_name 组成。数据库、表和索引名称定义了统计数据所针
对的索引。last_update 列用于查看统计信息上次更新的时间。stat_name 和 stat_value 为实际统计数据。sample_size
是为确定统计数据而检查的页数。这将是索引中的页数和为表设置的样本页数中较小的一个。最后，stat_description 列提供
了有关统计量的更多信息。对于卡片式统计，说明将显示索引中包含哪些列，每列将有一行（稍后将提供示例）。

如前所述，innodb_index_stats 表中包含多种统计信息。名称可以是以下值之一：

+ n_diff_pfxNN： 索引中前 NN 列的卡片数。NN 以 1 为单位，因此对于有两列的索引来说，n_diff_pfx01 和
  n_diff_pfx02 是存在的。对于具有这些统计量的行，stat_ description 包括统计量所包含的列。
+ n_leaf_pages： 索引中叶页的总数。您可以将其与 n_diff_pfxNN 统计量的样本大小进行比较，以确定已采样索引的部分。
+ size： 索引的总页数。这包括非叶页。

看一个示例可以更好地理解这些数据的含义。world.city 表有两个索引：ID 列上的主键和 CountryCode 列上的
CountryCode 索引。清单 15-2 显示了这两个索引的统计信息。请注意，如果执行相同的查询，统计值可能会有所不同，如果
仍有第 14 章中添加的额外索引，则会有更多记录。

#+begin_src sql
  select index_name, stat_name,
         stat_value, sample_size,
         stat_description
   from mysql.innodb_index_stats
   where database_name='world'
         and table_name='city'\G;
#+end_src

第 1-4 行是 CountryCode 索引，而第 5-7 行是主键。首先要注意的是，CountryCode 索引同时存在 n_diff_pfx01
和 n_diff_pfx02 统计量。为什么索引只包含一列？请记住，InnoDB 使用的是聚类索引，而非唯一索引总是会附加主键，因
为无论如何都需要它来定位实际行。这就是 n_diff_pfx01 代表 CountryCode 列，n_diff_pfx02 代表 CountryCode
列和 ID 列的组合。

CountryCode 索引共有 8 页，其中 7 页是叶节点。这意味着索引有两级，叶节点为第 0 级，根节点为第 1 级。我们建议你
回顾上一章关于 B 树索引的讨论，并在查看表中某些索引的大小统计时复习一下。

主键比较简单，因为它只有一列。这里有 24 个叶页，因此只对索引的一个子集进行了采样（记住，对于主键来说，索引就是表）。
(记住，对于主键来说，索引就是表。）这样做的后果是统计数据并不精确。主键的 n_diff_pfx01 预测值为 4188 个唯一值。
由于它是主键，这也是对总行数的估计。但是，如果查看 CountryCode 的统计信息，就会发现 CountryCode 和 ID 值有
4079 种不同的组合。由于 CountryCode 索引只有 7 个叶页，所有页都已检查过，因此行数估算是准确的。

另一个与持久化统计相关的表是 innodb_table_stats 表。它与 innodb_index_stats类似，不同之处在于它包含的是整
个表的汇总统计。表 15-2 概括了 innodb_table_stats 的列。


#+NAME: innodb_table_stats
| 列名                  | 数据类型          | 描述              |
|----------------------+-----------------+------------------|
| database_name         | varchar(64)     | 表所在的模式名      |
| table_name            | varchar(199)    | 表名              |
| last_update           | timestamp       | 上次表统计更新的时间 |
| n_rows                | bigint unsigned | 表中行的估算数据    |
| clustered_index_size   | bigint unsigned | 聚簇索引的页数      |
| sum_of_other_index_sizes | bigint unsigned | 二级索引的总计页数  |


主键由列 database_name 和 table_name 组成。需要注意的一点是，表统计量与索引统计量一样都是近似值。表中的行数只
是主键的估计卡入度。同样，聚类索引的大小与 innodb_index_stats 表中主键的大小相同。二级索引页数是每个二级索引大
小的总和。清单 15-3 以 world.city 表为例，展示了 innodb_table_stats 表的内容，使用的索引统计量与前面的示例
相同。

#+begin_src sql
  select *
     from mysql.innodb_table_stats
     where database_name='world'
       and table_name='city'\G
#+end_src

#+begin_comment
  innodb_index_stats和innodb_table_stats是常规表。在备份中包含这些表是很有用的，这样如果查询计划突然发生
  变化，就可以回过头来比较统计数据。

  还可以为具有 UPDATE 权限的用户更新表。这似乎是一个非常有用的属性，但要小心。如果不知道正确的统计信息，查询计划
  将非常糟糕。几乎不应该手动修改索引统计信息。如果进行了修改，只有在刷新表后才会生效。
#+end_comment

如果你觉得innodb_index_stats和innodb_table_stats中的信息与SHOW INDEX语句、TABLES和STATISTICS
信息模式表中的信息听起来很相似，那你就对了。两者有一些重叠。由于这些数据源也适用于瞬态统计，因此对它们的讨论将推迟到
介绍完瞬态索引统计之后。

** 暂态指数统计

瞬态索引统计是 InnoDB 最初实现的处理索引统计的方法。顾名思义，这种统计不是持久性的，也就是说，当 MySQL 重新启动
时，它们不会持久存在。相反，统计信息是在表首次打开时（还有其他时间）计算的，并且只保存在内存中。由于统计数据不持
久，因此稳定性较差，查询计划更容易发生变化。

有两个配置选项可以影响瞬态统计的行为。它们是

+ innodb_stats_transient_sample_pages： 更新索引统计时采样的页数。默认值为 8。
+ innodb_stats_on_metadata： 查询表的元数据时是否重新计算统计数据。默认值为 OFF，自 MySQL 5.6 起一直如此。

innodb_stats_transient_sample_pages选项等同于innodb_stats_ persistent_sample_pages，但它适用于使用
瞬态统计的表。使用瞬态统计的表不仅会在首次打开时重新计算统计量，而且还会在仅有 6.25% (1/16) 行发生变化时重新计算
统计量，并要求至少有 16 次更新。此外，在自动重新计算统计数据时，瞬态统计数据不使用后台线程，因此更新更有可能影响性
能。因此，innodb_stats_ transient_sample_pages 的默认值只有 8 页。

如果想更频繁地更新瞬时索引统计信息，可以启用 innodb_stats_on_metadata 选项。启用该选项后，查询信息模式中的
TABLES 表和 STATISTICS 表，或使用与之相当的 SHOW 语句，都会触发索引统计信息的更新。在实际操作中，很少会出现这
种情况，因此关闭该选项是安全的。

暂态统计没有专门的表。不过，MySQL 中的所有表都有可用的表和语句。

** 监控

索引统计信息对于优化器帮助确定执行查询的最佳方式非常重要。因此，了解如何检查表的索引统计也很重要。前面已经讨论过，
有 mysql.innodb_index_stats 表和 mysql.innodb_table_stats 表可用于持久化统计。不过，也有一些通用方法，本
文将讨论这些方法。

#+begin_comment
  请记住，information_schema_stats_expiry 变量会影响数据字典刷新索引统计相关数据视图的频率。
#+end_comment

** 信息模式 STATISTICS 视图

获取索引统计详细信息的主要表格是信息模式中的 STATISTICS 视图。该视图不仅包含索引统计信息本身，还包含索引的元信
息。事实上，你可以根据 STATISTICS 视图中的数据重新创建索引定义。这就是上一章中用来查找表中索引名称的视图。

表 15-3 包含视图中各列的摘要。您通常只需要其中的一部分列，但在需要时访问所有信息还是很方便的。CARDINALITY 列是
唯一受 information_schema_stats_expiry 变量影响的列。

#+NAME:STATISTICS
| 列名          | 数据类型       | 描述                                                             |
|--------------+--------------+-----------------------------------------------------------------|
| TABLE-CATALOG  | varchar(64)  | 表格所属的目录。                                                   |
| TABLE_SCHEMA  | varchar(64)  | 表格所在的模式。                                                   |
| TABLE_NAME    | varchar(64)  | 索引所在的表。                                                     |
| NON_UNIQUE    | int          | 索引是否唯一（0）或不唯一（1）。                                     |
| INDEXSCHEMA  | varchar(64)  | 与 TABLE_SCHEMA 相同（因为索引总是与表同处）。                         |
| INDEX_-NAME   | varchar(64)  | 索引名                                                           |
| SEQ_IN_INDEX   | int unsigned | 列在索引中的位置。对于单列索引，该值始终为 1。                          |
| COLUMN_NAME   | varchar(64)  | 列名                                                             |
| COLLATION    | varchar(1)   | 索引的排序方式。值可以是 NULL（不排序）、a（升序）或 d（降序）。          |
| CARDINALITY  | bigint       | 该行中包含列的索引部分的唯一值的估计数量。                             |
| SUB_PART      | bigint       | 对于前缀索引，它是被索引的字符数或字节数。如果整个列都被索引，则值为 NULL。 |
| PACKED       | binary(0)    | 对于 InnodB 表，该值始终为空。                                      |
| NULLABLE     | varchar(3)   | 是否允许 NULL 值，该列要么为空字符串，要么为 "是"。                    |
| INDEX_TYPE    | varchar(11)  | 索引类型，比如BTree索引                                            |
| COMMENT      | varchar(8)   | 关于索引的额外信息。                                                |
| IS_VISIBLE    | varchar(3)   | 索引是可见（是）还是不可见（否）。                                    |
| EXPRESSION   | longtext     | 对于功能索引，此列包含用于生成索引值的表达式。对于非功能索引，该值始终为空。 |

STATISTICS 视图不仅对索引统计有用，而且对索引本身也很有用，它包含了所有索引的信息，与索引类型无关。例如，你可以用
它来查找不可见索引和用于功能索引的表达式。关于索引统计，最有趣的列是 CARDINALITY，它是索引中估计存在的唯一值的数
量。

查询 STATISTICS 视图时，建议按照 TABLE_SCHEMA、TABLE_NAME、INDEX_NAME 和 SEQ_IN_INDEX 列对结果进行排序。
这将把相关行归类在一起，对于多列索引，将按照索引中列的顺序返回行。清单 15-4 显示了 world.countrylanguage 表索
引的示例。在这种情况下，由于表模式和表名是固定的，因此排序只取决于索引名和索引中的顺序。由于值的性质并不精确，因此
结果可能会有所不同。

#+begin_src sql
  select INDEX_NAME, NON_UNIQUE,
         SEQ_IN_INDEX, COLUMN_NAME,
         CARDINALITY, INDEX_TYPE,
         IS_VISIBLE
  from information_schema.STATISTICS
  where TABLE_SCHEMA='world' and
       TABLE_NAME='countrylanguage'
   order by INDEX_NAME, SEQ_IN_INDEX\G
#+end_src

countrylanguage 表有两个索引。CountryCode 和 Language 列上有一个主键，CountryCode 单独有一个辅助索引。在
mysql.innodb_index_stats 表中，主键被附加到二级非唯一索引的时间也有记录，但 STATISTICS 视图与此不同，不包含
该信息。

#+begin_comment
  由于 CountryCode 列是主键中的第一列，因此仅在 CountryCode 列上使用二级索引是多余的。最佳做法是避免冗余索引。
#+end_comment

您可能希望在统计视图中记录数据，并比较数据随时间的变化情况。突然的变化可能表明数据发生了意外，或者最新的索引统计重
新计算会导致不同的查询计划。

STATISTICS 视图中的某些信息也可通过 SHOW INDEX 语句获得。

** show index语句

SHOW INDEX 语句是获取 MySQL 中索引信息的最初方法。如今，它与 information_schema. STATISTICS（统计）的相同
来源获取数据，因此你可以使用其中任何一种最适合你的方式。STATISTICS 视图的一个主要优点是，你可以选择需要哪些信息以
及如何排序；而使用 SHOW INDEX 语句时，你总是能获得单个表的索引，并根据可用字段进行排序和筛选。

SHOW INDEX 返回的列与 STATISTICS 视图中的列相同，只是省略了表目录、表模式和索引模式。另一方面，SHOW INDEX 可
以选择使用 EXTENDED 关键字，其中包括索引隐藏部分的信息。这不应与不可见索引混淆，而是附加部分，如附加到二级索引中
的主键。标准输出和扩展输出的共同记录信息相同。

清单 15-5 显示了针对 world.city 表的 SHOW INDEX 输出示例（结果假定第 14 章中的索引已被删除）。首先返回标准输
出，然后是扩展输出。由于扩展输出长达数页，因此通过删除部分列和行对其进行了缩写。要查看完整输出，请自行执行语句或查
看本书 GitHub 代码库中的 listing_15_5.txt 文件。

#+begin_src sql
  show index from world.city\G
#+end_src

请注意，列名与统计视图使用的列名并不相同。不过，列的顺序相同，名称相似，因此很容易将这两种输出映射到对方。

在扩展输出中，主键有两个 InnoDB 内部的隐藏列：DB_TRX_ID（6 字节事务标识符）和 DB_ROLL_PTR（7 字节回滚指针，
指向写入回滚段的撤销日志记录）。这两个字段是 InnoDB 多版本支持的一部分。1 在这两个内部字段之后，表中的其余每一列
都会被添加。这表明 InnoDB 对其行使用了聚类索引，因此主键就是行。

对于 CountryCode 的二级索引，主键现在显示为索引的第二部分。这在意料之中，也反映了在 mysql.innodb_index_stats
表中看到的情况。

虽然在研究性能问题时，扩展输出通常不会引起很大兴趣，但在探索 InnoDB 如何工作时，它却很有价值。

INNODB_TABLESTATS 视图是另一个在处理索引统计时非常有用的信息模式视图。

** Information_scheam  INNODB_TABLESTATS 视图

信息模式中的 INNODB_TABLESTATS 视图是 InnoDB 内部内存结构顶部的一个视图，其中包含有关索引的信息。该视图不包
含任何可用于验证索引的卡片性和大小的信息，这些信息没有包含在已经描述过的表和视图中。不过，它可以提供一些索引统计
状态的信息，以及自上次分析表以来的修改次数。该视图包含所有 InnoDB 表的信息，无论它们使用的是持久统计还是瞬时统
计。表 15-4 总结了 INNODB_TABLESTATS 视图的列。

#+NAME: INNODB_TABLESTATS
| 列名              | 数据类型          | 描述                                                                                   |
|------------------+-----------------+---------------------------------------------------------------------------------------|
| TABLE_ID         | bigint unsigned | 内部 InnodB 表 Id。例如，可以用它在 INNODB_ TABLES 信息模式视图中查找表。                     |
| NAME             | varchar(193)    | 表格名称，格式为 <schema>/<table>，例如，world/city。                                      |
| STAT_INITIALIZED | varchar(193)    | 内存结构是否已为表初始化，这与索引统计是否存在不同。                                           |
| NUM_ROWS         | bigint unsigned | 表格中的估计行数。                                                                       |
| CLUST_INDEX_SIZE | bigint unsigned | 聚类索引中的页数。                                                                       |
| OTHER_INDEX_SIZE | bigint unsigned | 二级索引页数的总和。                                                                      |
| MODIFIED_COUNTER | bigint unsigned | 自上次更新索引统计信息以来使用 DML 语句更改的记录数。                                         |
| ATUOINC          | bigint unsigned | 自动递增计数器的值。对于没有自动递增列的表，该值为 0。                                         |
| ERF_COUNT        | int             | 元数据的引用次数。当引用计数器的引用次数为零时，InnodB 可能会驱逐数据，初始化状态也会恢复为未初始化。 |

初始化状态可能会引起混淆。它显示索引统计信息和相关元数据（在此视图中显示）是否已加载到内存中。即使存在统计数据，状
态一开始也总是未初始化。当某个连接或后台线程需要这些数据时，InnoDB 会将其加载到内存中，此时状态就会变成
Initialized。每当没有线程持有对表的引用时，InnoDB 就会再次释放信息，状态也就变成了 "未初始化"。例如，当表被刷新
或对表执行 ANALYZE TABLE 时，就会发生这种情况。

修改计数器很有意思，因为它可以用来查看自上次更新索引统计信息以来有多少行被修改。只有当 DML 查询影响到索引时，计数
器才会增加。这就意味着，如果更新了非索引列，但保持行的原样，计数器就不会增加。该计数器与自动更新有关，当发生一定量
的更改时会触发自动更新。

清单 15-6 是 INNODB_TABLESTATS 视图对 world.city 表的输出示例。如果执行相同的查询，表 ID、行数和引用计数可
能会有所不同。

#+begin_src sql
  select *
  from information_schema.INNODB_TABLESTATS;
#+end_src

输出结果显示，索引统计信息是最新的，因为自上次分析以来没有修改过记录。集群索引和二级索引的行数和大小与使用
mysql.innodb_index_ stats 表找到的相同。这些与表大小相关的数字也用于 information_ schema.TABLES 视图和
SHOW TABLE STATUS 语句。

** information_schema中的TABLES视图和show table status

索引统计信息也用于填充 information_schema.TABLES 视图和 SHOW TABLE STATUS 语句所用表中的某些列。这包括估计
行数以及数据和索引的大小。

表 15-5 显示了 TABLES 视图中列的摘要。SHOW TABLE STATUS 语句的输出中除了 TABLE_CATALOG、TABLE_SCHEMA、
TABLE_TYPE 和 TABLE_COMMENT 列外，其他列都是相同的，有几列的名称略有不同。标有星号 (*) 的列受
information_schema_stats_expiry 变量的影响。

#+NAME: TABLES
| 列名             | 数据类型          | 描述                                                                                                                          |
|-----------------+-----------------+------------------------------------------------------------------------------------------------------------------------------|
| TABLE_CATALOG   | varchar(64)     | 表格所属的目录。                                                                                                                |
| TABLE_SCHEMA    | varchar(64)     | 表格所在的模式。                                                                                                                |
| TABLE_NAME      | varchar(64)     | 表名                                                                                                                          |
| TABLE_TYPE      | enum            | 可能的值有基础表、视图和系统视图。基础表通过 CREATE TABLE 创建，视图通过 CREATE VIEW 创建，系统视图是视图，如 MySQL 创建的信息模式视图。      |
| ENGINE          | varchar(64)     | 表所用存储引擎                                                                                                                  |
| VERSION         | int             | 在 MySQL 8 中未使用，因为它与 MySQL 5.7 及更早版本中的 .frm 文件有关。版本值现在硬编码为 10。                                           |
| ROW_FORMAT      | enum            | 表格使用的行格式。可能的值有Fixed、Dynamic、Compressed、Redundant、Compact和Paged。                                                 |
| TABLE_ROWS      | bigint unsigned | 估计行数。对于 InnodB 表，这来自主键或聚类索引的卡长度。                                                                             |
| AVG_ROW_LENGTH  | bigint unsigned | 估计数据长度除以估计行数。                                                                                                        |
| DATA_LENGTH     | bigint unsigned | 行数据的估计大小。对于 InnodB，它是聚类索引的大小，即聚类索引中的页数乘以页大小。                                                         |
| MAX_DATA_LENGTH | bigint unsigned | 允许的最大数据长度。InnodB 不使用该值，因此该值为 NULL。                                                                             |
| INDEX_LENGTH    | bigint unsigned | 二级索引的估计大小。对于 InnodB，这是非聚类索引中的页面总和乘以页面大小。                                                               |
| DATA_FREE       | bigint unsigned | 表所属表空间中可用空间的估计值。对于 InnodB，这是完全可用的扩展空间大小减去安全系数。                                                     |
| AUTO_INCREMENT  | bigint unsigned | 表格自动递增计数器的下一个值。                                                                                                    |
| CREATE_TIME     | timestamp       | 表创建的时间                                                                                                                   |
| UPDATE_TIME     | datetime        | 表空间文件最后一次更新的时间。对于 InnodB 系统表空间中的表，该值为 NULL。由于数据是异步写入表空间的，因此时间一般不会反映最后一次更改数据的语句的时间。 |
| CHECK_TIME      | datetime        | 上次检查表的时间（CHECK TABLE）。对于分区表，InnodB 总是返回 NULL。                                                                  |
| TABLE_COLLATION | varchar(64)     | 用于对字符串列的值进行排序和比较的默认校对方式（如果没有为列明确设置）。                                                                 |
| CHECKSUM        | bigint          | 表格校验和。InnodB 不使用该值，因此该值为 NULL。                                                                                   |
| CREATE_OPTIONS  | varchar(256)    | 表选项，如 STATS_AUTO_RECALC 和 STATS_SAMPLE_PAGES。                                                                            |
| TABLE_COMMENT   | text            | 创建表格时指定的注释。                                                                                                                             |

在可用信息中，数据和索引的行数和大小与索引统计关系最为密切。TABLES 视图不仅可用于查询表大小的估计值，还可用于查询
哪些表明确设置了持久性统计变量。清单 15-7 显示了一个 chapter_15.t1 表示例，该表填充了整整 100 万行，然后查询了
该表的 TABLES 视图内容。

#+begin_src sql
  create table chapter_15.t1(
    id int unsigned not null auto_increment,
    val varchar(36) not null,
    primary key(id)
  ) STATS_PRESISTENT=1,
    STATS_SAMPLE_PAGES=50,
    STATS_AUTO_RECALC=1;
#+end_src

#+begin_src sql
  set session ctx_max_recursion_depth=1000000;
#+end_src

#+begin_src sql
  start transaction;
  insert into chapter_15.t1(val)
  with recursive seq(i) as (
    select 1
      union all
    select i+1
      from seq where i<1000000
  )
  select uuid() from seq;
  commit;
#+end_src

#+begin_src sql
  select *
    from information_schema.TABLES
    where TABLE_SCHEM='chapter_15'
    and TABLE_NAME='t1'\G
#+end_src

该表使用递归普通表表达式填充随机数据，以确保准确插入 100 万行。为此，有必要将 cte_max_recursion_depth 设置为
1000000，否则普通表表达式会因递归深度过高而失败。

请注意，估计的行数只有 996442 行，比实际行数少了约 0.3%。这在预期范围内--10% 或更大的差异并不罕见。该表还设置了
几个表选项，以明确配置表使用持久统计，并启用自动计算和使用 50 个样本页。

如果希望使用 SHOW TABLE STATUS 语句，可以不带参数地使用该语句，这样就会返回默认模式中所有表的状态。或者，也可以
添加 LIKE 子句，只包含表的子集。要检索非默认模式中表的状态，可使用 FROM 子句指定模式名称。例如，将 "世界 "模式视
为默认模式，那么以下查询都将返回城市表的表状态：

#+begin_src sql
  use world
  show table status like 'city';
  show table status like 'ci%';
  show table status from world like 'city';
#+end_src

前两个查询依赖默认模式来知道在哪里查找表。第三个查询明确地在世界模式中查找城市表。

如果指数统计没有数据了，如何更新？这是本章结束前要探讨的最后一个话题。

** 更新统计

最新的索引统计信息对于优化器获得最佳查询执行计划非常重要。索引更新有两种方式：自动更新，因为表有足够的变化来触发统
计数据的重新计算；手动更新。

** 自动更新

在介绍持久性和瞬时统计时，我们已经在一定程度上讨论了自动更新机制。表 15-6 总结了基于索引统计类型的功能。

#+NAME: InnoDB 索引统计自动重新计算摘要
| 属性                          | 持久                                                  | 暂存                               |
|------------------------------+------------------------------------------------------+-----------------------------------|
| 缓存行                        | 10%的表                                               | 6.25%的表                          |
| 因记录更改而进行更新的最短间隔时间 | 10秒                                                  | 16次更新                           |
| 其他动作触发的更改              |                                                      | 表的首次打开，查询表元数据时可选择使用。 |
| 后台更新                       | 是                                                    | 否                                 |
| 配置                          | innodb_stats_auto_recalc变量和STATS_AUTO_ RECALC表选项 | 空                                 |

总结显示，持久统计数据的更新频率一般较低，影响也较小，因为自动更新是在后台进行的。持久性统计数据也有更好的配置选项。

也可以手动触发索引统计更新。你可以使用 ANALYZE TABLE 语句或 mysqlcheck 命令行程序，这将在接下来的章节中讨论。

** analyze table语句

在使用 mysql 命令行客户端或 MySQL Shell 或由存储过程触发更新时，使用 ANALYZE TABLE 语句非常方便。该语句既可
以更新索引统计信息，也可以更新直方图。后者将在下一章讨论，因此这里只涉及索引统计信息的更新。

ANALYZE TABLE 有一个参数，即是否将语句记录到二进制日志中。如果在 ANALYZE 和 TABLE 之间指定
NO_WRITE_TO_BINLOG 或 LOCAL，语句将只应用于本地实例，而不会写入二进制日志。

执行 ANALYZE TABLE 时，会强制刷新索引统计信息和表缓存值，否则这些值将受制于 information_schema_stats_expiry
变量。因此，如果强制更新了索引统计信息，则无需更改 information_schema_stats_expiry，就能让
information_schema.STATISTICS 视图同样反映更新后的值。

可以选择指定多个表更新其索引统计信息。为此，可以用逗号分隔的列表列出这些表。更新世界模式中三个表的统计信息的示例见
清单 15-8。

#+begin_src sql
  analyze local table
    world.city, world.country,
    world.countrylanguage\G
#+end_src

在示例中，使用 LOCAL 关键字是为了避免将语句记录到二进制日志中。如果不在指定表名的同时指定模式名（例如，用 city
代替 world.city），MySQL 将在当前默认模式中查找表。

#+begin_comment
  虽然可以使用 ANALYZE TABLE 同时查询表，但请注意，作为最后一步（返回客户端后），分析过的表将被刷新（隐式 FLUSH
  TABLES 语句）。表刷新只能在所有正在进行的查询完成后进行，因此在进行长时间查询时，不应使用 ANALYZE TABLE（或
  mysqlcheck）。
#+end_comment

ANALYZE TABLE 语句非常适合临时更新，以及准确知道要分析哪些表的情况。对于分析给定模式中的所有表或实例中的所有表，
它的作用就没那么大了。为此，接下来讨论的 mysqlcheck 是更好的选择。

** mysqlcheck程序

如果你想通过 shell 脚本、cron 守护进程或 Windows 任务调度程序触发更新，那么 mysqlcheck 程序就非常方便。它不仅
可以像 ANALYZE TABLE 那样用于更新单个表或多个表的索引统计信息，还可以让 mysqlcheck 更新模式中所有表或实例中所
有表的索引统计信息。mysqlcheck 所做的是为符合你的条件的表执行 ANALYZE TABLE，因此从索引统计的角度来看，手动执行
ANAYZE TABLE 和使用 mysqlcheck 没有什么区别。

#+begin_comment
  mysqlcheck 程序的功能远不止分析表以更新索引统计。这里只介绍分析功能。要阅读 mysqlcheck 程序的完整文档，请参
  阅 https://dev.mysql.com/doc/refman/en/mysqlcheck.html。
#+end_comment

使用--analyze选项可以让mysqlcheck更新索引统计，而--write-binlog/-skip-write-binlog参数可以告诉你是否
要将语句记录到二进制日志中。默认情况下记录语句。你还需要说明如何连接到 MySQL；为此需要使用标准连接选项。

有三种方法可以指定要分析的表。默认情况下，分析同一模式下的一个或多个表，就像分析 TABLE 语句一样。如果选择这种方式，
就不需要添加任何额外选项，指定的第一个值将被解释为模式名称，可选参数将被解释为表名称。清单 15-9 展示了如何以两种方
式分析世界模式中的所有表：明确列出表名和不列出表名。

如果你想通过 shell 脚本、cron 守护进程或 Windows 任务调度程序触发更新，那么 mysqlcheck 程序就非常方便。它不仅
可以像 ANALYZE TABLE 那样用于更新单个表或多个表的索引统计信息，还可以让 mysqlcheck 更新模式中所有表或实例中所
有表的索引统计信息。mysqlcheck 所做的是为符合你的条件的表执行 ANALYZE TABLE，因此从索引统计的角度来看，手动执行
ANAYZE TABLE 和使用 mysqlcheck 没有什么区别。

#+begin_comment
  mysqlcheck 程序的功能远不止分析表以更新索引统计。这里只介绍分析功能。要阅读 mysqlcheck 程序的完整文档，请参
  阅 https://dev.mysql.com/doc/refman/en/mysqlcheck.html。
#+end_comment
  
使用--analyze选项可以让mysqlcheck更新索引统计，而--write-binlog/-skip-write-binlog参数可以告诉你是否
要将语句记录到二进制日志中。默认情况下记录语句。你还需要说明如何连接到 MySQL；为此需要使用标准连接选项。

有三种方法可以指定要分析的表。默认情况下，分析同一模式下的一个或多个表，就像分析 TABLE 语句一样。如果选择这种方式，
就不需要添加任何额外选项，指定的第一个值将被解释为模式名称，可选参数将被解释为表名称。清单 15-9 展示了如何以两种方
式分析world模式中的所有表：明确列出表名和不列出表名。

#+begin_src sh
  mysqlcheck --user=root --password --host=localhost --port=3306 --analyze world city country countrylanguage
#+end_src

#+begin_src sh
  mysqlcheck --user=root --password --host=localhost --analyze world
#+end_src

在这两种情况下，输出结果都会列出分析过的三个表格。

如果要分析多个模式中的所有表，但仍要列出包括哪些模式，可以使用 --databases 参数。使用该参数时，命令行中列出的所
有对象名称都会被解释为模式名称。清单 15-10 显示了分析 sakila 和 world 模式中所有表的示例。

#+begin_src sh
  mysqlcheck --user=root --password --host=localhost --port=3306 --analyze --database sakila world
#+end_src

最后一个选项是使用 --all-databases 选项来分析所有表，而不管它们位于哪个模式中。除了信息模式和性能模式外，这也包
括系统表。清单 15-11 显示了使用 mysqlcheck 和 --all-databases 的示例。

注意有两张表回复说它们的存储引擎不支持分析。mysqlcheck 程序会尝试分析所有表，而不管它们的存储引擎如何，因此出现示
例中这样的信息是意料之中的。mysql.general_log 和 mysql.slow_log 表默认使用的都是 CSV 存储引擎，它不支持索引，
因此也不支持 ANALYZE TABLE。

** 总结

本章接着上一章的内容，介绍 InnoDB 如何处理索引统计。InnoDB有两种存储统计信息的方式：在
mysql.innodb_index_stats表和mysql.innodb_table_stats表中持久存储，或者在内存中临时存储。一般来说，持
久性统计更受欢迎，因为它们能提供更一致的查询计划，允许对更多页面进行采样，在后台更新，而且可以在更大程度上进行配置，
包括支持表级选项。

有几个表、视图和 SHOW 语句可用于调查和了解 InnoDB 索引及其统计信息。其中，information_schema.STATISTICS 视
图具有 MySQL 中所有索引的详细信息。此外，还讨论了 information_schema.INNODB_TABLESTATS 和
information_schema.TABLES 视图、SHOW INDEX 和 SHOW TABLE STATUS 语句。

更新索引统计信息有两种方法：使用 ANALYZE TABLE 语句或 mysqlcheck 程序。前者适用于交互式客户端或存储过程，而后
者更适用于 shell 脚本和更新一个或多个模式中的所有表。这两种方法都会强制更新 MySQL 数据字典中表元数据和索引卡入度
的缓存值。

在讨论 ANALYZE TABLE 语句时，有人提到 MySQL 还支持直方图。这与索引有关，是下一章的主题。

* 直方图

在前两章中，我们学习了索引和索引统计。索引的目的是减少访问查询所需记录的读取次数，而索引统计则是为了帮助优化器确定
最佳查询计划。这些都很好，但索引并不是免费的，在有些情况下，索引并不十分有效，也不值得花费大量的开销，但优化器仍然
需要了解数据的分布情况。这就是直方图的作用所在。

本章首先讨论什么是直方图以及直方图对哪些工作负载有用。然后介绍直方图的实用性，包括添加、维护和检查直方图数据。最后，
以一个查询为例，说明添加直方图后查询计划会发生哪些变化。

** 直方图是什么

直方图支持是 MySQL 8 中的一项新功能，它可以分析和存储表中数据分布的相关信息。虽然直方图与索引有一些相似之处，但它
们并不相同，你可以为没有任何索引的列设置直方图。

创建直方图时，要告诉 MySQL 将数据划分成桶。具体做法是在每个桶中放入一个值，或者在每个桶中放入大致相同数量行的值。
有关数据分布的知识可以帮助优化器更准确地估计给定的 WHERE 子句或连接条件将过滤掉表中多少数据。举例来说，如果不了解
这些知识，优化器可能会假定某个条件会返回表中三分之一的数据，而直方图可能显示只有 5%的行符合条件。这些知识对于优化器
选择最佳查询计划至关重要。

同时，必须认识到直方图与索引不同。与不使用直方图的查询计划相比，MySQL 无法使用直方图来减少使用直方图的表所检查的
行数。不过，如果知道表中有多少内容会被过滤，优化器就能更好地确定最佳连接顺序。

直方图的一个优点是，只有在创建或更新直方图时才会产生成本。与索引不同，在更改数据时，直方图不会发生任何变化。您可能
会不时重新创建直方图，以确保统计信息是最新的，但 DML 查询不会产生任何开销。一般来说，直方图应与索引统计进行比较，
而不是与索引进行比较。

#+begin_comment
  了解索引和直方图之间的根本区别非常重要。索引可以用来减少访问所需记录所需的工作量，而直方图则不能。在查询中使用直
  方图时，直方图不会直接减少检查行的数量，但可以帮助优化器选择更优化的查询计划。
#+end_comment

就像为索引添加直方图一样，在为哪一列添加直方图时也要谨慎选择。因此，让我们来讨论一下哪些列应被视为候选列。

** 什么时候应该添加直方图

添加直方图的重要因素是在正确的列中添加直方图。简而言之，直方图对于那些不是索引中的第一列、值分布不均匀以及需要对这
些列应用条件的列最有用。这听起来可能是一个非常有限的用例，事实上，直方图在 MySQL 中的用处不如在其他一些数据库中那
么大。这是因为 MySQL 在估算索引列范围内的行数时效率很高，因此直方图不会与同一列上的索引一起使用。还要注意的是，虽
然直方图对数据分布不均匀的列特别有用，但在不值得添加索引的情况下，直方图对数据分布均匀的列也很有用。

#+begin_comment
  不要为索引中的第一列添加直方图。对于在索引中出现较晚的列，直方图对于查询仍有价值，因为在查询中，由于需要使用索引
  的左前缀，因此不能对该列使用索引。
#+end_comment

尽管如此，在有些情况下，直方图仍然可以大大提高查询性能。一个典型的用例是查询中包含一个或多个连接，以及数据分布不均
匀的列上的一些次要条件。在这种情况下，直方图可以帮助优化器确定最佳连接顺序，以便尽早过滤掉尽可能多的行。

数据分布不均匀的一些例子包括状态值、类别、时间、工作日和价格。状态列中可能有大量行处于 "已完成 "或 "失败 "等终端
状态，而只有少数值处于工作状态。同样，产品表中某些类别的产品可能多于其他类别。由于某些事件在某些时间或日子比其他时
间或日子更有可能发生，因此时间和工作日的值可能并不统一。例如，球赛发生的工作日可能（取决于运动项目）在周末比在工作
日更有可能发生。在价格方面，大多数产品的价格范围可能相对较窄，但最低和最高价格却远远超出这个范围。选择性低的列有枚
举数据类型的列、布尔值和其他只有几个唯一值的列。

与索引相比，直方图的一个优点是，在确定范围内的行数（例如，对于较长的 IN 子句或许多 OR 条件）时，直方图比索引潜入
更省钱。原因在于优化程序可以随时获得直方图统计数据，而估计范围内行数的索引挖掘是在确定查询计划时进行的，因此每次查
询都会重复。

#+begin_comment
  对于索引列，当存在 eq_range_index_dive_ limit（默认值为 200）或更多相等范围时，优化器将从执行相对昂贵但非常
  精确的索引下潜转换为仅使用索引统计信息来估计匹配行的数量。
#+end_comment

您可以争辩说，既然可以添加索引，为什么还要使用直方图，但请记住，在数据发生变化时维护索引并非没有代价。执行 DML 查
询时需要维护索引，而且索引会增加表空间文件的大小。此外，范围（包括相等范围）中值的数量统计是在执行查询的优化阶段即
时计算的。也就是说，它们是根据每次查询的需要进行计算的。而直方图只是存储统计信息，只有在明确要求时才会更新。直方图
统计信息也随时可供优化程序使用。

总之，符合以下条件的列是直方图的最佳候选列：

+ 数据分布不均匀，或者数据值过多，以至于优化器的粗略估计（将在下一章讨论）无法很好地估计数据的选择性。
+ 选择性差（否则指数可能是更好的选择）。
+ 用于在 WHERE 子句或连接条件中过滤表中的数据。如果不对列进行筛选，优化器就无法使用直方图。
+ 具有稳定的数据分布。直方图统计信息不会自动更新，因此如果在数据分布经常变化的列上添加直方图，直方图统计信息很可能
  不准确。存储事件日期和时间的列就是一个不适合使用直方图的典型例子。

这些规则有一个例外，那就是可以使用直方图统计来替代昂贵的查询。如 "检查直方图数据 "部分所示，可以查询直方图统计数据，
因此如果只需要数据分布的近似结果，可以用直方图统计数据来代替查询。

#+begin_comment
  如果您的查询需要确定给定范围内的数值数量，并且只需要近似值，那么即使您不打算使用直方图来改进查询计划，也可以考虑创
  建直方图。
#+end_comment

由于直方图存储的是列中的值，因此不允许在加密表中添加直方图。否则，加密数据可能会无意中以未加密的方式写入磁盘。此外，
临时表也不支持直方图。

为了以最佳方式应用直方图，您需要了解一些直方图的内部工作原理，包括支持的直方图类型。

** 直方图原理
为了有效地使用直方图，有必要了解一些与直方图有关的内部知识。您应该了解的概念包括桶、累积频率和直方图类型。本节将逐
一介绍这些概念。

*** 桶
创建直方图时，值会被分配到桶中。每个桶可能包含一个或多个不同的值，MySQL 会计算每个桶的累积频率。因此，桶的概念非
常重要，因为它与直方图统计的准确性密切相关。

MySQL 支持多达 1024 个数据桶。存储桶越多，每个存储桶中的值就越少，因此存储桶越多，每个值的统计数据就越准确。在
最好的情况下，每个存储桶中只有一个值，这样就能 "准确"（在统计数据准确的范围内）知道该值的行数。如果每个数据桶有一
个以上的值，则要计算值范围的行数。

在这种情况下，理解什么是独特的值非常重要。对于字符串，在比较值时只考虑前 42 个字符，对于二进制值，只考虑前 42
个字节。如果有长字符串或具有相同前缀的二进制值，直方图可能无法正常工作。

#+begin_comment
  仅使用字符串的前 42 个字符和二进制对象的前 42 个字节来确定直方图中存在的值。
#+end_comment

值是按顺序添加的，因此，如果从左到右排列水桶并检查某个水桶，就可以知道左边的所有水桶的值都较小，而右边的所有水桶
的值都较大。图 16-1 展示了数据桶的概念。

[[./images/hvRkxf.png]]

在图中，前面的深色列是每个桶中数值的频率。频率是具有该值的行的百分比。背景中（颜色较亮的列）是累积频率，其值与第
0 桶中的计数列相同，然后逐渐增加，直到第 7 桶中的计数列达到 100。什么是累积频率？这是您应该了解的直方图的第二个
概念。

*** 累计频率

数据桶的累积频率是指当前数据桶和之前数据桶中记录的百分比。如果您查看的是第 3 个数据桶，累积频率为 50%，那么就有
50%的行位于第 0、1、2 和 3 个数据桶中。这样，优化器就可以通过直方图轻松确定列的选择性。

计算选择性时需要考虑两种情况：相等条件和范围条件。对于相等条件，优化器会先确定条件值在哪个数据桶中，然后求出该数
据桶的累计频率，再减去上一个数据桶的累计频率（对于数据桶 0，不会减去任何值）。如果桶中只有一个值，就只需要这样
做。否则，优化器会假定数据桶中的每个值都以相同频率出现，因此数据桶的频率要除以数据桶中值的个数。

对于范围条件，其工作方式非常相似。优化器会找到边缘条件所在的数据桶。例如，当 val < 4 时，就会找到值为 4 的数据
桶。使用的累积频率取决于数据桶中的数值数量和条件类型。与相等条件一样，对于多值水桶，累积频率也是通过假设水桶中的
值分布相等来计算的。根据条件类型，累积频率的使用方法如下：

+ 小于： 使用前一个值的累积频率。
+ 小于或等于： 使用条件中数值的累积频率。
+ 大于或等于： 前一数值减去 1 的累积频率。
+ 大于： 条件中数值的累计频率减去 1。

这意味着，通过使用累计频率，最多只需考虑两个桶即可确定条件对表中记录的过滤效果。看一个示例可以更好地理解累积频率
的确切工作原理。表 16-1 显示了一个直方图的示例，该直方图中每个桶都有一个值，每个桶都有累计频率。  

| Bucket | Value | Cumulative Frequency |
|--------+-------+----------------------|
|      0 |     0 |                  0.1 |
|      1 |     1 |                 0.25 |
|      2 |     2 |                 0.37 |
|      3 |     3 |                 0.55 |
|      4 |     4 |                 0.63 |
|      5 |     5 |                 0.83 |
|      6 |     6 |                 0.95 |
|      7 |     7 |                  1.0 |
|        |       |                      |

在本例中，数值与数据桶的编号相同，但一般情况下并非如此。累积频率从 0.1（10%）开始，随着每个数据桶中行数百分比的
增加而增加，直到最后一个数据桶达到 100%。这种分布与图 16-1 中的分布相同。

如果将五种条件类型与数值 4 进行比较，那么每种类型估计的行数如下：

+ val=4：从水桶 4 的累积频率中减去水桶 3 的累积频率：估计值 = 0.63 - 0.55 = 0.08。因此，估计有 8%的行被包
  含在内。
+ val<4：使用第 3 桶的累积频率，因此估计包含 55% 的行。
+ val<=4：使用桶 4 的累积频率，因此估计有 63% 的行被包含在内。
+ val>=4：从 1 中减去第 3 桶的累积频率，因此估计有 45% 的行被包含在内。
+ val>4：桶 4 的累积频率减去 1，因此估计有 37% 的行被包含在内。

如果每个桶中包含一个以上的值，情况就会变得复杂一些。表 16-2 显示了相同的表格和数值分布，但这次直方图只有四个桶，
因此平均每个桶有两个数值。  

| Bucket | Values | Cumulative Frequency |
|--------+--------+----------------------|
|      0 |    0-1 |                 0.25 |
|      1 |    2-3 |                 0.55 |
|      2 |    4-5 |                 0.83 |
|      3 |    6-7 |                  1.0 |

在这种情况下，每个数据桶中恰好有两个值，但一般来说情况并非如此（在讨论直方图类型时会有更多介绍）。现在在评估同样
的五个条件时，需要考虑到每个数据桶都包含了对不止一个值的行数的估计：

+ val=4：用桶 1 的累计频率减去桶 2 的累计频率，然后将结果除以桶 2 中的数值：估计值 = (0.83 - 0.55)/2 =
  0.14。因此，估计有 14% 的行被包含在内。由于值 4 和值 5 的频率是一起考虑的，因此这个估计值要高于每桶一个值的
  更精确估计值。
+ val<4：桶 1 的累积频率是唯一需要的，因为桶 0 和桶 1 包含了所有小于 4 的值。因此，估计将包含 55% 的记录（这
  与上一个示例相同，因为在这两种情况下，估计只需要考虑完整的桶）。
+ val<=4：这就比较复杂了，因为有一半的数据桶 2 中的值被包含在筛选中，另一半没有。因此，估计值将是水桶 1 的累计
  频率加上水桶 2 的频率，再除以水桶中的数值个数：估计值 = 0.55 + (0.83 - 0.55)/2 = 0.69 或 69%。与使用每
  个水桶一个值的估计值相比，该估计值更高且更不准确。估计值不够准确的原因是假设值 4 和值 5 具有相同的频率。
+ val>=4：该条件要求 2 号和 3 号水桶中的所有值，因此估计值为 1 减去 1 号水桶的累积频率；即 45%--与每个水桶只
  有一个值的情况下的估计值相同。
+ val > 4：这种情况与 val <= 4 类似，只是要包含的值正好相反，因此可以用 0.69 减去 1，得到 0.31 或 31%。同
  样，由于涉及两个数据桶，估算结果不如每个数据桶的单个值那么准确。

正如您所看到的，将数值分配到桶中有两种不同的情况：一种是桶的数量至少和数值一样多，每个数值都可以分配到自己的桶，
另一种是多个数值共享一个桶。这是两种不同类型的直方图，具体细节将在下文讨论。

** 直方图类型

MySQL 8 中有两种直方图类型，创建或更新直方图时，会根据值是否多于桶自动选择直方图类型。这两种直方图类型是

+ Singleton: 对于单子直方图，每个柱状图都有一个值。这是最精确的直方图，因为在创建直方图时，每个值都有一个估计值。
+ Equi-height: 当列的值多于桶的数量时，MySQL 将分配这些值，因此每个桶的行数大致相同，也就是说，每个桶的高度大致
  相同。由于所有具有相同值的行都被分配到同一个桶中，因此桶的高度不会完全相同。对于等高直方图，每个数据桶所代表的数
  值数量是不同的。

在探索累积频率时，我们已经接触过这两种直方图类型。单子直方图是最简单、最准确的直方图，而等高直方图则是最灵活的直方
图，因为它可以处理任何数据集。

要演示单例直方图和等高直方图，可以从 world.city 表中创建 city_ 直方图表，其中包含基于八个国家代码的城市子集。可
以使用以下查询创建该表：

#+begin_src sql
  use world
  create table city_histogram like city;
  insert into city_histogram
  select *
    from city
  where CountryCode in ('AUS', 'BRA', 'CHN', 'DEU',
                        'FRA', 'GBR', 'IND', 'USA');
#+end_src

图 16-2 显示了国家代码列的单列直方图示例。由于有八个值，所以直方图有八个桶。(您将在本章后面学习如何创建和检索直方
图统计数据）。

[[./images/D1WO1k.png]]

直方图中的每个桶正好有一个值。频率范围从澳大利亚 (AUS) 的 1.0% 到中国 (CHN) 的 24.9%。在这个例子中，如果国家代
码列上没有索引，直方图可以极大地帮助我们更准确地估计筛选结果。原始的 world.city 表有 232 个不同的 CountryCode
值，因此单列直方图效果很好。

图 16-3 显示了相同数据的等高直方图，但统计量只有四个桶。

[[./images/HIuSd0.png]]

对于等高直方图，MySQL 的目标是每个桶的频率（高度）相同。但是，由于一列的值会全部出现在一个桶中，而且值是按顺序分
布的，因此一般不可能获得完全相同的高度。本例中的情况也是如此，数据桶 0 和 3 的频率略低于数据桶 1 和 2。

该图还显示了等高直方图的一个缺点。巴西 (BRA)、中国 (CHN) 和印度 (IND) 城市的高频率在一定程度上被与它们共享数据
桶的国家的低频率所掩盖。因此，等高直方图的准确性不如单子直方图。当数值的频率变化很大时，情况尤其如此。一般来说，等
高直方图在等高条件下比在等距条件下更容易出现精度降低的问题，因此等高直方图最适合主要用于等距条件的列。

在使用直方图统计之前，需要先创建直方图统计，创建后还需要维护直方图统计。如何维护是下一节的主题。

** 添加和维护直方图

直方图只作为统计信息存在，不像索引那样在表空间中实际存在。因此，使用 ANALYZE TABLE 语句创建、更新和删除直方图也
就不足为奇了，该语句也用于更新索引统计信息。该语句有两种变体：更新和删除统计数据。创建和更新直方图时，还需要注意采
样率。本节将逐一介绍这些主题。

** 创建和更新直方图

通过在分析表语句中添加 UPDATE HISTOGRAM 子句，可以创建或更新直方图。如果没有统计信息，并且有更新请求，则创建直
方图；否则，替换现有直方图。您需要指定要将统计数据划分为多少个桶。

要在 sakila.film 表的长度列中添加最多使用 256 个存储桶的直方图（长度以分钟为单位，因此 256 个存储桶应该足以确
保单个直方图），可以使用类似下面示例的语句：

#+begin_src sql
  analyze table sakila.film
    update histogram on length
      with 256 buckets\G
#+end_src

可以选择在 ANALYZE 和 TABLE 之间添加 NO_WRITE_TO_BINLOG 或 LOCAL 关键字，以避免将语句写入二进制日志。这种方
法与更新索引统计时的方法相同。

#+begin_comment
  如果不想将 ANALYZE TABLE 语句写入二进制日志，请添加 NO_WRITE_TO_BINLOG 或 LOCAL 关键字，例如 ANALYZE
  LOCAL TABLE ...
#+end_comment

当 ANALYZE TABLE（分析表）无差错地完成直方图创建时，Msg_type 将等于 status，Msg_text 显示已创建直方图统计数
据以及创建的列。如果出现错误，Msg_type 将等于 Error，Msg_text 将解释问题所在。例如，如果您尝试为一个不存在的列
创建直方图，则错误显示与本例类似：

#+begin_src sql
  analyze table sakila.film
    update histogram on len
      with 256 buckets\G
#+end_src

当 ANALYZE TABLE（分析表）无差错地完成直方图创建时，Msg_type 将等于 status，Msg_text 显示已创建直方图统计数
据以及创建的列。如果出现错误，Msg_type 将等于 Error，Msg_text 将解释问题所在。例如，如果您尝试为一个不存在的列
创建直方图，则错误显示与本例类似：

#+begin_src sql
  analyze table sakila.film
    update histogram on length, rating
      with 256 buckets\G
#+end_src

应该选择多少个数据集？如果唯一值少于 1024 个，建议使用足够多的数据桶来创建单例直方图（即至少与唯一值一样多的数据
桶）。如果选择的数据桶数量多于值的数量，MySQL 将只使用存储每个值的频率所需的数据桶。从这个意义上说，数据桶的数量应
视为要使用的最大数据桶数量。

如果有超过 1024 个不同的值，则需要足够多的数据桶才能很好地表示数据。25 到 100 个数据桶通常是一个很好的起点。如果
有 100 个数据桶，等高直方图的每个数据桶中平均会有 1%的行。行的分布越均匀，所需的桶数就越少，而分布差异越大，所需的
桶数就越多。目标是将最常出现的值放在自己的数据桶中。例如，在上一节使用的 world.city 表子集中，五个数据桶分别将中
国 (CHN)、印度 (IND) 和美国放在各自的数据桶中。

直方图是通过对数值进行采样创建的。如何取样取决于可用内存的大小。

** 采样

MySQL 创建直方图时，需要读取记录以确定可能的值及其频率。这种方法与索引统计的采样方法类似，但又有所不同。计算索引
统计时，要确定唯一值的数量，这是个简单的任务，因为只需要计数。因此，您只需指定要采样的页面数量即可。

对于直方图，MySQL 不仅要确定不同值的数量，还要确定它们的频率，以及如何将值分配到桶中。因此，采样值会被读入内存，
然后用于创建数据桶和计算直方图统计数据。这意味着，更自然的做法是指定可用于采样的内存大小，而不是页数。根据可用内存
量，MySQL 将决定可以采样多少页。

#+begin_comment
  在 mysQL 8.0.18 及更早版本中，总是需要全表扫描。在 mysQL 8.0.19 及更高版本中，InnoDB 可以自己直接执行采样，
  因此可以跳过采样中不会用到的页面。这使得大型表的采样效率大大提高。
#+end_comment

在 ANALYZE TABLE ... UPDATE HISTOGRAM ...语句中，可用内存由 histogram_generation_max_mem_size 选项指
定。默认值为 20,000,000 字节。在 "检查直方图数据 "一节中讨论的 information_schema.COLUMN_STATISTICS 视图
包含有关所产生的采样率的信息。如果没有获得预期的过滤精度，可以检查采样率，如果采样率较低，可以增加
histogram_generation_max_mem_size 的值。采样页数与可用内存量呈线性关系，而桶数则不会对采样率产生任何影响。

** 删除直方图

如果确定不再需要直方图，可以再次删除它。就像更新直方图统计一样，可以在使用 DROP HISTOGRAM 子句的 ANALYZE TABLE
语句中放弃统计。可以在一条语句中放弃一个或多个直方图。清单 16-2 中显示了一个删除 sakila.film 表中长度和评分列直
方图的示例。本章后面的示例部分包括一个查询，可以用来查找所有现有的直方图。

#+begin_src sql
  analyze table sakila.fiml
    drop histogram on length, rating\G
#+end_src

ANALYZE TABLE 语句的输出类似于创建统计数据。还可以选择在 ANALYZE 和 TABLE 之间添加 NO_WRITE_TO_BINLOG 或
LOCAL 关键字，以避免将语句写入二进制日志。

有了直方图后，如何检查其中的统计数据和元数据？您可以使用信息模式来实现这一目的，具体方法将在下文中讨论。

** 检查直方图数据

当查询计划与预期不符时，了解优化器可以使用哪些信息非常重要。与索引统计信息的各种视图一样，信息模式也包含一个视图，
以便查看直方图统计信息。这些数据可通过 information_schema.COLUMN_STATISTICS 视图获取。下一节包括使用该视图
检索直方图信息的示例。

COLUMN_STATISTICS 视图是数据字典中包含直方图信息部分的视图。表 16-3 总结了四个列。

#+NAME: COLUMN_STATISTICS
| 列名         | 数据类型      | 描述              |
|-------------+-------------+------------------|
| SCHEMA_NAME | varchar(64) | 表所在的模式名      |
| TABLE_NAME  | varchar(64) | 直方图列所在的表格。 |
| COLUMN_NAME | varchar(64) | 直方图列           |
| HISTOGRAM   | json        | 直方图的详细情况    |

前三列（SCHEMA_NAME、TABLE_NAME、COLUMN_NAME）构成主键，可用于查询感兴趣的直方图。HISTOGRAM 列最有趣，因为
它存储了直方图的元数据和直方图统计信息。

直方图信息以 JSON 文档的形式返回，该文档包含多个对象，其中包括统计数据创建时间、采样率和统计数据本身等信息。
表 16-4 显示了文档中包含的字段。字段按字母顺序排列，可能与查询 COLUMN_STATISTICS 视图时所包含字段的顺序不同。

#+NAME: HISTOGRAM
| 字段名                       | JSON类型 | 描述                                                                                                                          |
|-----------------------------+---------+------------------------------------------------------------------------------------------------------------------------------|
| buckets                     | 数组     | 是一个数组，每个桶有一个元素。每个桶的可用信息取决于直方图类型，稍后会有说明。                                                            |
| collation-id                | 整型     | 该 id 与 INFORMATION_SCHEMA.COLLATIONS 视图中的 ID 列相同。                                                                      |
| data-type                   | 字符串   | 创建直方图的列中数据的数据类型。这不是 mysQL 数据类型，而是更通用的类型，如用于字符串类型的 "string"。 可能的值有 int、uint（无符号整数）、double、decimal、datetime 和 string。 |
| histogram-type              | 字符串   | 直方图类型                                                                                                                     |
| last-updated                | 字符串   | 统计数据最后更新的时间。格式为 YYYY-mm-dd HH:MM:SS.uuuuuu。                                                                        |
| null-values                 | 浮点型   | 采样值中为 NULL 的部分。该值介于 0.0 和 1.0 之间。                                                                                 |
| number-of-buckets-specified | 整型     | 请求的数据桶数量。对于单个直方图来说，这可能大于实际的桶数。                                                                           |
| sampling-rate               | 浮点型   | 表格中已采样页面的百分比。该值介于 0.0 和 1.0 之间。当值为 1.0 时，表示读取了整个表，统计数据是精确的。                                     |

该视图不仅可用于确定直方图统计数据，还可用于检查元数据，例如，确定统计数据上次更新的时间，并以此确保统计数据定期更
新。

buckets字段值得更多关注，因为它是存储统计数据的地方。它是一个数组，每个水桶有一个元素。每个桶的元素本身也是 JSON
数组。对于单个直方图，每个桶有两个元素，而对于等高直方图，每个桶有四个元素。

单子直方图包含的元素有

+ Index 0:桶的列值。
+ Index 1:累计频率。

等高统计信息与此类似，但总共有四个元素，以考虑到每个数据桶可能包含不止一个列值的信息。这些元素是

+ Index 0: 数据桶中列值的下限值。
+ Index 1: 数据桶中列值的上限。
+ Index 2: 累计频率
+ Index 3: 桶中包含的数值个数。

回过头来再看看计算各种条件的预期过滤效果的例子，你就会发现，水桶信息包括了所有必要的信息，但也不包括任何额外的信息。

由于直方图数据是以 JSON 文档的形式存储的，因此值得看一看几个检索各种信息的查询示例。

** 直方图报告示例

COLUMN_STATISTICS 视图对于查询直方图数据非常有用。由于元数据和统计信息存储在 JSON 文档中，因此考虑使用一些可用
的 JSON 操作函数是非常有用的，这样就可以检索直方图报告。本节将展示为系统中的直方图生成报告的几个示例。所有示例均可
从本书的 GitHub 代码库中获取，例如，清单 16-3 中的查询可从 listing_16_3.sql 文件中获取。

** 显示所有直方图

基本报告是列出 MySQL 实例中的所有直方图。需要包含的相关信息有：直方图的模式信息、直方图类型、直方图最后更新时间、
采样率、桶数等。清单 16-3 显示了一个直方图的查询和输出（您可能会看到不同的直方图列表，这取决于您创建了哪些直方
图）。

#+begin_src sql
  select SCHAME_NAME, TABLE_NAME, COLUMN_NAME,
    HISTOGRAM->>'$."histogram-type"' as Histogram_Type,
    cast(HISTOGRAM->>'$."last-updated"'
      as DATETIME(6)) as Last_Updated,
    cast(HISTOGRAM->>'$."sampling-rate"'
      as DECIMAL(4,2)) as Sampling_Rate,
    JSON_LENGTH(HISTOGRAM->'$.buckets')
      as Number_of_Buckets,
    cast(HISTOGRAM->'$."number-of-buckets-specified"' as unsigned)
    as Number_of_Buckets_Specified
  from information_schema.COLUMN_STATISTICS\G
#+end_src

该查询提供了直方图的高级视图。->>操作符从 JSON 文档中提取一个值，->>操作符还能取消提取值的引号，这在提取字符串时
非常有用。例如，从示例输出中可以看到，sakila.film 表中长度列的直方图有 140 个数据桶，但请求的数据桶为 256 个。
您还可以看到这是一个单列直方图，这并不奇怪，因为并没有使用所有请求的数据桶。

** 列出单个直方图的所有信息

查看直方图的整个输出结果可能很有用。举例来说，本章前面创建了 world.city_histogram 表，并填充了八个国家的数据。
您可以创建一个等高直方图，在 CountryCode 列上设置四个桶，如下所示

#+begin_src sql
  analyze table world.city_histogram
    update histogram on CountryCode
      with 4 buckets;
#+end_src

清单 16-4 查询了该直方图的数据。这与讨论等值直方图时用于图 16-3 的直方图相同。

#+begin_src sql
  select JSON_PRETTY(HISTOGRAM) as Histogram
    from information_schema.COLUMN_STATISTICS
  where SCHEMA_NAME='world'
    and TABLE_NAME='city_histogram'
    and COLUMN_NAME='CountryCode'\G
#+end_src

这个查询有几个有趣的地方。使用 JSON_PRETTY() 函数可以更方便地读取直方图信息。如果没有 JSON_PRETTY()函数，整个
文档将以一行的形式返回。

还要注意的是，每个值的下限和上限都以 base64encoded 字符串的形式返回。这是为了确保直方图可以处理字符串和二进制列
中的任何值。其他数据类型则直接存储其值。

** 单直方图的列表桶信息

在前面的示例中，我们查询的是直方图的原始数据。通过使用 JSON_TABLE()函数将数组转换为表格输出，可以更好地处理桶信
息。示例中使用的表是 city_histogram，它是 world.city 表的副本，包含 8 个国家，以避免输出量过大。在
CountryCode 列上有一个单例直方图：

#+begin_src sql
  analyze table world.city_histogram
    update histogram on CountryCode
      with 8 buckets;
#+end_src

这与图 16-2 中讨论单例直方图时使用的直方图相同。清单 16-5 展示了一个单例直方图的示例。

#+begin_src sql
  select (Row_ID-1) as Bucket_Number,
    SUBSTRING_INDEX(Bucket_Value, ':', -1) as
      Bucket_Value,
    ROUND(Cumulative_Frequency*100, 2) as
      Cumulative_Frequency,
    ROUND((Cumulative_Frequency-LAG(Cumulative_Frequency, 1, 0)
    OVER())*100, 2) as Frequency
  from information_schema.COLUMN_STATISTICS
    inner join JSON_TABLE(
      histogram->'$.buckets',
      '$[*]' COLUMNS(
        Row_ID for ordinality,
        Bucket_Value varchar(42) path '$[0]',
        Cumulative_Frequency double path '$[1]'
      )
    ) buckets
  where SCHEMA_NAME='world'
    and TABLE_NAME='city_histogram'
    and COLUMN_NAME='CountryCode'
  order by Row_ID\G
#+end_src

该查询将 COLUMN_STATISTICS 视图与 JSON_TABLE() 函数1 连接，将 JSON 文档转换为 SQL 表。该函数接收两个参数，
第一个参数是 JSON 文档，第二个参数是值的路径和生成表的列定义。列定义包括为每个数据桶创建的三列：

+ Row_ID：这一列有一个 FOR ORDINALITY 子句，使其成为一个以 1 为基础的自动递增计数器，因此可以通过减去 1 来表
  示水桶编号。
+ Bucket_Value： 水桶使用的列值。请注意，值是从 base64 编码解码后返回的，因此同样的查询适用于字符串和数值。
+ Cumulative_Frequency： 以介于 0.0 和 1.0 之间的小数表示水桶的累积频率。

JSON_TABLE() 函数的结果与派生表的使用方法相同。在查询的 SELECT 部分中，累计频率被转换为百分比，LAG() 窗口函
数2 用于计算每个数据桶的频率（也是百分比）。

** 等高直方图的列表桶信息

检索等高直方图数据桶信息的查询与刚才讨论的单个直方图查询非常相似。唯一不同的是，等高直方图有两个值（区间的起点和终
点）来定义桶和桶中值的数量。

例如，您可以在 world.city_histogram 表中的 CountryCode 列上创建一个有四个数据桶的直方图：

#+begin_src sql
  analyze table world.city_histogram
    update histogram on CountryCode
      with 4 buckets;
#+end_src

清单 16-6 显示了提取 world.city_histogram 表中 CountryCode 列（有四个存储桶）存储桶信息的示例。

#+begin_src sql
  select (Row_ID-1) as Bucket_Number,
    SUBSTRING_INDEX(Bucket_Value1, ':', -1) as
      Bucket_Lower_Value,
    SUBSTRING_INDEX(Bucket_Value2, ':', -1) as
      Bucket_Upper_Value,
    ROUND(Cumulative_Frequency*100, 2) as
      Cumulative_Frequency,
    ROUND((Cumulative_Frequency-LAG(Cumulative_Frequency, 1, 0)
    OVER())*100, 2) as Frequency,
    Number_of_Values
  from information_schema.COLUMN_STATISTICS
    inner join JSON_TABLE(
      histogram->'$.buckets',
      '$[*]' COLUMNS(
        Row_ID for ordinality,
        Bucket_Value1 varchar(42) PATH '$[0]',
        Bucket_Value2 varchar(42) PATH '$[1]',
        Cumulative_Frequency double PATH '$[2]',
        Number_of_Values int unsigned PATH '$[3]'
      )
    ) buckets
  where SCHEMA_NAME='world'
    and TABLE_NAME='city_histogram'
    and COLUMN_NAME='CountryCode'
  order by Row_ID\G
#+end_src

现在你已经掌握了一些检查直方图数据的工具，剩下的就是举例说明直方图如何改变查询计划了。

** 查询示例

直方图的主要目的是帮助优化器实现执行查询的最佳方式。举例说明直方图如何影响优化器改变查询计划可能很有用，因此本章将
讨论一个查询，当在 WHERE 子句中的一列添加直方图时，查询计划将发生改变。

该查询使用 sakila 样本数据库，查询时间短于 55 分钟、演员名字为猫王的电影。这个示例看起来似乎有些牵强，但类似的查
询很常见，例如，查询满足某些条件的客户的订单。这个示例查询可以编写如下：

#+begin_src sql
  select film_id, title, length,
    group_concat(
      concat_ws(' ', first_name, last_name)
    ) as Actors
  from sakila.film
    inner join sakila.film_actor using (film_id)
    inner join sakila.actor using (actor_id)
  where length<55 and first_name='Elvis'
  group by film_id;
#+end_src

film_id、title 和 length 列来自电影表，first_ name 和 last_name 列来自演员表。使用 GROUP_CONCAT() 函数
是为了防止电影中有不止一个演员的名字叫猫王（此查询的另一种方法是使用 EXISTS()，但这样查询结果中就包含了名字叫猫王
的演员的全名）。

length 和 first_name 列上没有索引，因此优化器无法知道这些列上的条件过滤效果如何。默认情况下，它会假定 length
条件返回影片表中大约三分之一的记录，first_name 条件返回 10%的记录。(下一章将介绍这些默认过滤值的来源）。

图 16-4 显示了不存在直方图时的查询计划。查询计划以 Visual Explain 图表的形式显示，将在第 20 章中讨论。

#+begin_comment
  在 mysQL Workbench 中执行查询并单击查询结果右侧的执行计划按钮，即可创建 Visual explain 图表。
#+end_comment

[[./images/PFOONP.png]]


在查询计划中需要注意的是，优化程序选择从演员表的全表扫描开始，然后通过 film_actor 表，最后连接到电影表。计算得出
的总查询成本（位于图的右上角）为 467.20（图中的查询成本数字可能与您得到的数字不同，因为它们取决于索引和直方图统
计）。

如前所述，优化器默认估计约有三分之一的电影长度小于 55 分钟。从影片长度的可能取值范围来看，这个估计值并不准确（但优
化器对电影一无所知，所以无法看出这一点）。事实上，只有 6.6% 的电影长度在这个范围内。这使得长度列成为直方图的理想候
选，您可以像之前显示的那样添加直方图：

#+begin_src sql
  analyze table sakila.film
    update histogram on length
      with 256 buckets;
#+end_src

现在查询计划发生了变化，如图 16-5 所示。

[[./images/WE0PLj.png]]

直方图意味着优化器现在可以准确知道，如果先扫描电影表，将返回多少行。这将查询的总成本降低到 282.26，是一个很好的改
进。(同样，根据您的索引统计，您可能会看到不同的变化。示例中最重要的是直方图改变了查询计划和估计成本）。

#+begin_comment
  实际上，本示例所用表格中的行数很少，因此查询执行的顺序并不重要。不过，在实际示例中，使用直方图可以带来很大的收益，
  有时甚至超过一个数量级。
#+end_comment

这个示例还有一个有趣的地方，那就是如果把条件改为查找时间短于 60 分钟的电影，那么连接顺序就会变回首先扫描演员表。原
因是，有了这个条件，就会有足够多的影片根据长度被包含进来，从而更好地开始寻找候选演员。同样，如果在演员表的名_姓上添
加直方图，优化器就会发现名是该数据库中演员的一个很好的过滤器，尤其是只有一个演员叫猫王。读者可以尝试更改 WHERE 子
句和直方图，看看查询计划会有什么变化。

** 总结

本章介绍了如何使用直方图来改进优化程序在确定最佳查询计划时可用的信息。直方图将列的值分成多个桶，每个桶一个值，称为
单桶直方图；每个桶多个值，称为等高直方图。对于每个桶，要确定这些值出现的频率，并计算每个桶的累计频率。

直方图主要适用于没有候选索引的列，但在以连接为特征的查询中仍可用于过滤。在这种情况下，直方图可以帮助优化器确定最佳
连接顺序。本章末尾举例说明了直方图如何改变查询的连接顺序。

直方图的元数据和统计信息可在 information_ schema.COLUMN_STATISTICS 视图中查看。这些信息包括优化器使用的每个
数据桶的所有数据，以及直方图的最后更新时间、直方图类型和请求的数据桶数量等元数据。

在查询示例中，有人提到优化器对各种条件的估计过滤效果有一些默认值。迄今为止，在关于索引和直方图的讨论中，优化器大多
被忽略。现在是改变这种状况的时候了：下一章的主题就是查询优化器。

* 查询优化器

当你向 MySQL 提交查询以供执行时，并不像读取数据并返回数据那么简单。诚然，对于请求单个表中所有数据的简单查询来说，如
何检索数据的选择并不多。但是，大多数查询都比较复杂，有些甚至要复杂得多，而且完全按照所写的查询执行绝不是获取结果的最
有效方法。在阅读有关索引的内容时，我们已经接触到了这种复杂性。除了索引的选择、连接顺序、用于执行连接的算法、各种连接
优化等之外，还有更多。这就是优化器发挥作用的地方。

优化器的主要工作是为执行查询做好准备，并确定最佳查询计划。第一阶段包括对查询进行转换，目的是使改写后的查询能以比原始
查询更低的成本执行。第二阶段包括计算执行查询的各种方式的成本，并确定最便宜的方案。

#+begin_comment
  必须认识到，由于数据及其分布的变化，优化器所做的工作并非精确科学。优化器选择的转换和计算的成本在某种程度上都是基于
  估计的。通常情况下，这些估计值足以获得良好的查询计划，但有时也需要提供提示。本章后面的 "配置优化器 "一节将讨论如何
  配置优化器。
#+end_comment

本章首先讨论转换和基于成本的优化。然后，本章继续讨论基本的连接算法，接着讨论其他优化功能，如批量密钥访问。本章的最后
一部分涉及如何配置优化器以及如何使用资源组来优先处理查询。

** 转换

人类认为自然的查询书写方式可能与在 MySQL 中执行查询的最佳方式不同。优化器知道有几种转换可以用来改变查询，同时返回
相同的结果，这样查询对 MySQL 来说就变得更优化了。

当然，最重要的是原始查询和改写后的查询返回相同的结果。幸运的是，关系数据库以数学集合论为基础，因此许多转换都可以使
用标准数学规则，确保两个版本的查询返回相同的结果（排除执行错误）。

常量传播是优化器最简单的转换类型之一。举例来说，请看下面的查询：

#+begin_src sql
  select *
  from world.country
    inner join world.city
      on city.CountryCode=country.Code
  where city.CountryCode='AUS';
#+end_src

该查询有两个条件：city.CountryCode 列必须等于 "AUS"，city表的 CountryCode 列必须等于country表的 Code
列。根据这两个条件，可以得出 country.Code 列也必须等于 "AUS"。优化器利用这一知识直接过滤Country表。由于
"Code"列是Country表的主键，这意味着优化器知道只有一条记录符合条件，因此优化器可以将Country表视为常数。实际
上，查询最终是以国家表中的列值作为常量在选择列表中执行的，并以 CountryCode = 'AUS' 对城市表中的条目进行扫描：

#+begin_src sql
  select 'AUS' as `Code`,
         'Australia' as `Name`,
         'Ocenia' as `Continent`,
         'Austrialia and New Zealand' as `Region`,
         7741220.00 as `SurfacaeArea`,
         1901 as `IndepYear`,
         18886000 as `Population`,
         79.8 as `LifeExpectancy`,
         351182.00 as `GNP`,
         392911.00 as `GNPOld`,
         'Australia' as `LocalNmae`,
         'Constitutional Monarchy, Federation' as `GovernmentForm`,
         'Elisabeth II' as `HeadOfState`,
         135 as `Capital`,
         'AU' as `Code2`,
         city.*
   from world.city
   where CountryCode ='AUS';
#+end_src

从性能角度来看，这是一种安全的转换。其他转换更为复杂，并不总能提高性能。因此，可以配置是否启用优化。配置可以使用
optimizer_switch 选项和优化器提示来完成，这将在介绍优化和如何配置优化器时讨论。

一旦优化器决定了要进行哪些转换，就需要确定如何执行重写后的查询，这将在下文中讨论。

** 基于代价的优化

MySQL 使用基于成本的查询优化。这意味着优化器会计算执行查询所需的各种操作的成本，然后将这些部分成本结合起来，计算出
可能的查询计划的总体查询成本，并选择最便宜的计划。本节将介绍估算查询计划成本的原则。

*** 基础单表select

无论查询如何，计算成本的原则都是一样的，但显然查询越复杂，成本估算就越复杂。举个简单的例子，假设一个查询使用索引列
上的 WHERE 子句查询一个表：

#+begin_src sql
  select *
    from world.city
  where CountryCode='IND';
#+end_src

从表的定义中可以看出，world.city 表的 CountryCode 列上有一个二级非唯一索引：

#+begin_src sql
  show create table world.city\G
#+end_src

优化程序可以选择两种方式获取匹配的记录。一种方法是使用 CountryCode 索引查找索引中的匹配记录，然后查找请求的记录
值。另一种方法是进行全表扫描，检查每条记录以确定其是否满足筛选条件。

在这些接入方法中，哪种成本最低（速度最快）并不像看起来那么简单。这取决于几个因素：

+ 索引的选择性如何？通过二级索引读取一条记录，首先要在索引中找到该行，然后可能（参见下一条）要进行主键查找才能获得
  该行。这意味着，使用二级索引检查和检索记录比直接读取记录的成本更高，要使索引访问的总体成本低于表扫描，索引必须显
  著减少需要检查的记录数量。索引的选择性越强，使用索引的成本就相对越低。
+ 索引是覆盖索引吗？如果索引包含查询所需的所有列，就有可能跳过实际行的读取，从而更有利于使用索引。
+ 读取记录的成本有多高？这同样取决于多个因素，例如索引和行数据是否已经在缓冲池中，如果没有，从磁盘读取记录的速度有
  多快。考虑到读取索引和读取聚类索引之间的切换，使用索引将需要更多随机 I/O，因此定位记录所需的寻道时间变得非常重要。

MySQL 8的新功能之一是，优化器可以询问InnoDB，查询所需的记录是否可以在缓冲池中找到，或者是否有必要从磁盘读取。这
对改进查询计划大有帮助。

读取记录的成本问题比较复杂，因为 MySQL 不知道硬件的性能特征。MySQL 8 默认假定从磁盘读取记录的成本是内存的四倍。
可以按照 "配置优化器 "一节中的 "引擎成本 "进行配置。

一旦在查询中引入第二个表，优化器还需要决定以何种顺序连接这些表。

*** 表连接顺序

对于比单表 SELECT 语句更复杂的查询，优化器不仅需要考虑访问每个表的成本，还需要考虑每个表的包含顺序，以及每个表使
用哪个索引。

对于外连接和直连接，连接顺序是固定的，但对于内连接，优化器可以自由选择顺序，因此优化器必须计算每种组合的成本。可能
的组合数为 N！(阶乘），扩展性非常差。如果有五张表参与内部连接，优化器可以选择五张表作为第一张表，然后四张表作为第
二张表，三张表作为第三张表，两张表作为第四张表，最后一张表为一张表：

#+begin_comment
  Combinations = 5 * 4 * 3 * 2 * 1 = 5! = 120
#+end_comment

MySQL 支持连接多达 61 张表，在这种情况下，计算成本可能需要 5.1E83 种组合，成本过高，可能比执行查询本身花费更多
时间。因此，优化器默认根据对成本的部分评估来修剪查询计划，这样只有最有希望的计划才会得到全面评估。也可以让优化器在
包含一定数量的表后停止计算成本。剪枝和搜索深度分别通过 optimizer_prune_level 和 optimizer_search_depth 选
项进行配置，"配置优化器 "部分将对此进行讨论。

最佳连接顺序与表有多大以及过滤器在减少每个表所含记录数方面的效果有关。

*** 默认过滤效果

连接两个或多个表时，优化程序需要知道每个表中包含多少行，才能确定最佳连接顺序。这绝不是一件小事。

当使用索引时，优化器可以非常准确地估算出，当过滤器与其他表无关时，有多少行会与索引匹配。如果没有索引，则可以使用直
方图统计来获得良好的过滤估计值。如果没有被过滤列的统计数据，就会出现困难。在这种情况下，优化器会使用内置的默认估计
值。表 17-1 列出了在没有索引或直方图统计数据时使用的默认过滤效果示例。

#+NAME: 无统计数据条件的默认过滤效果
| 类型        |            过滤数 | 示例                                    |
|------------+------------------+----------------------------------------|
| ALL        |              100 | 在按索引筛选或没有筛选条件时使用。           |
| Equality   |               10 | Name='Sydney'                          |
| Not Euqal  |               90 | Nmae <> 'Sydney'                       |
| Inequality |            33.33 | Population>4000000                     |
| Between    |            11.11 | Population between 1000000 and 4000000 |
| IN         | min(#item*10,50) | Nmae IN('Sydeny', 'Melbourne')                                       |

过滤效果基于 Selinger 等人撰写的文章 "关系数据库管理系统中的访问路径选择 "1。例如

+ 已知不同值：包括枚举和位数据类型。考虑一下 world.country 表中的 Continent 列。这是一个有 7 个值的枚举，因此
  优化器会估计 WHERE 子句（如 Continent = 'Europe'）的过滤效果为 1/7。
+ 行数少： 如果表格中的行数少于 10 行，并添加了相等条件，则过滤估计值将为 1/行数，不相等过滤估计值也是如此。
+ 组合筛选器： 如果将多个非索引列上的筛选器组合在一起，估计的筛选效果就是组合效果。例如，在 world.city 表中，
  Name = 'Sydney' AND Population > 3000000 这个筛选条件由于在 Name 上是相等的，所以估计会占用 10%的行，
  而由于在 Population 上是不相等的，所以会占用 33%的行，因此综合效果为 P(Equality on Name) * P(Inequality
   on Population) = 0.1 * 0.33 = 0.0333 = 3.33%。

这个列表并不详尽，但可以让你很好地了解 MySQL 是如何得出过滤估计值的。默认过滤效果显然不是很准确，尤其是对于大型表，
因为数据并不遵循如此严格的规则。这就是为什么索引和直方图对于获得良好的查询计划如此重要。

在确定查询计划的最后，会对各个部分和整个查询进行成本估算。这些信息有助于了解优化器是如何得出查询执行计划的。

*** 查询代价

如果要检查优化器发现的成本，则需要使用树形（包括 EXPLAIN ANALYZE）或 JSON 格式的 EXPLAIN 输出、MySQL
Workbench Visual Explain 图表或优化器跟踪。第 20 章将详细介绍这些方法。

举个简单的例子，请看连接世界样本数据库的国家表和城市表的查询：

#+begin_src sql
  select *
    from world.country
      inner join world.city
        on CountryCode=Code;
#+end_src

图 17-1 显示了查询的 Visual Explain 图表，包括城市表的额外详细信息。

[[./images/xyfB7m.png]]

该图显示了优化器是如何决定执行查询的。如何读图将在第 20 章讨论。这里最重要的部分是箭头所指的数字。这些是优化器为查
询执行的各个部分得出的成本估算，成本越低越好。从示例中可以看出，成本估算是针对读取数据、评估过滤条件等非常具体的任
务计算得出的。在图表顶端，总查询成本估计为 1535.43。

#+begin_comment
  由于计算出的成本取决于索引统计信息等，而索引统计信息并不精确，因此成本不会随着时间的推移而变化。这也意味着，如果
  执行相同的查询，您可能会看到与本书示例中显示的不同的成本估算。
#+end_comment

执行查询后，还可以从 Last_query_cost 状态变量中获取估计成本。清单 17-1 展示了对图 17-1 中相同查询进行此操作的
示例。

#+begin_src sql
  select *
    from world.country
      inner join world.city
        on CountryCode=Code;

  show session status like 'Last_query_cost';
#+end_src

查询结果已从输出中删除，因为它对本讨论并不重要。关于 Last_query_cost，需要注意的一点是它是估计成本，这也是为什么
它在 Visual Explain 图表中显示的值与总成本相同。如果想了解执行查询的实际成本，则需要使用 EXPLAIN ANALYZE。

Visual Explain 图中提到查询是使用嵌套循环执行的。这只是 MySQL 支持的连接算法之一。

** 连接算法

在 MySQL 中，"连接 "是一个非常宽泛的概念，以至于可以说所有东西都是连接。甚至查询单个表也被认为是连接。也就是说，
最有趣的连接是两个或多个表之间的连接。在本讨论中，表也可以是派生表。

执行查询时，如果需要连接两个表，MySQL 支持三种不同的算法。这些算法是

+ Nested loop
+ Block nested loop
+ Hash join

#+begin_comment
  本节所示时序仅供参考。您在自己系统上看到的时序会有所不同，时序之间也可能存在差异。
#+end_comment

本节和下一节将引用几个优化开关和优化提示的名称。优化器开关指的是 optimizer_switch 配置选项，优化器提示指的是
/*+ ... 注释。*/ 注释，这些注释可以添加到查询中，告诉优化器你希望查询如何执行。本章后面的 "配置优化器 "一节将进
一步讨论这两个概念以及如何使用它们。

*** 嵌套循环

嵌套循环算法是 MySQL 中使用的最简单的算法。在 MySQL 5.6 之前，它也是唯一可用的算法。顾名思义，它是通过嵌套循环
工作的，在连接中每个表都有一个循环。嵌套连接算法不仅非常简单，而且还能很好地进行索引查找。

#+begin_src sql
  select CountryCode, country.Name as Country,
    city.Name as City, city.District
  from world.country
    inner join world.city
      on city.CountryCode=country.Code
   where Continent='Asia';
#+end_src

在执行该查询时，将使用嵌套循环对country表进行表扫描，在country表中应用 WHERE 子句中的过滤器，然后对city
表进行索引查找。查询用树形符号表示如下

[[./images/NgO7k4.png]]

您也可以将其写成伪代码。使用类似 Python 的语法，嵌套循环连接可以写成下面的代码片段：

#+begin_src python
  result = []
  for country_row in country:
      if country_row.Continent == 'Asia':
          for city_row in city.CountryCode['country_row.Code']:
              result.append(join_rows(country_row, city_row))
#+end_src

在伪代码中，country 和 city 分别代表国家表和城市表，city.CountryCode 是城市表的 CountryCode 索引，
country_ row 和 city_row 代表一条记录。join_rows() 函数用于表示将两行中所需列合并为结果集中一行的过程。

图 17-2 使用图表显示了相同的嵌套循环连接。为了简单起见，同时也为了突出连接的重点，尽管所有记录都是从country表
中读取的，但只包含了匹配记录的主键值。

[[./images/BA1Dr7.png]]

从图中可以看出，MySQL 会扫描country表，直到找到与 WHERE 子句匹配的记录。在图中，第一条匹配行是 AFG（阿富汗）。
然后在city表中找到 CountryCode = AFG 的所有行（ID 分别等于 1、2、3 和 4），每个组合在结果中形成一行。继续
查找国家代码等于 ARE（阿拉伯联合酋长国）的行，依此类推，直到 YEM（也门）。

在country表和city表中的 CountryCode 索引中扫描记录的确切顺序取决于索引定义以及优化器、执行器和存储引擎的
内部因素。除非有明确的 ORDER BY 子句，否则绝对不能指望排序保持不变。

一般来说，连接可能比本例中的更复杂，因为可能会有额外的筛选器。但概念是相同的。

嵌套循环连接虽然简单，但也有一些局限性。它不能用于执行完全外连接，因为嵌套循环连接要求第一个表返回记录，而完全外连
接并不总是这样。解决方法是将全外连接写成左外连接和右外连接的结合。考虑一下查找所有国家和城市的查询，包括一个国家没
有城市和一个城市没有国家的情况。这可以写成全外连接（在 MySQL 中无效）：

#+begin_src sql
  select *
    from world.country
      full outer join world.city
        on city.CountryCode=country.Code;
#+end_src

要在 MySQL 中执行该操作，可以使用国家 LEFT JOIN 城市和国家 RIGHT JOIN 城市的联合，例如

#+begin_src sql
  select *
    from world.country
      left outer join world.city
        on city.CountryCode=country.Code;
  union
  select *
     from world.country
       right outer join world.city
         on city.CountryCode=country.Code;
#+end_src

另一个限制是嵌套循环连接对于不能使用索引的连接不是很有效。由于嵌套循环连接每次只处理连接中第一个表中的一条记录，因
此必须对第一个表中的每一条记录进行第二个表的全表扫描。这很快就会变得过于昂贵。考虑一下前面研究过的查询，即查找亚洲
的所有城市：

#+begin_src sql
  select PS_CURRENT_THREAD_ID();
#+end_src

#+begin_src sql
  select CountryCode, country.Name as Country,
    city.Name as City, city.District
  from world.country
    inner join world.city
       on city.CountryCode=country.Code
   where Continent='Asia';
#+end_src

通过对country表进行表扫描（239 行）和对city表进行索引查找，将总共检查 2005 行（在第二个连接中执行此查询）：

#+begin_src sql
  select rows_examined, rows_sent,
     last_statement_latency as latency
   from sys.session
   where thd_id=30\G
#+end_src

thd_id 上的过滤器需要与执行查询的连接的性能模式线程 ID 相匹配（在 MySQL 8.0.16 及更高版本中可以使用
PS_CURRENT_THREAD_ ID() 函数找到）。2005 条检查记录来自对国家表中的 239 条记录进行全表扫描，然后读取亚洲国
家城市表中的 1766 条记录。

如果 MySQL 不能为连接使用索引，那么查询性能就会发生巨大变化。您可以使用嵌套循环连接执行查询，而不使用索引，方法如
下（NO_BNL(city) 注释是优化器提示）：

#+begin_src sql
  select /* NO_BNL(city) */
    CountryCode, country.Name as Country,
    city.Name as City, city.District
  from world.country ignore index(Primary)
    inner join world.city ignore index(CountryCode)
      on city.CountryCode=country.Code
  where Continent='Asia';
#+end_src

IGNORE INDEX () 子句是一个索引提示，它告诉 MySQL 忽略括号中的索引。该版本查询的查询统计结果显示，现在要检查
200,000 多条记录，执行查询所需的时间是以前的 10 倍左右（执行该测试的方法与前面的测试方法相同，即在一个连接中执行
查找亚洲城市的查询，在另一个连接中执行下面针对 sys.session 的查询，并将 thd_id = 30 改为使用第一个连接的线程
id）：

#+begin_src sql
  select rows_examined, rows_sent,
         last_statement_latency as latency
  from sys.session
  where thd_id=30\G
#+end_src

有 51 个国家的大陆 ="亚洲"，这意味着需要对城市表进行 51 次全表扫描。由于城市表中有 4079 条记录，因此必须检查的记
录总数为 51 * 4079 + 239 = 208268。额外的 239 行来自国家表中 239 行的表扫描。

为什么需要在示例中添加 NO_BNL(country,city)注释？BNL 代表块嵌套循环，它可以帮助改进无索引连接，而注释会禁用这种
优化。通常情况下，我们希望保持启用状态，这将在下文中解释。

*** 块嵌套循环

块嵌套循环算法是嵌套循环算法的扩展。它也被称为 BNL 算法。在连接中，不是逐条提交第一张表中的记录，而是使用连接缓冲区
收集尽可能多的记录，并在对第二张表的一次扫描中进行比较。与嵌套循环算法相比，这可以大大提高某些查询的性能。

如果使用与嵌套循环算法示例相同的查询，但禁止使用索引（以模拟两个没有索引的表），并且不允许哈希连接（在 8.0.18 及更
高版本中），那么就可以使用块嵌套循环算法。该查询为

#+begin_src sql
  select /*+ NO_HASH_JOIN(country, city) */
    CountryCode, country.Name as Country,
    city.Name as City, city.District
  from world.country ignore index(Primary)
    inner join world.city ignore index(CountryCode)
      on city.CountryCode=country.Code
  where Continet='Asia';
#+end_src

在 MySQL 8.0.17 及更早版本中，删除带有 NO_HASH_JOIN() 优化器提示的注释。

清单 17-2 展示了使用类 Python 代码实现块嵌套循环算法的伪代码示例。

#+begin_src python
  result=[]
  join_buffer=[]
  for country_row in country:
      if country_row.Continent=='Asia':
          join_buffer.append(country_row.Code)

          if is_full(join_buffer):
              for city_row in city:
                  CountryCode=city_row.CountryCode
                  if CountryCode in join_buffer:
                      country_row=get_row(CountryCode)
                      result.append(
                          join_rows(country_row, city_row)
                      )
              join_buffer=[]
      if len(join_buffer)>0:
          for city_row in city:
              CountryCode=city_row.CountryCode
              if CountryCode in join_buffer:
                  country_row=get_row(CountryCode)
                  result.append(join_rows(country_row, city_row))
          join_buffer=[]
#+end_src

join_buffer 列表表示连接缓冲区，其中存储连接所需的列。在伪代码中，列是用 required_columns() 函数提取的。在作为
示例的查询中，只需要country表中的代码列。这一点很重要，稍后将进一步讨论。当连接缓冲区满时，将对city表进行表扫
描；如果城市表中的 CountryCode 列与连接缓冲区中存储的其中一个 Code 值匹配，则将生成结果行。

图 17-3 是表示连接的图表。为简单起见，只包含了连接所需行的主键值，尽管对两个表都执行了全表扫描。

[[./images/9NKHfM.png]]

图中显示了如何将country表中的记录一起读取并存储到连接缓冲区中。每次连接缓冲区满时，都会对city表进行一次全表扫
描，并逐步建立结果。在图中，连接缓冲区一次可容纳六条记录。由于 "代码 "列每行只需要 3 个字节，因此实际上，除了使用尽
可能小的 join_buffer_size 设置外，连接缓冲区将能够容纳所有国家代码。

使用连接缓冲区缓冲多个国家代码对查询统计有什么影响？与前面的示例一样，首先在一个连接中执行查找亚洲城市的查询：

图中显示了如何将country表中的记录一起读取并存储到连接缓冲区中。每次连接缓冲区满时，都会对city表进行一次全表扫
描，并逐步建立结果。在图中，连接缓冲区一次可容纳六条记录。由于 "代码 "列每行只需要 3 个字节，因此实际上，除了使用尽
可能小的 join_buffer_size 设置外，连接缓冲区将能够容纳所有国家代码。

使用连接缓冲区缓冲多个国家代码对查询统计有什么影响？与前面的示例一样，首先在一个连接中执行查找亚洲城市的查询：

#+begin_src sql
  select /*+ NO_HASH_JOIN(country,city) */
    CountryCode, country.Name as Country,
    city.Name as City, city.District
  from world.country ignore index (Primary)
    inner join world.city ignore index(CountryCode)
  where continent='Asia';
#+end_src

然后在另一个连接中查询 sys.session，以获得已检查行数和查询延迟（更改 thd_id = 30 以使用第一个连接的线程 ID）：

#+begin_src sql
  select rows_examined, rows_sent,
    last_statement_latency as latency
  from sys.session
  where thd_id=30\G
#+end_src

结果假设 join_buffer_size 为默认值。统计数据显示，块嵌套循环的性能明显优于不使用索引的嵌套循环算法。相比之下，使
用索引执行查询检查了 2005 条记录，耗时约 4 毫秒，而使用不带索引的嵌套循环连接检查了 208268 条记录，耗时约 45
毫秒。这看似是查询执行时间上无关紧要的差异，但country表和city表都非常小。对于大型表来说，差异将呈非线性增长，
这意味着查询完成与似乎永远运行之间的差异。

关于块嵌套循环，你应该注意一些要点，因为这有助于你优化使用它。这些要点包括

+ 连接缓冲区中只存储连接所需的列。这意味着，连接缓冲区所需的内存比最初预计的要少。
+ 连接缓冲区的大小由 join_buffer_size 变量配置。join_buffer_size 的值是缓冲区的最小大小！在讨论的示例中，如
  果 join_buffer_size 设置为 1 GiB，即使在连接缓冲区中存储的国家代码值小于 1 KiB，也将分配 1 GiB。因此，应
  保持较低的 join_buffer_size 值，只在需要时才增加。配置优化器 "部分包含如何更改单个查询的连接缓冲区大小的信息。
+ 使用块嵌套循环算法为每个连接分配一个连接缓冲区。
+ 每个连接缓冲区在整个查询过程中都会被分配。
+ 块嵌套循环算法可用于全表扫描、全索引扫描和范围扫描。
+ 块嵌套循环算法绝不会用于常量表以及第一个非常量表。这意味着，要使用块嵌套循环算法，需要在使用唯一索引过滤后，在两
  个有一条以上记录的表之间进行连接。

通过设置 block_nested_loop 优化开关，可以配置是否允许优化器选择块嵌套循环算法。默认情况下是启用。对于单个查询，
可以使用 BNL() 和 NO_BNL() 优化器提示为特定连接启用或禁用块嵌套循环。

虽然块嵌套循环对非索引连接有很大改进，但在大多数情况下，使用哈希连接可以做得更好。

*** 哈希连接

散列连接算法是 MySQL 最新添加的算法，在 MySQL 8.0.18 及更高版本中受支持。它标志着嵌套循环连接（包括块嵌套循环
变体）传统的重大突破。它特别适用于无索引的大型连接，在某些情况下甚至优于索引连接。

MySQL 实现了经典内存散列连接和磁盘 GRACE 散列连接算法之间的混合算法。如果可以将所有哈希值存储在内存中，则使用纯
内存实现。加入缓冲区用于内存部分，因此可用于哈希值的内存大小受 join_buffer_ size的限制。当连接无法在内存中完成
时，连接会溢出到磁盘，但实际连接操作仍在内存中执行。

内存散列连接算法包括两个步骤：

1. 在连接表中选择一个表作为构建表。计算连接所需列的哈希值并加载到内存中。这就是构建阶段。
2. 连接中的另一个表是探针输入表。对于这个表，每次读取一条记录并计算哈希值。然后对构建表计算出的哈希值进行哈希键查
   找，并从匹配的行中生成连接结果。这就是所谓的探测阶段。

当构建表的哈希值无法放入内存时，MySQL 会自动切换到使用磁盘实现（基于 GRACE 哈希连接算法）。如果在构建阶段连接缓
冲区已满，就会从内存算法切换到磁盘算法。磁盘上算法包括三个步骤：

1. 计算构建表和探测表中所有记录的哈希值，并将其存储到磁盘上的多个小文件中，文件以哈希值为分区。分区数量的选择是为
   了让探测表的每个分区都能放入连接缓冲区，但最多只能有 128 个分区。

2. 将构建表的第一个分区加载到内存中，然后遍历探测表中的哈希值，方法与内存中算法的探测阶段相同。由于步骤 1 中的分
   区对构建表和探测表使用了相同的哈希函数，因此只需遍历探测表的第一个分区即可。

3. 清除内存缓冲区，然后继续逐个处理其余分区。

内存算法和磁盘算法都使用 xxHash64 哈希函数，该函数以速度快而著称，同时还能提供高质量的哈希值（减少哈希碰撞次数）。
为了获得最佳性能，连接缓冲区必须足够大，以容纳构建表中的所有哈希值。也就是说，哈希连接和块嵌套循环连接需要考虑的
join_buffer_size 问题是一样的。

只要不选择块嵌套循环，并且查询支持散列连接算法，MySQL 就会使用散列连接。在撰写本文时，使用散列连接算法有以下要求：

1. 连接必须是内连接。
2. 无法使用索引执行连接，要么是因为没有可用索引，要么是因为索引在查询中被禁用。
3. 查询中的所有连接必须在连接中的两个表之间至少有一个等连接条件，且条件中只引用两个表中的列和常量。
4. 自 8.0.20 起，还支持反连接、半连接和外部连接。如果连接两个表 t1 和 t2，哈希连接支持的连接条件示例包括
   t1.t1_val=t2.t2_val
   t1.t1_val=t2.t2_val+2
   t1.t1_val1=t2.t2_val and t1.t1_val2>100
   MONTH(t1.t1_val)=MONTH(t2.t2_val)

如果考虑本节中的循环示例查询，可以忽略可用于连接的表上的索引，使用哈希连接执行该查询：

#+begin_src sql
  select CountryCode, country.Name as Country,
    city.Name as City, city.District
  from world.country ignore index(primary)
    inner join world.city ignore index(CountryCode)
      on city.CountryCode=country.Code
  where Coninent='Asia';
#+end_src

执行此连接的伪代码与块嵌套循环的伪代码类似，但连接所需的列是散列，并且支持溢出到磁盘。伪代码如清单 17-3 所示。

#+begin_src python
  result=[]
  join_buffer=[]
  partitions=0
  on_disk=False
  for country_row in country:
      if country_row.Continent='Asia':
          hash=xxHash64(country_row.Code)
          if not on_disk:
              join_buffer.append(hash)

              if is_full(join_buffer):
                  # Create partitions on disk
                  on_disk=True
                  partitions=write_buffer_to_disk(join_buffer)
                  join_buffer=[]
              else:
                  write_hash_to_disk(hash)
  if not on_disk:
      for city_row in city:
          hash=xxHash64(city_row.CountryCode)
          if hash in join_buffer:
              country_row=get_row(hash)
              city_row=get_row(hash)
              result.append(join_rows(country_row, city_row))
  else:
      for city_row in city:
          hash=xxHash64(city_row.CountryCode)
          write_hash_to_disk(hash)

      for partition in range(partitions):
          join_buffer=load_build_from_disk(partition)
          for hash in load_hash_from_disk(partition):
              if hash in join_buffer:
                  country_row=get_row(hash)
                  city_row=get_row(hash)
                  result.append(join_rows(country_row, city_row))

      join_buffer=[]
#+end_src

伪代码首先从country表中读取记录，然后计算代码列的哈希值，并将其存储在连接缓冲区中。如果缓冲区已满，代码就会切换
到磁盘算法，并从缓冲区写出哈希值。这也是确定分区数的地方。在此之后，country表的其余部分将被散列。

在下一部分中，对于内存算法，只需对city表中的行进行简单的循环，将哈希值与缓冲区中的哈希值进行比较。对于磁盘算法，
首先计算城市表的哈希值并存储在磁盘上，然后逐一处理分区。

#+begin_comment
  与实际使用的算法相比，所述算法略有简化。实际算法必须考虑哈希碰撞，而且对于磁盘上算法来说，有些分区可能会变得太大，
  无法放入连接缓冲区，在这种情况下，就会分块处理，以避免使用比配置更多的内存。
#+end_comment

图 17-4 显示了内存散列连接算法的示意图。为简单起见，只包含了连接所需行的主键值，尽管对两个表都执行了全表扫描。


[[./images/YJkf7z.png]]


country表中匹配行的代码列值被散列并存储在连接缓冲区中。然后对city表执行表扫描，对每一行的 CountryCode 计算
哈希值，并从匹配行中生成结果。

首先在一个连接中执行查询，检查查询统计数据的方法与前一种算法相同：

#+begin_src sql
  select CountryCode, country.Name as Country,
    city.Name as city, city.District
  from world.country ignore index(primary)
    inner join world.city ignore index (CountryCode)
      on city.CountryCode=country.Code
  where Continent='Asia';
#+end_src

然后，您可以通过在第二个连接中查询 sys.session 视图（更改 thd_id = 30 以使用第一个连接的线程 ID）来查看查询的
性能模式统计信息：

#+begin_src sql
  select rows_examined, rows_sent,
    last_statement_latency as latency
  from sys.session
  where thd_id=30\G
#+end_src

我们可以看到，散列连接在检查与块嵌套循环相同数量的记录时，查询执行得非常好，但它比索引连接更快。这并不是错误：在某
些情况下，散列连接甚至比索引连接更快。您可以使用以下规则来估算散列连接算法与索引连接和块嵌套循环连接相比的性能：

+ 对于不使用索引的连接，哈希连接通常比块嵌套连接快得多，除非添加了 LIMIT 子句。已观察到的改进超过 1000 倍。
+ 对于存在 LIMIT 子句的无索引连接，块嵌套循环可以在找到足够多的记录后退出，而散列连接将完成整个连接（但可以跳过获
  取记录）。如果因 LIMIT 子句而包含的记录数与通过连接找到的记录总数相比很少，嵌套循环可能会更快。
+ 对于支持索引的连接，如果索引的选择性较低，散列连接算法的速度会更快。

使用散列连接的最大好处是不带索引和 LIMIT 子句的连接。最后，只有测试才能证明哪种连接策略最适合您的查询。

可以使用 hash_join 优化器开关启用或禁用散列连接支持。此外，还必须启用 block_nested_loop 优化器开关。这两个开
关默认都已启用。如果要为特定连接配置散列连接的使用，可以使用 HASH_JOIN() 和 NO_HASH_JOIN() 优化器提示。

关于 MySQL 支持的三种高级连接策略的讨论到此为止。还有一些低层次的优化也值得考虑。

** 连接优化

MySQL 可以使用连接优化来改进上一节讨论的连接算法的基本概念，或决定如何执行查询的部分内容。本节将详细介绍索引合并、
多范围读取（MRR）和分批键访问（BKA）优化。这三种优化最有可能需要你帮助优化器使查询计划达到最优。其余可配置的优化将
在本节末尾介绍，但不太详细。

*** 索引合并

通常，MySQL 对每个表只使用一个索引。但是，如果您对同一表中的多个列都有条件，而您又没有涵盖所有列的单一索引，那么
这样做并不是最佳选择。在这种情况下，MySQL 支持索引合并。

#+begin_comment
  使用多列索引覆盖带有过滤条件的列比使用索引合并优化更有效。你应该权衡性能差异和可能多出的索引。
#+end_comment

支持三种索引合并算法。表 17-2 总结了这些算法、使用时间以及查询计划中包含的信息。

#+NMAE: 索引合并算法
| 算法          | 用例            | Extra列              | json键值         |
|--------------+----------------+---------------------+-----------------|
| Intersection | and            | Using interset(...) | intersect(...)  |
| Union        | or             | Using union(...)    | union(...)      |
| Sort-Union   | or with ranges | sort_union(...)     | sort_union(...) |

除了表中列出的 EXPLAIN 信息外，访问类型还被设置为index_merge

用例指定了连接条件的运算符。联合算法和排序联合算法的区别在于，联合算法适用于相等条件，而排序联合算法适用于范围条件。
对于 EXPLAIN 输出，括号内列出了与索引合并一起使用的索引名称。

在讨论这三种算法时，可以考虑使用每种算法的实际查询。sakila 数据库中的payment表就是一个很好的例子。
sakila.payment表的定义是

#+begin_src sql
  CREATE TABLE `payment` (
  `payment_id` smallint unsigned NOT NULL,
  `customer_id` smallint unsigned NOT NULL,
  `staff_id` tinyint unsigned NOT NULL,
  `rental_id` int(DEFAULT NULL, `amount` decimal(5,2) NOT NULL,
  `payment_date` datetime NOT NULL,
  `last_update` timestamp NULL,
  PRIMARY KEY (`payment_id`),
  KEY `idx_fk_staff_id` (`staff_id`),
  KEY `idx_fk_customer_id` (`customer_id`),
  KEY `fk_payment_rental` (`rental_id`)
  )ENGINE=InnoDB DEFAULT CHARSET=utf8
#+end_src

表中的默认值、自动递增信息和外键定义已被删除，重点放在列和索引上。该表有四个索引，都在单列上，因此很适合进行索引合并
优化。

索引合并讨论的其余部分将讨论每种索引合并算法、性能注意事项以及如何配置使用索引合并。所有示例都只包含两列的条件，但
这些算法确实支持涉及更多列的索引合并。

#+begin_comment
  优化器是否选择索引合并取决于索引统计信息。这意味着，对于同一个查询，WHERE 子句中的不同值可能会导致不同的查询计
  划，而索引统计量的变化会使查询在使用索引合并和不使用索引合并的条件完全相同的情况下执行不同的计划，反之亦然。
#+end_comment

**** Intersection算法

当多个索引列上的条件用 AND 分隔时，就会使用交集算法。使用交集索引合并算法的两个查询示例是

#+begin_src sql
  select *
  from sakila.payment
  where staff_id=1 and customer_id=75;
#+end_src

#+begin_src sql
  select *
  from sakila.payment
  where payment_id>10 and customer_id=318;
#+end_src

第一个查询在两个二级索引上有一个相等条件，第二个查询在主键上有一个范围条件，在二级索引上有一个相等条件。第二个查询
的索引合并优化仅适用于 InnoDB 表。清单 17-4 显示了 EXPLAIN 输出，其中第一个查询使用了两种不同的格式。

#+begin_src sql
  explain select *
  from sakila.payment
  where staff_id=1 and customer_id=75\G
#+end_src

#+begin_src sql
  explain format=tree
  select *
  from sakila.payment
  where staff_id=1 and customer_id=75\G
#+end_src

注意 Extra 列和树形格式输出中索引范围扫描的 Using intersect(...) 信息。这表明索引合并使用了
idx_fk_customer_id 和 idx_fk_staff_id 索引。传统输出还包括 key 列中的两个索引，并在 key_len 列中提供了两
个键长度。

**** Union算法

#+begin_src sql
  select *
  from sakila.payment
  where staff_id=1 or customer_id=318;
#+end_src

#+begin_src sql
  select *
  from sakila.payment
  where payment_id>15000 or customer_id=318;
#+end_src

第一个查询在二级索引上有两个相等条件，而第二个查询在主键上有一个范围条件，在二级索引上有一个相等条件。第二个查询只对
InnoDB 表使用索引合并。清单 17-5 显示了第一个查询的相应 EXPLAIN 输出示例。

#+begin_src sql
  explain select * from sakila.payment where staff_id=1 or customer_id=318\G
#+end_src

#+begin_src sql
  explain format=tree select * from sakila.payment where staff_id=1 or customer_id=318\G
#+end_src

注意在 Extra 列和树形格式输出中的索引范围扫描中使用了 union(...)。这表明索引合并使用了 idx_fk_staff_id 和
idx_fk_customer_ id 索引。

**** Sort-Union算法

排序联合算法用于类似使用联合算法的查询，但条件是范围条件而不是相等条件。可以使用排序联合算法的两个查询示例是

#+begin_src sql
  select * from sakila.payment
  where customer_id<30 or rental_id<10;
#+end_src

#+begin_src sql
  select * from sakila.payment
  where customer_id<30 ro rental_id>16000;
#+end_src

这两个查询都在两个二级索引上设置了范围条件。清单 17-6 显示了使用传统格式和树形格式对第一个查询进行的相应
EXPLAIN 输出。

#+begin_src sql
  explain select * from sakila.payment
  where customer_id<30 or rental_id<10;
#+end_src

#+begin_src sql
  explain format=tree
  select * from sakila.payment
  where customer_id<30 or rental_id<10;
#+end_src

注意在 Extra 列和树形格式输出中的索引范围扫描中使用了 sort_union(...)。这表明 idx_fk_customer_id 和
fk_payment_ rental 索引用于索引合并。

**** 性能考虑因素

优化器很难知道何时合并索引比只使用单个索引更优化。初看起来，为更多列使用索引总能取得胜利，但索引合并的开销很大，因
此只有当索引的选择性组合正确时，索引合并才会有用。由于索引统计数据过期而选择索引合并，是导致性能严重下降的常见原因
之一。

如果优化器选择了索引合并，而查询的执行效果不佳（例如，与通常的执行效果相比），那么首先要做的就是对使用了索引合并的
表执行 ANALYZE TABLE。这通常会改善查询计划。否则，可能需要更改优化器配置，以决定是否使用索引合并。

**** 配置

索引合并功能由四个优化器开关控制，其中一个控制整体功能，其他三个分别控制三种算法。选项包括

+ index _merge： 是启用还是完全禁用索引合并。
+ index _merge _intersection： 是否启用交集算法。
+ index _merge _union： 是否启用联合算法。
+ index _merge _sort _union： 是否启用排序联合算法。

默认情况下，所有索引合并优化器开关都已启用。

此外，还有两个优化器提示： INDEX_MERGE() 和 NO_INDEX_MERGE()。这两个提示都将表名作为参数，并可选择考虑或忽略
索引。例如，如果要执行查询，查找 staff_id 设置为 1、customer_id 设置为 75 的付款，而不使用索引合并，可以使用
以下查询之一：

#+begin_src sql
  select /* NO_INDEX_MERGE(payment) */ *
  from sakila.payment and customer_id=75
#+end_src

#+begin_src sql
  select /* NO_INDEX_MERGE(payment
            idx_fk_staff_id, idx_fk_customer_id) */
      *
  from sakila.payment
  where staff_id=1 and customer_id=75;
#+end_src

由于索引合并被认为是范围优化的一种特殊情况，因此 NO_RANGE_OPTIMIZATION() 优化器提示也禁用了索引合并。可以通过
EXPLAIN 输出确认不再使用索引合并，如清单 17-7 中第一个查询所示。


#+begin_src sql
  explain select /*+ NO_INDEX_MERGE(payment) */
     ,*
  from sakila.payment
  where staff_id=1 and customer_id=75\G
#+end_src

#+begin_src sql
  explain format=tree
  select /*+ NO_INDEX_MERGE(payment) */
     ,*
  from sakila.payment
  where staff_id=1 and customer_id=75\G
#+end_src

*** 多范围读取(MRR)

多范围读取（MRR）优化旨在减少由二级索引上的范围扫描引起的随机 I/O 量。该优化首先读取索引，根据行 id（InnoDB 的
聚类索引）对键进行排序，然后按照行的存储顺序检索行。多范围读取优化可用于使用索引的范围扫描和等连接。虚拟生成列上的
二级索引不支持该优化。

InnoDB 多范围读取优化的主要用例是没有覆盖索引的磁盘绑定查询。优化的效果取决于需要多少行以及存储的寻道时间。MySQL
会尝试估算优化何时有用；不过，成本估算偏向于过于悲观，而不是过于乐观，因此可能有必要提供信息，帮助优化器做出正确的
决定。

多范围读取优化由两个优化开关控制：

+ mrr 是否允许优化器使用多范围读取优化。默认为on
+ mrr_cost_based： 是否基于成本决定使用多范围读取优化。可以禁用此选项，以便在支持时始终使用该优化。默认为on

另外，也可以使用 MRR() 和 NO_MRR() 优化器开关，按表或索引启用或禁用多范围读取优化。

从查询计划中可以看到是否使用了多范围读取优化。在这种情况下，传统的 EXPLAIN 输出会在 Extra 列中指定使用 MRR，而
JSON 输出会将 using_MRR 字段设置为 true。清单 17-8 显示了使用多范围读取优化时传统格式的完整 EXPLAIN 输出示
例。

#+begin_src sql
  explain select /*+ MRR(city) */ from world.city
  where CountryCode between 'AUS' and 'CHN' \G
#+end_src

有必要使用 MRR() 优化器提示或禁用 mrr_cost_based 优化器开关明确要求使用多范围读取优化，因为示例查询的估计行数太
少，无法使用多范围读取优化和基于成本的优化来选择多范围读取优化。

使用优化时，MySQL 会使用随机读取缓冲区来存储索引。缓冲区的大小由 read_rnd_buffer_size 选项设置。

*** 批量键访问(BKA)

分批键访问 (BKA) 优化结合了分块嵌套循环和多范围读取优化。这样就能以与非索引连接类似的方式将连接缓冲区用于索引连接，
并使用多范围读取优化来减少随机 I/O 量。

对分批密钥访问最有用的查询类型是大型磁盘绑定查询，但并没有明确的指南来确定何时优化会有帮助，何时优化会导致性能下降。
当优化效果最好时，查询执行时间会缩短 2-10 倍。但是，当优化效果最差时，查询执行时间会增加 2-3 倍。

由于 "分批键访问 "优化只对范围相对较小的查询有效，而其他查询的性能可能会下降，因此默认情况下禁用了该优化。启用优
化的最佳方法是在发现优化能带来收益的查询中使用 BKA() 优化器提示。

如果要使用 optimizer_switch 变量启用优化，必须启用 batched_key_access 优化开关（默认已禁用），禁用
mrr_cost_based 优化开关（默认已启用），并确保 mrr 优化开关已启用（默认已启用）。要为会话启用分批密钥访问，可以
使用以下查询：

#+begin_src sql
  set session optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on';
#+end_src

以这种方式启用优化后，还可以使用 BKA() 和 NO_BKA() 优化器提示来影响是否使用该优化。使用该优化时，传统 EXPLAIN
输出中的 Extra 列包括使用连接缓冲区（分批键访问），而在 JSON 输出中，using_join_buffer 字段被设置为分批键访
问。清单 17-9 显示了使用分批键访问时完整 EXPLAIN 输出的示例。

#+begin_src sql
  explain select /*+ BKA(ci) */
    co.Code, co.Name, as Country,
    ci.Name as City
  from world.country co
    inner join world.city ci on ci.CountryCode=co.Code\G
#+end_src

在此示例中，使用优化器提示对使用 CountryCode 索引的城市 (ci) 表连接启用了分批键访问。

连接缓冲区的大小由 join_buffer_size 选项配置。由于分批键访问优化主要用于大型连接，因此连接缓冲区通常应配置得相
对较大，一般为 4 兆字节或更大。由于大型连接缓冲区对于大多数查询来说都不是好的选择，因此建议只在使用分批键访问优化
的查询中增加连接缓冲区的大小。

*** 其他优化

MySQL 还支持其他几种优化。优化程序会在有利于查询时自动使用这些优化，很少需要手动禁用这些优化。不过，了解这些优化
是什么还是很有用的，这样在遇到这些优化（例如在 EXPLAIN 输出中）时，就能知道它们的含义，并且在优化程序偶尔需要向
正确方向推动时，知道如何改变行为。

本小节将按字母顺序介绍剩余的一些优化，重点是那些可以配置的优化。对于每种优化，都包括优化器开关、优化器提示以及传统
格式（Extra 列）和 JSON 格式的 EXPLAIN 输出详情。

*** 条件过滤

当一个表有两个或多个相关条件，且索引可用于部分条件时，就会使用条件过滤优化。启用条件过滤后，在估算表的整体过滤效果
时会考虑其余条件的过滤效果。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: condition_fanout_filter 默认启用
+ Optimizer Hints: None
+ EXPLAIN Ouput: None


*** 派生合并

优化器可以将派生表、视图引用和通用表表达式合并到它们所属的查询块中。优化的另一种方法是将表、视图引用或通用表表达式
具体化。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: derived_merge 默认启用
+ Optimizer Hints: MERGE(), NO_MERGE()
+ EXPLAIN Output: 查询计划反映派生表已被合并。


*** 存储引擎条件下推

该优化将条件向下推送到存储引擎。目前只有 NDBCluster 存储引擎支持该功能。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch:engine_condition_pushdown 默认启用
+ Optimizer Hints: None
+ EXPLAIN Output: 这些警告包括有关被下推的条件的信息。


*** 索引条件下推

MySQL 可以向下推送通过使用单个索引中的列就能确定的条件，但索引只能直接过滤部分条件。例如，当你有一个条件，如
Name LIKE '%abc%' 而 Name 是多列索引的一部分时，就会出现这种情况。这种优化也用于二级索引的范围条件。对于
InnoDB，索引条件下推仅支持二级索引。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: index_condition_pushdown 默认启用
+ Optimizer Hints: NO_ICP()
+ EXPLAIN Output: 传统格式在 Extra 列中使用索引条件，而 JSON 格式则用推送的索引条件设置 index_condition
  字段


*** 索引扩展

InnoDB 中的所有二级非唯一索引都会将主键列附加到索引中。启用索引扩展优化后，MySQL 会将主键列视为索引的一部分。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: use_index_extensions 默认启用
+ Optimizer Hints: None
+ EXPLAIN Output: None


*** 索引可见性

当表中有不可见索引时，默认情况下优化器在创建查询计划时不会考虑该索引。如果启用了索引可见性优化器开关，则会考虑不可
见索引。例如，这对于测试已添加但尚未可见的索引的效果非常有用。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: use_invisible_indexes 默认禁用
+ Optimizer Hints: None
+ EXPLAIN Outpupt: None
  

*** 松散索引扫描

在某些情况下，MySQL 可以使用部分索引来提高聚合数据或包含 DISTINCT 子句的查询的性能。这就要求用于对数据分组的列与
未用于分组的其他列组成多列索引的左前缀。使用 GROUP BY 子句时，只允许使用 MIN() 和 MAX() 聚合函数。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: None
+ Optimizer Hints: NO_RANGE_OPTIMIZATION() 会禁用松散索引扫描优化以及索引合并和范围扫描。
+ EXPLAIN Output: 传统格式在 Extra 列中设置了分组索引。JSON 格式将 using_index_for_group_by 字段设置为
  true。


*** 范围访问方法

范围优化与其他优化有点不同，因为它被视为一种访问方法。MySQL 将只扫描表或索引的一个或多个部分，而不是进行完整的表
或索引扫描。范围访问方法通常用于涉及运算符 >、>=、<、=<、BETWEEN.、IN()、IS NULL、LIKE 和类似运算符的过滤条
件。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: None
+ Optimizer Hints: NO_RANGE_OPTIMIZATION() - 这也会禁用松散索引扫描和索引合并优化。不过，它不会禁用跳过扫
  描优化，尽管跳过扫描也使用范围访问。
+ Explain Output: 访问方法设置为范围。

可以使用 range_optimizer_max_mem_size 选项来限制范围访问所使用的内存大小。默认值为 8MB。如果将该值设为 0，则
表示可以无限量使用内存。

*** 半连接

半连接优化用于 IN 和 EXIST 条件。支持的策略有四种：具体化、重复剔除、首次匹配和松散扫描（不要与松散索引扫描优化
混淆）。启用子查询实体化后，半连接优化会在可能的情况下使用实体化策略。对于 EXISTS，半连接优化仅在 MySQL 8.0.16
及更高版本中支持，而对于 NOT EXISTS（以及类似的情况，这也称为反连接），则需要 MySQL 8.0.17 或更高版本。

半连接优化可以使用半连接优化开关来控制，以启用或完全禁用该优化。可以使用一个或多个 MATERIALIZATION、
DUPSWEEDOUT、FIRSTMATCH 和 LOOSESCAN 作为参数，在单个查询中使用 SEMIJOIN() 和 NO_SEMIJOIN() 优化器提
示。  

具体化策略与子查询具体化优化相同。详情请参见。

重复剔除策略将半连接当作普通连接执行，并使用临时表删除重复数据。优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: duplicateweedout 默认启用
+ Optimizer Hints: SEMIJOIN(DUPSWEEDOUT),NO_SEMIJOIN(DUPSWEEDOUT)
+ EXPLAIN Output: 传统格式在相关表的 Extra 列中有Start temporary和End temporary。JSON 格式的输出使
  用名为 duplicates_removal 的块。

首次匹配策略返回每个值的首次匹配值，而不是所有值。优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: firstmatch默认启用
+ Optimizer Hints: SEMIJOIN(FIRSTMATCH), NO_SEMIJOIN(FIRSTMATCH)
+ EXPLAIN  Ouput: 传统格式的 Extra 列中有 FirstMatch(...)，括号之间的值是引用表的名称。JSON 格式将
  first_ match 字段的值设置为引用表的名称。

松散扫描策略使用索引从子查询的每个值组中选择一个值。优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: loosescan默认启用
+ Optimizer Hints: SEMIJOIN(LOOSESCAN), NO_SEMIJOIN(LOOSESCAN)
+ EXPLAIN Output: 传统格式的 Extra 列中有 LooseScan(m..n)，其中 m 和 n 表示松散扫描使用索引的哪些部分。
  JSON 格式将 loosescan 字段设置为 true。


*** 跳过扫描

跳过扫描优化是 MySQL 8.0.13 中的新功能，其工作原理与松散索引扫描类似。当多列索引的第二列上有范围条件，但第一列上
没有条件时，就会使用它。跳过扫描优化将全索引扫描转化为一系列范围扫描（索引中第一列的每个值进行一次范围扫描）。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: skip_scan 默认启用
+ Optimizer Hints: SKIP(), NO_SKIP()
+ EXPLAIN Output: 传统格式在 Extra 列中设置了用于跳过扫描的使用索引，而 JSON 格式则将
  using_index_for_skip_scan 字段设置为 true。


*** 子查询物化

子查询实体化策略将子查询的结果存储在内部临时表中。在可能的情况下，优化程序会在临时表上添加一个自动生成的哈希索引，
这样就能快速将临时表连接到查询的其他部分。

优化器开关、提示和 EXPLAIN 详情如下：

+ Optimizer Switch: materialization 默认启用
+ Optimizer Switch: SUBQUERY(MATERALIZATION)
+ EXPLAIN Output: 传统格式的选择类型是 MATERIALIZED。JSON 格式创建了一个名为
   materialized_ from_subquery 的块。

启用子查询具体化成本优化器开关（默认）后，优化器将使用估计成本在子查询具体化优化和 IN 到 EXIST 子查询转换（将 IN
条件重写为 EXISTS）之间做出选择。关闭开关后，优化器将始终选择子查询具体化。

从上两节可以看出，优化器的配置有很多可能性。下一节将对此进行更深入的探讨。

** 配置优化器

有几种方法可以配置 MySQL 以影响优化器。你已经接触过一些配置选项、优化器开关和优化器提示。本节将首先介绍如何配置与
不同操作相关的引擎和服务器成本，然后介绍配置选项以及有关优化器开关的更多细节。最后将讨论优化器提示。

*** 引擎代价

引擎成本提供了读取数据的成本信息。由于数据可以从内存或磁盘读取，而不同的存储引擎读取数据的成本可能不同，因此不能一
刀切。因此，MySQL 允许你配置每个存储引擎从内存和磁盘读取数据的成本。

您可以使用 mysql.engine_cost 表来更改读取数据的成本。该表有以下列：

+ engine_name: 成本数据所针对的存储引擎。默认值用于表示没有特定数据的所有存储引擎。
+ device_type: 目前未使用，其值必须为 0。
+ cost_name: 成本名称。目前支持两种值：io_block_read_cost 用于基于磁盘的读取，memory_block_read_cost 用
  于基于内存的读取。
+ cost_value: 读取操作的成本。如果值为 NULL（默认值），则表示使用 default_value 列中存储的值。
+ last_update: 记录最后更新的时间。时间以 time_zone 会话变量设置的时区返回。
+ comment: 可选注释，用于说明更改成本的原因。注释长度不超过 1024 个字符。
+ default_value: 操作使用的默认成本。这是一个只读列。io_block_read_cost 的默认值为 1，
  memory_block_read_cost 的默认值为 0.25。

主键由 engine_name、device_type 和 cost_name 列组成。引擎成本对 InnoDB 特别有用，因为在 MySQL 8 中，
InnoDB 可以向优化程序提供估计值，以确定数据是在缓冲池中还是需要从磁盘读取。

您可以使用 UPDATE 语句更新现有的成本估算。如果要为存储引擎插入估算值，则使用 INSERT 语句；如果要删除自定义成本
值，则使用 DELETE 语句。无论哪种情况，都必须执行 FLUSH OPTIMIZER_COSTS 语句才能使更改对新连接生效（现有连接
继续使用旧值）。例如，如果要在磁盘 I/O 速度较慢而内存速度很快的主机上添加 InnoDB 的特定数据，可以使用以下语句

#+begin_src sql
  insert into mysql.engine_cost
    (engine_name, device_type, cost_name,
    const_value, comment)
    values('InnoDB', 0, 'io_block_read_cost',
          '2', 'InnoDB on non-local cloud storage'),
          ('InnoDB', 0, 'memory_block_read_cost',
          0.15, 'InnoDB with very fast memory');
#+end_src

#+begin_src sql
  flush optimizer_costs;
#+end_src

如果要更改成本值，建议先将成本值降低一倍或一半，然后评估效果。由于引擎成本是全局性的，因此应确保在更改前有一个良好
的监控基线，并比较更改后的查询性能，以检测更改是否达到预期效果。

MySQL 还有一些更通用的服务器成本，可用于影响与查询有关的各种操作。

*** 服务器代价

MySQL 使用基于成本的方法来确定最佳查询计划。要使这种方法尽可能有效，必须知道各类操作的成本有多高。计算中最重要的部
分是相对成本的正确性，幸运的是这一点很有帮助。然而，不同系统的相对成本及其对工作负载的影响可能存在差异。

您可以使用 mysql.server_cost 表来更改多个操作的成本。该表有以下列：

+ cost_name: 操作名称
+ cost_value: 执行操作的成本。如果成本设置为 NULL，则使用默认成本（default_value 列）。代价以浮点数形式提供。
+ last_update: 代价最后一次更新的时间。时间以 time_zone 会话变量设置的时区返回。
+ comment: 可选注释，用于说明更改成本的原因。注释长度不超过 1024 个字符。
+ default_value: 操作使用的默认成本。此栏为只读栏。

目前有六种操作可以在 server_cost 表中进行配置。它们是  
+ disk_temptable_create_cost: 在磁盘上创建内部临时表的成本。disk_temptable_create_cost 和
  disk_temptable_row_cost 的成本越低，优化器就越有可能选择需要磁盘上临时表的查询计划。默认成本为 20。
+ disk_temptable_row_cost: 在磁盘上创建的内部临时表的行操作成本。默认成本为 0.5。
+ key_compare_cost: 比较记录键的成本。如果查询计划在使用文件排序按索引排序时遇到问题，而基于非索引的排序会更快，
  那么可以增加这些操作的成本。默认成本为 0.05。
+ memory_temptable_create_cost: 在内存中创建内部临时表的成本。memory_temptable_create_cost 和
  memory_temptable_row_cost 的成本越低，优化器就越有可能选择需要内存内部临时表的查询计划。默认成本为 1。
+ memory_temptable_row_cost: 内存中创建的内部临时表的行操作成本。默认成本为 0.1。
+ row_evaluate_cost: 评估行条件的一般成本。成本越低，MySQL 就越倾向于检查许多行，如使用全表扫描。成本越高，
  MySQL 就越倾向于减少检查记录的数量，并使用更多的索引查找和范围扫描。默认成本为 0.1。

如果确实要更改服务器的某项费用，则需要使用常规的 UPDATE 语句，并在后面加上 FLUSH OPTIMIZER_COSTS。更改将影响
新的连接。例如，如果您将磁盘上的内部临时表存储在 RAM 磁盘（共享内存磁盘）上，并希望降低成本以反映以下情况

#+begin_src sql
  update mysql.server_cost
    set cost_value=1,
      Comment='Stored on memory disk'
      where cost_name='disk_temptable_create_cost';
#+end_src

#+begin_src sql
  update mysql.server_cost
    set cost_value=0.1,
      Comment='Stored on memory disk'
      where cost_name='disk_temptable_row_cost';
#+end_src

#+begin_src sql
  flush optimizer_costs;
#+end_src

更改成本可能并不总是会影响查询计划，因为优化器可能除了使用给定的查询计划外别无选择，或者计算出的成本差别太大，以至
于更改服务器成本来影响查询计划会对其他查询造成太大影响。请记住，服务器成本对所有连接都是全局性的，因此只有在出现系
统性问题时才应更改成本。如果问题只影响几个查询，最好使用优化器提示来影响查询计划。

影响查询计划的另一个选项是优化器开关。

*** 优化器开关

优化器开关在本章中已有提及。它们通过 optimizer_switch 选项进行配置。优化器开关的工作原理与其他配置选项有些不同，
因此值得深入研究其使用方法。

optimizer_switch 选项是一个复合选项，所有优化开关都使用同一个选项，但可以更改单个开关，而不包括不想更改的开关。
你可以将想要更改的开关设置为 "开 "或 "关"，以启用或禁用它。优化器开关既可以在影响所有新连接的全局范围内更改，也可
以在会话级别上更改。例如，如果要禁用当前连接的 derived_merge 优化器开关，可以使用以下语句：

#+begin_src sql
  set session optimizer_swith='derived_merge=off';
#+end_src

如果要永久更改值，可以使用 SET PERSIST 或 SET PERSIST_ONLY 进行更改：

#+begin_src sql
  set persist optimizer_switch='derived_merge=off';
#+end_src

如果希望将值存储在 MySQL 配置文件中，原则也是一样的：

#+begin_comment
[mysqld]
optimizer_switch = "derived_merge=off"
#+end_comment

表 17-3 列出了自 MySQL 8.0.18 起可用的优化器开关及其默认值，以及开关作用的简要说明。优化器开关按照它们在
optimizer_switch 选项中出现的顺序排列。

#+NAME: 优化器开关
| 优化器开关                              | 默认值 | 描述                                                                      |
|---------------------------------------+-------+--------------------------------------------------------------------------|
| index_merge                           | on    | 控制索引合并的总开关。                                                       |
| index_merge_union                     | on    | 联合索引合并策略                                                            |
| index_merge_sort_union                | on    | sort_union索引合并策略                                                     |
| index_merge_intersection              | on    | intersection索引合并策略                                                   |
| engine_condition_pushdown             | on    | NDBCluster的引擎条件下推                                                    |
| index_condition_pushdown              | on    | 存储引擎的索引条件下推                                                       |
| mrr                                   | on    | 多范围读取的优化                                                            |
| mrr_cost_based                        | on    | 基于代价估算是否采用mrr                                                      |
| block_nested_loop                     | on    | 块嵌套循环连接算法。它与 hash_join 开关一起控制是否可以使用散列连接。              |
| batched_key_access                    | off   | 分批键访问优化。要使用分批键访问，还必须启用 mrr 开关，并禁用 mrr_cost_based 开关。 |
| materialization                       | on    | 是否可以使用物化子查询。这也会影响物化半连接优化是否可用。                         |
| semijoin                              | on    | 启用或禁用半连接优化的总开关。                                                |
| loosescan                             | on    | 半连接损失扫描策略                                                          |
| firstmatch                            | on    | 半连接的首次匹配                                                            |
| duplicateweedout                      | on    | 半连接重复剔除策略                                                          |
| subquery_ materialization_cost_ based | on    | 是否使用子查询具体化基于成本估算。                                             |
| use_index_extensions                  | on    | InnoDB 添加到非唯一二级索引中的主键列是否作为索引的一部分使用。                    |
| condition_fanout_filter               | on    | 访问方法未处理的条件是否包含在过滤估计中。                                      |
| derived_merge                         | on    | 得出的合并优化结果。                                                         |
| use_invisible_indexes                 | on    | 是否使用不可见索引。                                                         |
| skip_scan                             | on    | 跳过扫描优化。                                                              |
| hash_join                             | on    | 散列连接算法。要启用散列连接，必须同时启用 block_nested_loop 开关。              |

本章前文对各种优化、策略和算法进行了详细介绍。

如果你想在全局或整个会话期间更改设置，optimizer_switch 选项是个不错的选择；但在很多情况下，你只需要为单个查询更
改优化器开关或设置。在这种情况下，优化器提示是更好的选择。

*** 优化器提示

优化器提示（optimizer hints）功能在 MySQL 5.7 中引入，并在 MySQL 8 中进行了扩展。 它允许你向优化器提供信息，
从而影响查询计划的最终结果。与打开或关闭选项的 optimizer_switch 选项不同，优化器提示等价物可以按查询块、表或索
引设置。此外，还支持在查询过程中更改配置选项的值。当优化器本身无法获得最佳查询计划，或者你需要执行查询时，例如某个
选项的值大于全局默认值时，这是提高查询性能的一种有效方法。

优化器提示是在 SELECT、INSERT、REPLACE、UPDATE 或 DELETE 子句后使用特殊注释语法设置的。该语法使用内联注释，
并在注释开始后紧跟一个 +，例如

#+begin_src sql
  select /*+ MAX_EXECUTION_TIME(2000) */
      id, Name, District
  from world.city
  where CountryCode='AUS';
#+end_src

此示例将查询的最长执行时间设置为 2000 毫秒。

表 17-4 列出了 MySQL 8.0.18 可用的优化器提示，包括每个提示支持的作用域和简要说明。对于许多提示，有两个版本，一
个用于启用功能，另一个用于禁用功能；这两个版本一起列出。除了 NO_ICP 和 NO_RANGE_OPTIMIZATION 提示没有相应的
提示来启用该功能外，其他提示均按启用该功能的提示的字母顺序排列。

#+优化器提示
| 提示                       | 作用域   | 描述                                                                                                                   |
|----------------------------+----------+------------------------------------------------------------------------------------------------------------------------|
| BKA NO_BKA                 | 查询块表 | bka优化                                                                                                                |
| NO_BKA NO_BNL              | 查询块表 | bnl连接算法                                                                                                            |
| HASH_JOIN NO_HASH_JOIN     | 查询块表 | hash连接算法                                                                                                           |
| INDEX_MERGE NO_INDEX_MERGE | 表索引   | 索引合并优化                                                                                                           |
| JOIN_FIXED_ORDER           | 查询块   | 强制按查询中列出的顺序执行查询块中的所有连接。这与使用 SELECT STRAIGHT_JOIN 相同。                                     |
| JOIN_ORDER                 | 查询块   | 强制两个或多个表按特定顺序连接。优化程序可自由更改未列出的表的连接顺序                                                 |
| JOIN_SUFFIX                | 查询块   | 强制指定的表成为连接的最后一个表，并按指定的顺序连接它们。                                                             |
| MAX_EXECUTION_TIME         | 全局     | 限制 SELECT 语句的查询执行时间。单位为毫秒。                                                                           |
| MERGE NO_MERGE             | 表       | 得出的合并优化结果。                                                                                                   |
| MRR NO_MRR                 | 表索引   | mrr优化                                                                                                                |
| NO_ICP                     | 表索引   | 索引条件下推优化                                                                                                       |
| NO_RANGE_OPTIMIZATION      | 表索引   | 不要对表和/或索引使用范围访问。这也会禁用索引合并和松散索引扫描。如果查询会导致大量范围扫描，并造成性能或资源问题，那么这种方法就非常有用。 |
| QB_NAME                    | 查询块   | 设置查询模块的名称。该名称可用于在其他优化器提示中引用查询块。                                                         |
| RESOURCE_GROUP             | 全局     | 资源组用于查询，下一节将讨论资源组。                                                                                   |
| SEMIJOIN NO_SEMIJOIN       | 查询块   | 半连接优化                                                                                                             |
| SKIP_SCAN NO_SKIP_SCAN     | 表索引   | 跳跃扫描优化                                                                                                           |
| SET_VAL                    | 全局     | 在查询过程中设置配置变量的值。                                                                                         |
| SUBQUERY                   | 查询块   | 子查询是否可以使用物化优化或 IN-to-EXISTS 转换。                                                                       |

在本章前面讨论连接算法和优化时，曾遇到过其中几个优化器提示。范围指定了提示适用于查询的哪一部分。范围包括

+ 全局：提示适用于整个查询。
+ 查询块： 提示适用于一组连接。例如，查询的顶层是一个查询块；子查询是另一个查询块。在某些情况下，适用于查询块的提示
  也可以使用连接的表名，以便将提示限制在特定的连接上。
+ 表： 提示适用于特定表格。
+ 索引： 提示适用于特定索引的使用。


指定表时，需要使用查询中使用的表名。如果为表指定了别名，则需要使用别名而不是表名，这样才能确保查询块中的所有表都能被
唯一识别。

#+begin_comment
  本书无法详细介绍使用优化器提示的所有细节。随着新功能的添加，提示列表也会经常更新。建议您阅读
  https://dev.mysql.com/doc/refman/en/optimizer-hints.html ，查看当前的优化器提示列表，以及有关使用和可
  能冲突的所有详细信息。
#+end_comment

优化器提示的指定方式与函数调用相同，参数用括号指定。如果优化器提示不包含任何参数，则使用空括号。您可以为同一查询指
定多个优化器提示，在这种情况下，可以使用空格将它们分隔开来。如果指定了多个参数，而不是一个前导查询块名称，则必须用
逗号分隔参数（但要注意，在某些情况下，空格用于将两个信息合并为一个参数，例如，指定索引时，表名和索引名用空格分隔）。

对于包含多个查询块的复杂查询，命名查询块是非常有用的，这样就可以指定优化器提示应适用于哪个查询块。你可以使用
QB_NAME() 优化器提示来设置查询块的名称：

#+begin_src sql
  select /* QB_NAME(payment) */
     rental_id
  from sakila.payment
  where staff_id=1 and customer_id=75;
#+end_src

然后，在指定提示时，您可以在查询块名称前添加 @ 来引用查询块：

#+begin_src sql
  select /*+ NO_INDEX_MERGE(@payment payment) */
      rental_id, rental_date, return_date
  from sakila.rental
  where rental_id in (
        select /*+ QB_NAME(payment) */
             rental_id
        from sakila.payment
        where staff_id=1 and customer_id=75
  );
#+end_src

这种方法与上一个示例中的方法相同，但它的优点是可以对不同查询块中的表使用同一个提示。

优化器提示的一个重要用途是在查询过程中改变配置变量的值。这对 join_buffer_size 和 read_rnd_buffer_size 等
选项特别有用，这些选项最好保持较小的全局值，但在某些查询中使用较大的值可以提高性能。您可以使用 SET_VAR()优化器提
示，参数就是变量赋值。在参考手册中，可以使用 SET_VAR() 优化器提示的变量有 "SET_VAR 提示适用： 是"。例如，要将
join_buffer_size 设置为 1 MiB，将 optimizer_search_depth 设置为 0（稍后将解释该选项），可以使用

#+begin_src sql
  select /*+ SET_VAR(join_buffer_size=1048576)
             SET_VAR(optimizer_search_depath=0) */
     CountryCode, country.Name as Country,
     city.Name as City, city.District
  from world.country ignore index(Primary)
     inner join world.city ignore index(CountryCode)
       on city.CountryCode=country.Code
  where Continent='Asia';
#+end_src

从示例中可以看出一些值得注意的地方。首先，SET_VAR() 提示不支持在同一提示中设置两个选项，因此需要为每个选项指定一
次提示。其次，该提示不支持表达式或单位，因此对于 join_ buffer_size，必须直接提供以字节为单位的值。

有一件事优化器提示帮不了你。如果你对优化器选择的索引不满意，就需要使用索引提示。

*** 索引提示

索引提示在 MySQL 中存在已久。你可以使用它们为每个表指定哪些索引允许优化程序使用，哪些应该忽略。在禁用用于块嵌套循
环和散列连接算法示例的索引时，你已经遇到过 IGNORE INDEX 提示。

MySQL支持三种索引提示

+ IGNORE INDEX: 优化器完全不允许使用已命名索引。
+ USE INDEX: 如果使用索引，优化器应使用其中一个已命名的索引。
+ FORCE INDEX: 这与 USE INDEX 相同，但如果有可能使用其中一个已命名的索引，则应避免进行表扫描。

使用索引提示时，需要在括号内以逗号分隔的列表中提供受提示影响的索引名称。索引提示紧跟在表名之后。如果为表添加了别名，
则将索引提示放在别名之后。例如，要查询亚洲所有城市，而不使用country表的主键或城市表的 CountryCode 索引，可以
使用下面的查询：

#+begin_src sql
  select ci.CountryCode, co.Name as Country,
    ci.Name as City, ci.District
  from world.country co ignore index(Primary)
  inner join world.city ci ignore index(Country) on ci.CountryCode=co.Code
  where co.Continent='Asia';
#+end_src

请注意主键被称为 Primary。在示例中，索引提示适用于所有可以使用表索引的操作。例如，可以通过添加 FOR JOIN、FOR
ORDER BY 或 FOR GROUP BY 将范围限制为连接、排序或分组：

#+begin_src sql
  select *
  from world.city use index for order by (Primary)
  where CountryCode='AUS'
  order by id;
#+end_src

虽然在大多数情况下，最好限制索引提示的使用，这样优化器就可以根据索引和数据的变化自由更改查询计划，但索引提示是最强
大的工具之一，在需要时不应该回避使用。

影响优化器的最后一种方法是使用配置选项。

*** 配置选项

除了 optimizer_switch选项外，还有一些配置选项会影响优化器。这些选项控制优化器搜索最优查询计划的穷尽程度，以及
是否使用优化器跟踪功能跟踪优化器的步骤。优化器跟踪功能将推迟到第 20 章与 EXPLAIN 语句一起讨论。

以下两个选项会被讨论

+ optimizer_prune_level
+ optimizer_search_depth

optimizer_prune_level 选项的值可以是 0 或 1，默认值是 1。它决定优化器是否会剪枝查询计划，以避免进行穷举搜索。
值为 1 时将启用剪枝。如果遇到的查询因剪枝而导致优化器无法找到足够好的查询计划，可以更改会话的
optimizer_prune_level 值。全局值几乎都应为 1。  

optimizer_search_depth 选项决定在搜索中应包含多少表（连接），以获得最佳查询计划。允许值为 0-62，默认值为 62。
由于一个查询块所允许的表的最大数量是 61，因此值 62 意味着除了剪枝所删除的搜索路径外，将进行穷举搜索。值 0 表示
MySQL 挑选最大搜索深度；目前这与将值设置为 7 相同。

如果查询块中有许多通过内连接方式连接的表，而且与查询执行时间相比，确定查询计划所需的时间较长，则可能需要将
optimizer_search_depth 设置为 0 或小于 62 的值。另一种方法是使用 JOIN_ORDER()、JOIN_PREFIX() 和
JOIN_SUFFIX() 优化器提示来锁定部分查询的连接顺序。

到目前为止，我们一直在围绕优化过程和优化器的选项进行讨论。还有一个层面需要考虑：执行查询时应使用哪个资源组。

** 资源组

资源组的概念是 MySQL 8 中的新概念，允许您为一个查询或一组查询可以使用的资源使用量设置规则。这可以成为提高高并发
系统性能的有力方法，并允许你优先处理某些查询。本节将介绍如何获取有关现有资源组的信息、管理资源组以及如何使用资源组。

#+begin_comment
 此外，在 Solaris 和 FreeBSD 以及 Linux 上，如果未设置 CAP_SYS_NICE 功能，线程优先级将被忽略。要了解最新限
 制以及如何启用 CAP_SYS_ NICE 功能，请参阅
 https://dev.mysql.com/doc/refman/en/resourcegroups.html#resource-group-restrictions。
#+end_comment

*** 检索有关资源组的信息

有关现有资源组的信息可在 information_ schema.RESOURCE_GROUPS 视图中找到，该视图是存储资源组的数据字典表顶部
的一个视图。该视图包括以下列：

+ RESOURCE_GROUP_NAME: 资源组的名称。
+ RESOURCE_GROUP_TYPE: 资源组是用于 SYSTEM 还是 USER 级别的线程。SYSTEM 用于系统线程，USER 用于用户连接。
+ RESOURCE_GROUP_ENABLED: 资源组是否可用
+ VCPU_IDS: 资源组允许使用哪些虚拟 CPU。虚拟 CPU 会考虑物理 CPU 内核、超线程、硬件线程等。
+ THREAD_PRIORITY:  使用资源组的线程的优先级。值越小，优先级越高。

清单 17-10 显示了随 MySQL 安装一起提供的默认资源组的资源组信息。VCPU_IDS 列的值取决于系统中虚拟 CPU 的数量。

#+begin_src sql
  select * from information_schema.resource_groups\G
#+end_src


默认情况下有两个资源组：USR_default 组用于用户连接，SYS_default 用于系统线程。这两个组的配置相同，允许使用所有
CPU。这两个组既不能丢弃，也不能修改。不过，您可以创建自己的资源组。

*** 管理资源组

只要不修改或删除默认组，就可以创建、更改和删除资源组。这样就可以创建资源组，用于在查询中分配资源。创建、更改或删除
资源组需要 RESOURCE_GROUP_ADMIN 权限。

以下语句可用于管理资源组：

+ CREATE RESOURCE GROUP: 创建新的资源组
+ ALTER RESOURCE GROUP: 修改已存在的资源组
+ DROP RESOURCE GROUP: 删除资源组

对于所有这三个语句，必须始终指定组名，并且指定时不带任何参数名（稍后将举例说明）。表 17-5 列出了这三个语句的参数。
其中数值指定 N 或 M-N，M 和 N 表示整数。

#+NAME: 管理资源组可使用的参数
| 选项      | 语法             | 值              | 操作               |
|----------+-----------------+----------------+-------------------|
| Name     |                 | 至多64个字符     | CREATE,ALTER,DROP |
| Type     | TYPE= ...       | SYSTEM,USER    | CREATE            |
| CPUs     | VCPU= ...       | N或M-N列表      | CREATE,ALTER      |
| Priority | THREAD_PRIORITY | N              | CREATE, ALTER     |
| Status   |                 | ENABLE,DISABLE | CREATE，ALTER      |
| Force    | FORCE           |                | ALTER,DROP                  |


优先级的有效值范围取决于组类型。SYSTEM 组的优先级介于 -20 和 0 之间，而 USER 类型的优先级介于 -20 和 19 之间。
优先级的含义遵循 Linux 中 nice 功能的原则，即优先级值越低，线程获得的优先级就越高。因此，-20 是最高优先级，而
19 则是最低优先级。在 Microsoft Windows 中，有五种可用的本地优先级。表 17-6 列出了资源组优先级与 Microsoft
Windows 优先级的映射关系。

#+: 在Windows中的资源组级别
| 初始优先级 | 结束优先级 | Windows Priority Level       |
|----------+----------+------------------------------|
|      -20 |      -10 | THREAD_PRIORITY_HIGHEST      |
|       -9 |       -1 | THREAD_PRIORITY_ABOVE_NORMAL |
|        0 |        0 | THREAD_PRIORITY_NORMAL       |
|        1 |       10 | THREAD_PRIORITY_BELOW_NORMAL |
|       11 |       19 | THREAD_PRIORITY_LOWEST                             |

创建新资源组时，必须设置组的名称和类型。其余参数为可选参数。默认情况下，VCPU 包括主机上所有可用的 CPU，优先级设为
0，并启用组。为用户连接创建一个名为 my_group 的已启用组，该组可以使用id 为 2、3、6 和 7 的 CPU 的示例如下（这
要求主机至少有 8 个虚拟 CPU）：

#+begin_src sql
  create resource group my_group
    type=user
    vcpu=2-3,6,7
    thread_priority=0
  enable;
#+end_src

VCPU 参数的指定说明了如何逐个列出 CPU 或使用一个范围。资源组名称被视为标识符，因此只需在与模式和表名相同的情况下
使用反斜线引述即可。

ALTER RESOURCE GROUP 语句与 CREATE RESOURCE GROUP 语句类似，但不能更改组名称或组类型。例如，要更改名为
my_group 的组的 CPU 和优先级

#+begin_src sql
  alter resource group my_group
   vcpu=2-5
   thread_priority=10;
#+end_src

如果需要删除一个资源组，可以使用 DROP RESOURCE GROUP 语句，例如，该语句只要求提供组名：

#+begin_src sql
  drop resource group my_group;
#+end_src

对于 ALTER RESOURCE GROUP 和 DROP RESOURCE GROUP 语句，有一个可选参数 FORCE。这指定了当有线程使用资源组
时，MySQL 应如何处理。表 17-7 总结了这些行为。

#+NAME: 使用 FORCE 或不使用 FORCE 的效果
| Forcing     | ALTER                                                                       | DROP                              |
|-------------+-----------------------------------------------------------------------------+-----------------------------------|
| Not forcing | 当使用该组的所有现有线程都已终止时，更改才会生效。在此之前，任何新线程都不能使用该资源组。 | 如果有任何线程分配给该组，则会发生错误。 |
| Forcing     | 现有线程会根据线程类型移动到默认组中。                                             | 现有线程会根据线程类型移动到默认组中。   |

在修改和删除资源组时，如果使用 FORCE 选项，分配给该组的现有线程将重新分配给默认组。这意味着用户连接使用
USR_default 组，系统线程使用 SYS_default 组。对于 ALTER RESOURCE GROUP，只有同时指定了 DISABLE 选项，才
能使用 FORCE 选项。

现在可以为线程分配资源组了。

*** 分配资源组

为线程设置资源组有两种方法。可以显式地为线程设置资源组，也可以使用优化器提示为单个查询设置资源组。无论使用哪种方法，
将线程分配到资源组都需要 RESOURCE_GROUP_ADMIN 或 RESOURCE_GROUP_USER 权限。

首先，重新创建 my_group 组（这次只使用一个 CPU，以便在所有系统上运行）：

#+begin_src sql
  create resource group my_group
    type=user
    vcpu=0
  thread_priority=0
  enable;
#+end_src

#+begin_comment
  目前不允许使用 X 协议（mySQL Shell 的默认协议）的连接创建、修改或设置资源组，除非使用优化器提示为单个查询设置
  资源组。
#+end_comment

使用 SET RESOURCE GROUP 语句可将线程分配到资源组。这对系统线程和用户线程都有效。要将连接本身分配给资源组，请使
用以资源组名称为唯一参数的语句，例如

#+begin_src sql
  set resource group my_group;
#+end_src

如果要更改一个或多个其他线程的资源组，可在末尾添加 FOR 关键字，然后用逗号分隔列出要分配给该组的 Performance
Schema 线程 ID。例如，要将线程 47、49 和 50 分配到 my_group（在本例中，线程 id 显然与您的情况不同，请用您系
统中存在的线程替换）

#+begin_src sql
  set resource group my_group for 47, 49, 50;
#+end_src

作为一种替代方法，您可以使用 RESOURCE_GROUP() 优化器提示，例如在查询期间为线程分配资源组：

#+begin_src sql
  select /*+ RESOURCE_GROUP(my_group) */
    ,*
  from world.city
  where CountryCode='USA';
#+end_src

一般来说，优化器提示是使用资源组的最佳方式，因为它允许你在每个查询中设置资源组，而且在使用 X 协议时也支持优化器提
示。它还可以与 MySQL 重写插件或 ProxySQL 等支持在查询中添加优化器提示注释的代理结合使用。

您可以使用 performance_schema.threads 表中的 RESOURCE_GROUP 列查看每个线程使用的资源组。例如，要查看之前
使用 SET RESOURCE GROUP FOR 47、49、50 语句更改的三个线程所使用的资源组

#+begin_src sql
  select THREAD_ID, RESOURCE_GROUP
  from performance_schema.threads
  where THREAD_ID in (47, 49, 50);
#+end_src

*** 性能考虑因素

使用资源组的效果取决于多个因素。默认情况下，所有线程都可以在任何 CPU 上以相同的中端优先级执行，这与 MySQL 5.7 及
更早版本中的行为相同。当 MySQL 开始遇到资源争用时，使用不同的资源组配置会带来主要好处。

由于资源组的最佳使用在很大程度上取决于硬件和查询工作量的组合，因此无法给出具体建议。资源组的最佳使用也会随着 MySQL
代码的新改进而改变。这意味着，您需要一如既往地使用监控来确定更改资源组和使用资源组的效果。

尽管如此，我们还是可以就如何使用资源组来提高性能或改善用户体验提出一些建议。这些建议包括但不限于

+ 为不同的连接赋予不同的优先级。例如，这可以确保批处理作业不会过多地影响与前端应用程序相关的查询，也可以为不同的应
  用程序赋予不同的优先级。
+ 将不同应用的线程分配到不同的 CPU 组，以减少它们之间的干扰。
+ 将写线程和读线程分配到不同的 CPU 组，为不同任务设置最大并发量。举例来说，如果写线程遇到资源争用问题，这可以用来
  限制写线程的并发量。
+ 为执行需要很多锁的事务的线程赋予高优先级，以便事务能尽快完成并再次释放锁。

根据经验，如果没有足够的 CPU 资源来并行执行所有操作，或者写并发量过大，那么资源组就会非常有用。对于低并发工作负载，
通常最好使用默认资源组。  

** 总结

本章将介绍优化器的工作原理、可用的连接算法和优化、如何配置优化器以及资源组。

MySQL 使用基于成本的优化器，在这种优化器中，会估算查询执行过程中每个部分的成本，并选择使成本最小化的整体查询计划。
作为优化的一部分，优化器将使用各种转换重写查询，找到最佳连接顺序，并做出其他决定，如应使用哪些索引。

MySQL 支持三种连接算法。最简单也是最原始的算法是嵌套循环连接，它只是遍历最外层表中的记录，然后对下一个表进行嵌套
循环，以此类推。块嵌套循环是一种扩展，非索引连接可以使用连接缓冲区来减少内表的表扫描次数。MySQL 8.0.18 中新增了
哈希连接算法，该算法也用于不使用索引的连接，对于它所支持的连接来说非常有效--对于低选择性索引来说，它的效果甚至超过
了索引连接。

还可以使用一系列其他优化方法。我们特别关注了索引合并、多范围读取和分批键访问优化。索引合并优化允许 MySQL 对每个表
使用一个以上的索引。多范围读取优化用于减少二级索引读取造成的随机 I/O 量。分批键访问优化结合了块嵌套循环和多范围读
取优化。

有几种方法可以改变 MySQL 的配置来影响优化器。mysql.engine_cost 表存储从内存和磁盘读取的成本信息。可根据每个存
储引擎进行设置。mysql.server_cost 包含各种操作的基本成本估算，如使用内部临时表和比较记录。optimizer_switch
配置选项用于启用或禁用各种优化器功能，如块嵌套循环、分批键访问等。

影响优化器的两个灵活选项是使用优化器提示和索引提示。优化器提示可用于启用或禁用功能，以及设置查询选项或更细粒度的索
引级选项。索引提示可用于启用或禁用表的索引。索引提示还可以选择限制在特定操作上，如排序。最后，
optimizer_prune_level 和 optimizer_search_depth 选项可用于限制优化器为找到最优查询计划所做的工作。

最后一项功能是资源组，它是 MySQL 8 中新增的功能。资源组可用于指定允许线程使用的 CPU 以及线程执行的优先级。这对某
些线程的优先级高于其他线程或防止资源争用非常有用。

* 锁原理和监控

与上一章讨论的优化器一起，锁可能是查询优化中最复杂的主题。当锁表现出其最糟糕的一面时，即使是最优秀的锁专家也会感到头
皮发麻。不过，不要绝望。本章将向你介绍你所需要的大部分锁知识--可能还有更多。读完本章后，你就可以开始研究锁，并借此获
得更多知识。

本章首先讨论了为什么需要锁以及锁的访问级别。然后，本章的最大部分讨论了 MySQL 中最常遇到的锁。本章的另一半讨论了锁请
求可能失败的原因、如何减少锁的影响以及如何监控锁。

#+begin_comment
  大多数示例都包含重现输出重要部分的语句（有些数据在本质上会因情况而异）。由于锁定的重要部分通常包括一个以上的连接，
  因此在设置查询提示时会指明在重要的情况下哪些查询应使用哪些连接。例如，连接 1> 表示查询应由第一个连接执行。
#+end_comment

** 为什么需要锁

这似乎是一个不需要锁定数据库的完美世界。然而，这样做的代价会很高，只有少数用例可以使用该数据库，而对于 MySQL 这样
的通用数据库来说，这是不可能的。如果没有锁定，就不可能有并发性。试想一下，数据库永远只允许一个连接（你可以说这本身
就是一个锁，因此系统无论如何都不是无锁的）--这对大多数应用来说都不是很有用。

#+begin_comment
  通常情况下，MySQL 中所谓的锁实际上是一个锁请求，它可能处于授权或待定状态。
#+end_comment

当多个连接同时执行查询时，需要某种方法来确保这些连接不会互相干扰。这就是锁的作用。你可以把锁想象成道路交通中的交通
信号灯，它可以控制资源的访问以避免事故的发生。在道路交叉口，必须确保两辆汽车不会相互交叉和碰撞。在数据库中，有必要
确保两个查询对数据的访问不会发生冲突。

由于交叉路口的通行控制有不同级别--让行、停车标志和交通信号灯，因此数据库中也有不同的锁类型。

** 锁访问级别

锁的访问级别决定了特定锁允许的访问类型。有时它也被称为锁类型，但由于这可能与锁的粒度相混淆，因此这里使用锁访问级别
一词。

主要有两种访问级别：共享或独占。访问级别的作用如其名称所示。共享锁允许其他连接也获得共享锁。这是权限最大的锁访问级
别。独占锁只允许一个连接获得锁。共享锁也称为读取锁，独占锁也称为写入锁。

MySQL 还有一个称为意图锁的概念，用于指定事务的意图。意图锁可以是共享的，也可以是独占的。下一节将介绍 MySQL 中的主
要锁粒度级别，在介绍隐式表锁时会更详细地讨论意图锁。

** 锁定粒度

MySQL 使用一系列不同的锁粒度（也称为锁类型）来控制对数据的访问。通过使用不同的锁粒度，可以最大限度地允许并发访问
数据。本节将介绍 MySQL 使用的主要粒度级别。

*** 用户级锁

用户级锁是一种显式锁类型，应用程序可以用它来保护工作流等。它们并不常用，但对于一些需要序列化访问的复杂任务来说，
它们可能很有用。 所有用户锁都是排他锁，使用名称获取，名称长度最多为 64 个字符。

您可以使用一组函数来操作用户级锁：

+ GET_LOCK(name, timeout): 通过指定锁的名称来获取锁。第二个参数是超时时间，单位为秒；如果在该时间内未获得
  锁，函数返回 0；如果获得锁，返回值为 1。 如果超时时间为负数，函数将无限期等待锁可用。
+ IS_FREE_LOCK(name): 检查指定的锁是否可用。如果锁可用，函数返回 1；如果锁不可用，函数返回 0。
+ IS_USED_LOCK(name): 该函数与 IS_FREE_LOCK() 函数相反。如果锁正在使用（不可用），函数将返回持有锁的连接
  的连接 ID；如果锁未使用（可用），函数将返回 NULL。
+ RELEASE_ALL_LOCKS(): 释放连接持有的所有用户级锁。返回值是释放的锁的数量。
+ RELEASE_LOCK(name): 释放提供名称的锁。如果锁已释放，返回值为 1；如果锁存在但不为连接所有，返回值为 0；如果
  锁不存在，返回值为 NULL。

通过多次调用 GET_LOCK()，可以获得多个锁。如果这样做，请注意确保所有用户以相同的顺序获取锁，否则可能会出现死锁。
如果发生死锁，将返回 ER_USER_LOCK_DEADLOCK 错误（错误代码 3058）。示例见清单 18-1。

#+begin_src sql
  select get_lock('my_lock_1', -1);
#+end_src

#+begin_src sql
  select get_lock('my_lock_2', -1);
#+end_src

当连接 2 试图获取 my_lock_1 锁时，语句将阻塞，直到连接 1 试图获取 my_lock_2 锁触发死锁。如果获得多个锁，则
应做好处理死锁的准备。请注意，对于用户级锁，死锁不会触发事务回滚。

如清单 18-2 所示，已授权和待授权的用户级锁可以在 OBJECT_TYPE 列设置为 USER LEVEL LOCK 的
performance_schema.metadata_locks 表中找到。列出的锁假定您在清单 18-1 中的死锁触发时的系统状态。请注意，
某些值（如 OBJECT_INSTANCE_ BEGIN）对您来说会有所不同。

#+begin_src sql
  select * from performance_schema.metadata_locks;
#+end_src

用户级锁的 OBJECT_TYPE 是 USER LEVEL LOCK，锁的持续时间是 EXPLICIT，因为再次释放锁取决于用户或应用程序。
在第 1 行中，性能模式线程 id 76 的连接已被授予 my_lock_1 锁，而在第 3 行中，线程 id 62 正在等待（挂起）该
锁。线程 id 62 也有一个已被授予的锁，包括在第 2 行中。

下一级锁涉及非数据表级锁。首先要讨论的是刷新锁。

*** FLUSH锁

对于大多数参与过备份的人来说，刷新锁并不陌生。它在使用 FLUSH TABLES 语句时产生，并在该语句的持续时间内有效，
除非添加 WITH READ LOCK（读取锁），在这种情况下，共享（读取）锁将被保留，直到显式释放锁为止。ANALYZE TABLE
语句结束时也会触发隐式刷新表。刷新锁是表级锁。使用 "FLUSH TABLES WITH READ LOCK "获取的读锁将在后面的显式锁
中讨论。

导致刷新锁问题的一个常见原因是长时间运行的查询。只要存在打开表的查询，FLUSH TABLES 语句就无法刷新表。这意味着，
如果在执行 FLUSH TABLES 语句时，正在使用一个或多个被刷新的表进行长时间运行的查询，那么 FLUSH TABLES 语句将阻
止所有需要使用这些表的其他语句，直到锁定情况得到解决。

刷新锁受 lock_wait_timeout 设置的限制。如果获得锁的时间超过 lock_wait_timeout 秒，MySQL 将放弃该锁。如果
FLUSH TABLES 语句被杀死，情况也一样。然而，由于 MySQL 的内部原因，一个称为表定义缓存（TDC）版本锁的较低级锁并
不总能在长期运行的查询完成之前被释放1 。这意味着，确保锁问题得到解决的唯一方法是杀死长期运行的查询，但要注意，
如果查询更改了许多行，可能需要很长时间才能回滚查询。

当存在围绕刷新锁的锁竞争时，FLUSH TABLES 语句和随后启动的查询都会将状态设置为 "等待表刷新"。清单 18-3 展示了
一个涉及三个查询的示例。要重现该示例，首先执行三个查询，提示符设置为 Connection N>，N 分别为 1、2 或 3，代表
三个不同的连接。针对 sys.session 的查询在第四个连接中执行。所有查询必须在第一个查询完成之前执行完毕（需要三分
钟）。

#+begin_src sql
  select *, sleep(180) from world.city where id=130;
#+end_src

#+begin_src sql
  flush tables;
#+end_src

#+begin_src sql
  select * from world.city where id=201;
#+end_src

#+begin_src sql
  select thd_id, conn_id, state, current_statement
  from sys.session
  where current_statement is not null
  and thd_id <> ps_current_thread_id();
#+end_src

该示例使用了 sys.session 视图；使用 performance_schema.threads 和 SHOW PROCESSLIST 可以获得类似结果。
为了减少输出，只包含与刷新锁讨论相关的查询，当前线程和没有正在进行的查询的线程被过滤掉了。

conn_id = 21 的连接正在执行一个使用 world.city 表的慢速查询（使用了 SLEEP(180) 以确保查询耗时较长）。在此
期间，conn_ id = 22 执行了一条针对 world.city 表的 FLUSH TABLES 语句。由于第一个查询仍在打开表（查询完成
后会释放），FLUSH TABLES 语句最终等待表刷新锁。最后，conn_id = 23 尝试查询表，因此必须等待 FLUSH TABLES
语句。

*** 元数据锁

元数据锁是 MySQL 中较新的锁类型之一。它们是在 MySQL 5.5 中引入的，目的是保护模式，使其在查询或事务依赖模式不
变时不会被更改。元数据锁在表级别工作，但应将其视为独立于表锁的锁类型，因为它们不保护表中的数据。

SELECT 语句和 DML 查询使用共享元数据锁，而 DDL 语句使用独占锁。连接会在首次使用表时获得该表的元数据锁，并将该
锁保持到事务结束。在元数据锁保留期间，其他连接不得更改表的模式定义。但是，执行 SELECT 语句和 DML 语句的其他连
接不受限制。通常，元数据锁的最大问题是空闲事务会阻止 DDL 语句开始工作。

如果遇到元数据锁冲突，就会看到进程列表中的查询状态设置为 "Waiting for table metadata lock"。包括要设置的
查询在内的示例如清单 18-4 所示。

#+begin_src sql
  select connection_id();
#+end_src

#+begin_src sql
  start transaction;
  select * from world.city where id=130\G
#+end_src

#+begin_src sql
  optimize table world.city;
#+end_src

#+begin_src sql
  select thd_id, conn_id, state,
        current_statement,
        last_statement
  from sys.session
  where conn_id in (21, 22)\G
#+end_src

在本例中，conn_id = 21 的连接有一个正在进行的事务，并在上一条语句中查询了 world.city 表（本例中的当前语句是
相同的，因为在执行下一条语句之前不会清除）。当事务仍处于活动状态时，conn_id = 22 执行了一条 OPTIMIZE TABLE
语句，现在正在等待元数据锁。(OPTIMIZE TABLE 不会更改模式定义，但作为 DDL 语句，它仍会受到元数据锁的影响）。

当元数据锁的起因是当前语句或最后一条语句时，使用这种方法很方便。在更常见的情况下，可以使用 OBJECT_TYPE 列设置
为 TABLE 的 performance_schema.metadata_ locks 表来查找已授予和待定的元数据锁。清单 18-5 显示了一个已
授权和待授权元数据锁的示例，其设置与上一示例相同。第 22 章将详细介绍如何调查元数据锁。

#+begin_src sql
  select *
  from performance_schema.metadata_locks
  where object_schema='world'
  and object_name='city'\G
#+end_src

在示例中，由于正在进行事务，线程 id 61（与 sys.session 输出中的 conn_id = 22 相同）拥有 world.city 表上
的共享读锁，而线程 id 62 正在等待锁，因为它正试图执行表上的 DDL 语句。

*** 显示表锁

通过 LOCK TABLES 和 FLUSH TABLES WITH READ LOCK 语句可以获得显式表锁。LOCK TABLES 语句可以使用共享锁或
独占锁；FLUSH TABLES WITH READ LOCK 总是使用共享锁。表一直处于锁定状态，直到使用 UNLOCK TABLES 语句明确
释放为止。当执行 FLUSH TABLES WITH READ LOCK 时，如果没有列出任何表，则会使用全局读锁（即影响所有表）。虽然
这些锁也保护数据，但它们在 MySQL 中被视为元数据锁。

除了在备份时使用 "FLUSH TABLES WITH READ LOCK "外，InnoDB并不经常使用显式表锁，因为InnoDB复杂的锁功能
在大多数情况下都比自己处理锁要好。不过，如果你真的需要锁定整个表，显式锁可能会很有用，因为它们对MySQL的检查成
本很低。

连接对 world.country 表和 world.countrylanguage 表有显式读取锁，对 world.city 表有写入锁的示例是

#+begin_src sql
  lock tables world.country READ,
              world.countrylanguage READ,
              world.city WRITE;

#+end_src

在使用显式锁时，只允许使用已锁定的表，而且必须与所请求的锁一致。这就意味着，如果你使用了读锁并试图向表写入（
ER_TABLE_NOT_LOCKED_FOR_WRITE），或者试图使用一个你没有锁定的表（ER_TABLE_NOT_LOCKED），都会出现错误：

#+begin_src sql
  update world.country
  set Population=Population+1
  where Code='AUS';
  select * from sakila.film
  where film_id=1;
#+end_src

由于显式锁被视为元数据锁，因此 performance_schema.metadata_locks 表中的症状和信息与隐式元数据锁相同。

*** 隐式锁

在查询表时，MySQL 会使用隐式表锁。除了刷新、元数据和显式锁之外，表锁在 InnoDB 表中的作用并不大，因为 InnoDB
使用记录锁来允许并发访问表，只要事务不修改相同的行（粗略地说--正如接下来的小节所示--不止如此）。

不过，InnoDB 在表级别使用了意向锁的概念。由于在调查锁问题时很可能会遇到意图锁，因此值得熟悉一下。正如在讨论锁
访问级别时提到的，意图锁标记了事务的意图。如果使用显式 LOCK TABLES 语句，表将直接按照所请求的访问级别被锁定。

对于事务获取的锁，首先会获取一个意向锁，然后根据需要升级。要获取共享锁，事务首先要获取意向共享锁，然后再获取共
享锁。同样，要获取独占锁，首先要获取意图独占锁。意图锁的一些示例如下：

+ SELECT ... FOR SHARE 语句对所查询的表进行意向共享锁定。SELECT ... LOCK IN SHARE 模式语法是一个同义词。
+ SELECT ... FOR UPDATE 语句会对所查询的表进行意向排他性锁定。
+ DML 语句（不包括 SELECT）会对修改的表使用意图独占锁。如果修改了外键列，则会在父表上使用意图共享锁。

两个意图锁总是相互兼容的。这意味着，即使一个事务拥有意图独占锁，也不会阻止另一个事务获得意图锁。但它会阻止另一个
事务将其意图锁升级为完全锁。表 18-1 显示了锁类型之间的兼容性。共享锁以 S 表示，独占锁以 X 表示。意图锁的前缀是
I，因此 IS 是意图共享锁，IX 是意图独占锁。  

|          | 排他锁 | 意向排他锁 | 共享锁 | 意向共享锁 |
|----------+-------+----------+-------+----------|
| 排他锁    | ✘     | ✘        | ✘     | ✘        |
| 意向排他锁 | ✘     | ✓        | ✘     | ✓        |
| 共享锁    | ✘     | ✘        | ✓     | ✓        |
| 意向共享锁 | ✘     | ✓        | ✓     | ✓        |

在表中，复选标记表示两个锁是兼容的，而交叉标记表示两个锁相互冲突。唯一冲突的意图锁是独占锁和共享锁。独占锁与所有
其他锁冲突，包括两种意图锁类型。共享锁只与独占锁和意图独占锁冲突。

为什么需要意图锁？它们允许 InnoDB 按顺序解决锁请求，而不会阻塞兼容操作。具体细节不在本文讨论范围之内。重要的是，
你要知道意图锁的存在，所以当你看到它们时，你就知道它们来自哪里。

表级锁位于 performance_schema.data_locks 表中，LOCK_TYPE 列设置为 TABLE。清单 18-6 显示了一个意图共享
锁的示例。

#+begin_src sql
  start transaction;
  select *
  from world.city
  where id=130;

  select *
  from world.city
  where id=130 for share;
#+end_src

#+begin_src sql
  select * from performance_schema.data_locks;
#+end_src

#+begin_src sql
  rollback;
#+end_src

表级锁位于 performance_schema.data_locks 表中，LOCK_TYPE 列设置为 TABLE。清单 18-6 显示了一个意图共享
锁的示例。

如前所述，InnoDB 的主要访问级别保护位于记录级别，因此我们来看看这些记录。

*** 记录锁

记录锁通常被称为行锁；然而，它不仅仅是行锁，还包括索引锁和间隙锁。在谈到 InnoDB 锁时，通常指的就是这些锁。它们
是细粒度锁，旨在锁定最少的数据，同时确保数据完整性。

记录锁可以是共享的，也可以是独占的，只影响事务访问的行和索引。独占锁的持续时间通常是事务的持续时间，但也有例外，
例如在 INSERT INTO ... 和 REPACE 语句中用于唯一性检查的删除标记记录。ON DUPLICATE KEY 和 REPLACE 语句
中用于唯一性检查的删除标记记录。对于共享锁，持续时间可能取决于事务隔离级别，"减少锁定问题 "一节中的 "事务隔离级
别 "对此进行了讨论。

记录锁可使用 performance_schema.data_locks 表查找，该表也用于查找表级别的意向锁。清单 18-7 显示了使用二
级索引 CountryCode 更新 world.city 表中记录的锁的示例。

#+begin_src sql
  start transaction;
  update world.city
  set Population=Population+1
  where CountryCode='LUX';
#+end_src

#+begin_src sql
  select *
  from performance_schema.data_locks\G
#+end_src

#+begin_src sql
  rollback;
#+end_src

第一行是已经讨论过的意向排他性表锁。第二行是对 CountryCode 索引值（'LUX, ' 2452）的下一键锁（稍后详述），
其中'LUX'是 WHERE 子句中使用的国家代码，2452 是添加到非唯一二级索引中的主键 id。ID = 2452 的城市是唯一匹配
WHERE 子句的城市，主键记录（行本身）显示在输出的第三行。锁定模式为 X,REC_NOT_GAP，这意味着是对记录的独占锁，
而不是对间隙的锁。


*** 间隙锁、次键锁和谓词锁

间隙锁保护两条记录之间的空间。这可以是在通过聚类索引的行中，也可以是在二级索引中。在索引页的第一条记录之前和最后
一条记录之后，有两个伪记录，分别称为下位记录（infimum record）和上位记录（supremum record）。间隙锁通常是最
容易引起混淆的锁类型。研究锁问题的经验是熟悉它们的最好方法。

请看上一个例子中的查询：

#+begin_src sql
  update world.city
  set Population=Population+1
  where CountryCode='LUX';
#+end_src

该查询会更改国家代码为 "LUX "的所有城市的人口。如果在事务的更新和提交之间插入了一个新城市，会发生什么情况？如果
UPDATE 和 INSERT 语句的提交顺序与执行顺序相同，则一切正常。但是，如果以相反的顺序提交更改，结果就会不一致，因
为插入的记录应该也已更新。

这就是间隙锁发挥作用的地方。它保护着插入新记录（包括从不同位置移动过来的记录）的空间，因此在持有间隙锁的事务完成
之前，它不会被更改。如果查看清单 18-7 示例输出中第四行的最后一列，就能看到间隙锁的一个示例：

[[./images/AkeSfe.png]]

这是对国家代码索引值（'LVA, ' 2434）的独占间隙锁。由于查询要求更新 CountryCode 设置为 "LUX "的所有记录，
因此间隙锁确保不会为 "LUX "国家代码插入新记录。国家代码 "LVA "是 CountryCode 索引中的下一个值，因此 "LUX"
和 "LVA "之间的间隙受独占锁保护。另一方面，国家代码为 "LVA "的新城市仍然可以插入。在某些地方，这种情况被称为
"记录前的间隙"，这使我们更容易理解间隙锁是如何工作的。

使用 READ COMMITTED 事务隔离级别而不是 REPEATABLE READ 或 SERIALIZABLE 时，间隙锁的使用程度要低得多。
这将在 "减少锁定问题 "一节中的 "事务隔离级别 "中进一步讨论。

与间隙锁相关的是下一键锁和谓词锁。下一键锁是记录锁和记录前的间隙锁的组合。这实际上是 InnoDB 的默认锁类型，因此
在锁输出中只会显示为 S 和 X。在本小节和上一小节讨论的示例中，CountryCode 索引上的值（'LUX, ' 2452）锁和之前
的间隙锁就是下一键锁的示例。清单 18-7 中 performance_schema.data_locks 表输出的相关部分是

[[./images/K6FrIs.png]]


因此，概括地说，第 2 行是下一键锁，第 3 行是主键（行）上的记录锁，第 4 行是 "LUX "和 "LVA "之间的间隙锁（或
前-LVA 间隙锁）。

谓词锁与间隙锁类似，但适用于无法进行绝对排序的空间索引，因此间隙锁没有意义。对于REPEATABLE READ和
SERIALIZABLE事务隔离级别中的空间索引，InnoDB会在查询使用的最小边界矩形（MBR）上创建一个谓词锁，而不是间隙
锁。这将防止最小边界矩形内的数据发生变化，从而实现一致的读取。

*** 意向插入锁

请记住，对于表锁，InnoDB 有意图锁，用于确定事务是以共享还是独占的方式使用表。同样，InnoDB在记录级别也有插入
意图锁。顾名思义，InnoDB在INSERT语句中使用这些锁，向其他事务发出插入意图信号。因此，锁是针对尚未创建的记录
的（所以是间隙锁），而不是针对现有记录的。使用插入意图锁有助于提高插入操作的并发性。

除非 INSERT 语句正在等待授予锁，否则你不太可能在锁输出中看到插入意图锁。你可以在另一个事务中创建一个间隙锁，阻止
INSERT 语句完成，从而迫使这种情况发生。清单 18-8 中的示例在连接 1 中创建了间隙锁，然后在连接 2 中尝试插入一
条与间隙锁冲突的记录。最后，在第三个连接中检索锁信息。

#+begin_src sql
  start transaction;
  select * from world.city
  where id>4079
  for update;
#+end_src

#+begin_src sql
  select ps_current_thread_id();
  start transaction;
  insert into world.city
  values(4080, 'Darwin', 'AUS',
        'Northern Territory', 146000);
#+end_src

#+begin_src sql
  select *
  from performance_schema.data_locks;
#+end_src

#+begin_src sql
  rollback;
#+end_src

#+begin_src sql
  rollback;
#+end_src

连接 2 拥有 Performance Schema 线程 ID 62，因此在连接 3 中，只需查询该线程就可以排除连接 1 占用的锁。请
注意，对于 RECORD 锁，锁模式包括 INSERT_INTENTION - 插入意图锁。在这种情况下，锁定的数据是上位伪记录，但也
可以是主键的值，这取决于具体情况。如果你还记得关于下一键锁的讨论，那么 X 就意味着下一键锁，但这是一种特殊情况，
因为锁是在上伪记录上的，而这是不可能锁定的，所以实际上它只是上伪记录之前的间隙锁。

*** 自增锁

在具有自动递增计数器的表中插入数据时，有必要保护计数器，以确保两个事务获得唯一值。如果使用基于语句的日志记录到二
进制日志，则会有更多限制，因为在重放语句时，除了第一行外，所有行的自动递增值都会重新创建。

InnoDB支持三种锁定模式，因此你可以根据需要调整锁定量。你可以使用 innodb_autoinc_lock_mode 选项来选择锁定
模式，该选项的值有 0、1 和 2，在 MySQL 8 中默认值为 2。 改变该值需要重启 MySQL。表 18-2 概括了这些值的含义。

| 值 | 模式 | 描述                                                                                                               |
|---+-----+-------------------------------------------------------------------------------------------------------------------|
| 0 | 传统 | 锁会保持到语句结束，因此值会以可重复和连续的顺序分配。                                                                     |
| 1 | 连续 | 对于在查询开始时已知行数的 INSERT 语句，会在轻量级互斥下分配所需的自动递增值，并避免使用自动递增锁。对于不知道行数的语句，会使用自动增加锁并保持到语句结束。 |
| 2 | 间隔 | 该模式只有在二进制日志被禁用或 binlog_format 被设置为 ROW 时才安全，它是 MySQL 8 中的默认值。                                |

innodb_autoinc_lock_mode 的值越大，锁定的次数就越少。为此付出的代价是自动递增值序列中的间隙数量增加，以及
innodb_autoinc_lock_mode = 2 时出现交错值的可能性。除非你不能使用基于行的二进制日志记录，或者对连续的自动
递增值有特殊需求，否则建议使用 2。

*** 备份锁

备份锁是实例级锁；也就是说，它会影响整个系统。它是 MySQL 8 中引入的一种新锁。 备份锁可以阻止那些可能导致备份不
一致的语句，同时还允许其他语句与备份同时执行。被阻止的语句包括

+ 创建、重命名或删除文件的语句。这包括 CREATE TABLE、CREATE TABLESPACE、RENAME TABLE 和 DROP TABLE
  语句。
+ 账户管理语句，如 CREATE USER、ALTER USER、DROP USER 和 GRANT。
+ 不将更改记录到重做日志中的 DDL 语句。例如，这包括添加索引。

备份锁通过 LOCK INSTANCE FOR BACKUP 语句创建，并通过 UNLOCK INSTANCE 语句释放。执行 LOCK INSTANCE
FOR BACKUP 需要 BACKUP_ADMIN 权限。获取备份锁并再次释放的示例如下  

#+begin_src sql
  lock instance for backup;

  unlock instance;
#+end_src

#+begin_comment
  在撰写本文时，使用 X 协议（通过 mysqlx_ port 指定的端口或 mysqlx_socket 指定的套接字进行连接）时不允许
  使用备份锁并释放它： ERROR: 3130： Command not supported by pluggable protocols。
#+end_comment

此外，与备份锁冲突的语句也会占用备份锁。由于 DDL 语句有时包含多个步骤，例如在新文件中重建表并重命名文件，因此可
以在这些步骤之间释放备份锁，以避免阻塞 LOCK INSTANCE FOR BACKUP 超过必要时间。

备份锁可以在 OBJECT_TYPE 列设置为 BACKUP LOCK 的 performance_schema.metadata_locks 表中找到。清单
18-9 显示了一个查询等待由 LOCK INSTANCE FOR BACKUP 持有的备份锁的示例。

#+begin_src sql
  lock instance for backup;
#+end_src

#+begin_src sql
  optimize table world.city;
#+end_src

#+begin_src sql
  select *
  from performance_schema.metadata_locks
  where object_type='BACKUP LOCK'\G
#+end_src

#+begin_src sql
  unlock instance;
#+end_src

在示例中，线程 id 49 的连接拥有备份锁，而线程 id 60 的连接正在等待备份锁。请注意，LOCK INSTANCE FOR BACKUP
拥有共享锁，而 DDL 语句请求的是意图独占锁。

*** 日志锁

创建备份时，通常希望包含备份与日志位置一致的信息。在 MySQL 5.7 及更早版本中，您需要使用全局读锁来获取这些信息。
在 MySQL 8 中，日志锁被引入，允许你在不使用全局读锁的情况下读取信息，如已执行的全局事务标识符（GTID）、二进制
日志位置和 InnoDB 的日志序列号（LSN）。

日志锁可防止更改日志相关信息的操作。在实际操作中，这意味着提交、FLUSH LOGS 等类似操作。日志锁通过查询
performance_schema.log_status 表来隐式获取。访问该表需要 BACKUP_ADMIN 权限。清单 18-10 显示了
log_ status 表的输出示例。

#+begin_src sql
  select *
  from performance_schema.log_status\G
#+end_src

** 未获取锁

锁的整个概念是限制对对象或记录的访问，以避免同时执行相互冲突的操作。这意味着有时无法授予锁。在这种情况下会发生什么？
这取决于所请求的锁和具体情况。元数据锁（包括明确请求的表锁）会有超时。InnoDB 记录锁支持超时和显式死锁检测。

#+begin_comment
  要确定两个锁是否相互兼容是非常复杂的。这一点变得特别有趣，因为它们之间的关系不是对称的，也就是说，一个锁可能允许
  另一个锁的存在，但反之亦然。例如，插入意图锁必须等待间隙锁，但间隙锁不需要等待插入意图锁。另一个例子（缺乏传递性）
  是，间隙加记录锁必须等待纯记录锁，插入意图锁必须等待间隙加记录锁，但插入意图锁不需要等待纯记录锁。
#+end_comment

重要的是要明白，在使用数据库时，获取锁失败是不争的事实。原则上，你可以使用非常粗粒度的锁，并避免失败的锁，超时除外，
MyISAM 存储引擎就是这样做的，结果是写并发性非常差。然而，在实践中，为了实现写入工作负载的高并发性，我们更倾向于使
用细粒度锁，这也会带来死锁的可能性。

总之，你应该让你的应用程序时刻准备好重试获取锁或优雅地失败。无论是显式锁还是隐式锁，这一点都适用。

#+begin_comment
 始终做好处理无法获得锁的准备。不过，正如 "减少锁定问题 "一节所讨论的，在开发应用程序时，有一些减少锁定争用的技术
 值得注意。
#+end_comment

*** 元数据和Backup锁等待超时

请求刷新、元数据或备份锁时，获取锁的尝试将在 lock_wait_timeout 秒后超时。默认超时时间为 31536000 秒（365
天）。你可以在全局和会话范围内动态设置 lock_wait_timeout 选项，这样就能根据特定进程的具体需要调整超时时间。

当超时发生时，语句会以 ER_LOCK_WAIT_TIMEOUT 错误（错误编号 1205）失败。例如

#+begin_src sql
  lock tables world.city write;
#+end_src

lock_wait_timeout 选项的推荐设置取决于应用程序的要求。使用一个较小的值可以防止锁请求长时间阻塞其他查询。这通
常需要对锁定请求失败进行处理，例如重试语句。另一方面，使用大值可以避免重试语句。对于 FLUSH TABLES 语句，还要记
住它与底层表定义缓存（TDC）版本锁交互，这可能意味着放弃该语句并不允许后续查询继续进行。在这种情况下，最好为
lock_wait_timeout 设置一个较高的值，以便更清楚地说明锁的关系。

*** innodb锁等待超时

当查询请求 InnoDB 中的记录级锁时，会受到与刷新、元数据和备份锁的超时类似的超时限制。由于记录级锁争用比表级锁争
用更常见，而且记录级锁会增加死锁的可能性，所以超时默认为50秒。可以使用 innodb_ lock_wait_timeout 选项来
设置超时时间。

当超时发生时，查询会以 ER_LOCK_WAIT_TIMEOUT 错误（错误编号 1205）失败，就像表级锁超时一样。清单 18-11 显
示了发生 InnoDB 锁等待超时的示例。

#+begin_src sql
  start transaction;

  update world.city
  set Population=Population+1
  where id=130;
#+end_src

#+begin_src sql
  set session innodb_lock_wait_timeout=3;
  update world.city
  set Population=Population+1
  where id=130;
#+end_src

#+begin_src sql
  rollback;
#+end_src

在本例中，连接 2 的锁定等待超时设置为 3 秒，因此无需等待通常的 50 秒超时。

当超时发生时，innodb_rollback_on_timeout（超时时回滚）选项会定义回滚多少事务工作。禁用
innodb_rollback_on_timeout（默认值）时，只回滚触发超时的语句。启用该选项后，整个事务都会回滚。
innodb_rollback_on_timeout选项只能在全局级别配置，而且需要重启才能更改值。

#+begin_comment
  处理锁等待超时是非常重要的，否则可能会给事务留下未释放的锁。
#+end_comment

一般建议将 InnoDB 记录级锁的超时时间保持在较低水平。通常情况下，最好将默认值从 50 秒降至更低。允许查询等待锁的
时间越长，其他锁请求受影响的可能性就越大，这可能导致其他查询也停滞不前。这也会增加发生死锁的可能性。如果你禁用了
死锁检测（将在下文讨论），就应该为 innodb_lock_wait_timeout 设置一个很小的值，比如一秒或两秒，因为你将使用
超时来检测死锁。如果没有死锁检测，也建议启用 innodb_rollback_on_timeout 选项。

*** 死锁

死锁听起来是一个非常可怕的概念，但你不应该被它的名字所吓倒。就像锁等待超时一样，死锁也是高并发数据库世界中的一个
事实。它的真正含义是锁请求之间存在循环关系。解决僵局的唯一办法就是迫使其中一个请求放弃。从这个意义上说，死锁与锁
等待超时并无不同。事实上，你可以禁用死锁检测，在这种情况下，其中一个锁将以锁等待超时结束。

那么，既然不需要死锁，为什么还会有死锁呢？由于死锁是在锁请求之间存在循环关系时发生的，因此InnoDB可以在循环完
成后立即检测到死锁。这样，InnoDB 就可以立即告诉用户发生了死锁，而不必等待锁等待超时。被告知发生了死锁也是很有用
的，因为这往往提供了改进应用程序中数据访问的机会。因此，应将死锁视为朋友而不是敌人。图 18-1 显示了两个事务查询表
导致死锁的示例。

[[./images/7CAVag.png]]

在示例中，事务 1 首先更新 ID = 130 的记录，然后更新 ID = 3805 的记录。在这之间，事务 2 首先更新 ID = 3805
的记录，然后更新 ID = 130 的记录。这意味着，当事务 1 尝试更新 ID = 3805 的记录时，事务 2 已经锁定了该行。事
务 2 也无法继续，因为它无法获得 ID = 130 上的锁，因为事务 1 已经持有该锁。这是一个简单死锁的典型示例。循环锁关
系也如图 18-2 所示。

[[./images/CubdJ1.png]]

从图中可以清楚地看出，事务 1 和事务 2 分别持有哪个锁，哪个锁被请求，以及如何在没有干预的情况下永远无法解决冲突。
这使得它成为一个死锁。

在现实世界中，死锁往往更为复杂。在这里讨论的例子中，只涉及主键记录锁。一般来说，通常还会涉及次键、间隙锁和其他可
能的锁类型。还可能涉及两个以上的事务。但是，原理是相同的。

#+begin_comment
  死锁甚至可能发生在两个事务中每个事务只有一个查询的情况下。如果一个查询按升序读取记录，另一个按降序读取记录，就
  有可能出现死锁。
#+end_comment

当发生死锁时，InnoDB 会选择 "工作最少 "的事务成为受害者。你可以检查 information_ schema.INNODB_TRX 视图
中的 trx_weight 列，查看 InnoDB 使用的权重（完成的工作越多，权重越高）。实际上，这意味着持有锁最少的事务将被
回滚。发生这种情况时，被选为受害者的事务中的查询会失败，并返回错误 ER_LOCK_DEADLOCK（错误代码 1213），事务会
被回滚以释放尽可能多的锁。发生死锁的示例如清单 18-12 所示。

#+begin_src sql
  start transaction;
  update world.city
  set Population=Population+1
  where id=130;
#+end_src

#+begin_src sql
  start transaction;
  update world.city
  set Population=Population+1
  where id=3805;
#+end_src

#+begin_src sql
  update world.city
  set Population=Population+1
  where id=3805;
#+end_src

#+begin_src sql
  rollback;
#+end_src

#+begin_src sql
  rollback;
#+end_src

在大多数情况下，自动死锁检测功能非常强大，可以避免查询停滞超过所需的时间。不过，死锁检测不是免费的。对于查询并发
量非常高的MySQL实例来说，寻找死锁的成本可能会很高，因此最好禁用死锁检测，将innodb_deadlock_detect选项
设为OFF即可。不过，在MySQL 8.0.18及更高版本中，死锁检测被移到了专门的后台线程，从而提高了性能。

如果确实禁用了死锁检测，建议将innodb_lock_wait_超时设置为很低的值，比如一秒，以便快速检测锁竞争。此外，还要
启用 innodb_rollback_on_timeout 选项，以确保锁被释放。

既然已经了解了锁的工作原理和锁请求失败的原因，就需要考虑如何减少锁的影响。

** 减少锁造成的问题

在编写应用程序和设计其数据和访问模式时，必须考虑到锁定问题。减少锁定的策略包括添加索引、更改事务隔离级别和抢占式锁
定。

#+begin_comment
  如果只是偶尔遇到锁等待超时和死锁问题，通常最好重试查询或事务，而不是花时间回避问题。多频繁取决于你的工作量，但对
  许多应用程序来说，每小时重试几次并不是问题。
#+end_comment

*** 事务大小和年龄

减少锁问题的一个重要策略是将事务保持在较小规模，并避免出现使事务开放时间超过必要时间的延迟。造成锁问题的最常见原
因是修改大量行的事务或活动时间超过必要时间的事务。

事务的大小是指事务的工作量，尤其是所占用锁的数量，但事务的执行时间也很重要。正如本讨论的其他一些主题所涉及的，可
以通过索引和事务隔离级别来减少部分影响。不过，考虑整体结果也很重要。如果需要修改很多行，请考虑是否可以将工作分成
较小的批次，或者是否需要在同一个事务中完成所有工作。也可以将一些准备工作拆分出来，在主事务之外进行。

#+begin_comment
  启用autocommit选项，除非有特殊原因。启用自动提交选项后，innodB 还能在许多 SELECT 查询中检测到这是一个只
  读事务，从而减少查询的开销。
#+end_comment

另一个陷阱是启动事务，并在事务激活时在应用程序中执行慢速操作。这可能是发送回用户的数据、交互式提示或文件 I/O。确
保在 MySQL 中没有打开活动事务时执行此类慢速操作。

*** 索引

索引可以减少访问给定记录的工作量。因此，索引是减少锁定的好工具，因为只有在执行查询时访问的记录才会被锁定。

举个简单的例子，在 world.city 表中查询名称为 Sydney 的城市：

#+begin_src sql
  start transaction;

  select *
  from world.city
  where Name='Sydeney' for share;
#+end_src

FOR SHARE 选项用于强制查询对读取的记录使用共享锁。默认情况下，"name"列上没有索引，因此查询将执行全表扫描以查找
结果中所需的记录。在没有索引的情况下，有 4103 个记录锁（有些是重复的）：

#+begin_src sql
  select INDEX_NAME, LOCK_TYPE,
         LOCK_MODE, COUNT(*)
  from performance_schema.data_locks
  where OBJECT_SCHEMA='world'
        and OBJECT_NAME='city'
  group by INDEX_NAME, LOCK_TYPE, LOCK_MODE;
#+end_src

如果在 "name"列上添加索引，锁数将减少到总共三个记录锁：

#+begin_src sql
  select INDEX_NAME, LOCK_TYPE,
         LOCK_MODE, COUNT(*)
  from performance_schema.data_locks
  where OBJECT_SCHEMA='world'
        and OBJECT_NAME='city'
  group by INDEX_NAME, LOCK_TYPE, LOCK_MODE;
#+end_src

如果在 "Name"列上添加索引，锁数将减少到总共三个记录锁：

*** 记录访问顺序

确保不同事务访问记录的顺序尽可能一致。在本章前面讨论的死锁示例中，导致死锁的原因是两个事务访问记录的顺序相反。
如果它们以相同的顺序访问记录，就不会出现死锁。这同样适用于访问不同表中的记录。

确保相同的访问顺序绝非易事。在执行连接时，优化程序甚至会为两个查询决定不同的连接顺序，从而导致不同的访问顺序。
如果不同的连接顺序会导致过多的锁问题，可以考虑使用第 17 章中描述的优化器提示来告诉优化器更改连接顺序，但在这种
情况下当然也要考虑查询性能。

*** 事务隔离级别

InnoDB 支持多种事务隔离级别。不同的隔离级别有不同的锁要求：特别是 REPEATABLE READ 和 SERIALIZABLE，它们比
READ COMMITTED 需要更多的锁。

READ COMMITTED 事务隔离级别在两个方面有助于解决锁定问题。使用的间隙锁会少得多，而且在 DML 语句中被访问但未修
改的行会在语句完成后再次被释放锁。对于 REPEATABLE READ 和 SERIALIZABLE，只有在事务结束时才会释放锁。

#+begin_comment
  人们常说，READ COMMITTED事务隔离级别不需要间隙锁。虽然间隙锁的数量要少得多，但仍有一些间隙锁是需要的。例如，
  当 innodB 在更新中执行页面分割时，就需要间隙锁（页面分割将在第 25 章讨论）。(页面分割将在第 25 章讨论）。
#+end_comment

例如，使用国家代码列更改名为悉尼的城市人口，将查询限制在一个国家。可以通过以下查询来实现：

#+begin_src sql
  start transaction;

  update world.city
  set Population=5000000
  where name='Sydney'
    and CountryCode='AUS';
#+end_src

Name 列上没有索引，但 CountryCode 列上有索引。因此，更新需要扫描 CountryCode 索引的一部分。清单 18-13 显
示了在 REPEATABLE READ 事务隔离级别下执行查询的示例。

#+begin_src sql
  set transaction_isolation='REPEATABLE-READ';
  start transaction;
  update world.city
  set Population=5000000
  where Name='Sydney'
     and CountryCode='AUS';
#+end_src

#+begin_src sql
  select INDEX_NAME, LOCK_TYPE,
         LOCK_MODE, COUNT(*)
  from performance_schema.data_locks
  where OBJECT_SCHEMA='world'
        and OBJECT_NAME='city'
  group by INDEX_NAME, LOCK_TYPE, LOCK_MODE;
#+end_src

#+begin_src sql
  rollback;
#+end_src

在 CountryCode 索引和主键上分别加了 14 个记录锁，在 CountryCode 索引上加了一个间隙锁。如清单 18-14 所示，
比较一下在 READ COMMITTED 事务隔离级别下执行查询后所持有的锁。

在这里，记录锁减少为 CountryCode 索引和主键上各一个锁。没有间隙锁。

并非所有工作负载都能使用 READ COMMITTED 事务隔离级别。如果必须让 SELECT 语句在同一事务中多次执行时返回相同的
结果，或者让不同的查询对应同一时间快照，就必须使用 REPEATABLE READ 或 SERIALIZABLE。不过，在许多情况下，可
以选择降低隔离级别，而且可以为不同的事务选择不同的隔离级别。如果你正在从 Oracle DB 迁移应用程序，那么你已经在使
用 READ COMMITTED，在 MySQL 中也可以使用它。

*** 抢占式锁

最后要讨论的策略是抢占式锁定。如果您有一个执行多个查询的复杂事务，那么在某些情况下，执行 SELECT ... FOR
UPDATE 或 SELECT ... FOR SHARE 查询来锁定您知道在事务中稍后会用到的记录会更有优势。另一种有用的情况是确保不
同任务以相同的顺序访问记录。

抢占式加锁对减少死锁的频率特别有效。它的一个缺点是，你最终会持有锁的时间更长。总的来说，抢占式锁定是一种应谨慎使
用的策略，但在正确的情况下，它可以有效防止死锁。

** 监控锁

前面已经举了几个查询所持锁信息的例子。本节将回顾已提及的来源，并介绍一些额外的来源。第 22 章将进一步介绍调查锁问题
的示例。监控选项可分为四组：性能模式、系统模式、状态度量和 InnoDB 锁监控。

*** 性能模式

性能模式中包含了除死锁外的大部分锁信息。您不仅可以直接使用性能模式中的锁信息，还可以将其用于系统模式中两个与锁相
关的视图。

这些信息通过四个表格提供：

+ data_locks:该表包含 InnoDB 级别的表和锁记录详情。它显示了当前持有或待处理的所有锁。
+ data_lock_waits: 与 data_locks 表一样，该表显示与 InnoDB 相关的锁，但只显示那些等待授予的锁，以及阻塞请
  求的线程信息。
+ metadata_locks: 此表包含用户级锁、元数据锁等类似信息。要记录信息，必须启用 wait/ lock/metadata/sql/mdl
   性能模式工具（MySQL 8 默认已启用）。OBJECT_TYPE 列显示持有的锁类型。
+ table_handles: 该表保存当前哪些表锁生效的信息。必须启用 wait/lock/table/sql/handler 性能模式工具才能记
  录数据（默认情况）。该表的使用频率低于其他表。

metadata_locks 表是最通用的表，支持从全局读取锁到访问控制列表（ACL）等低级锁的各种锁。表 18-3 按字母顺序总
结了 OBJECT_TYPE 列的可能值，并简要说明了每个值所代表的锁。

#+performance_schema.metadata_locks
| 对象类型            | 描述                                                  |
|-------------------+------------------------------------------------------|
| ACL_CACHE         | ACL缓存                                               |
| BACKUP_LOCK       | backup锁                                              |
| CHECK_CONSTRAINT  | check约束的名称                                        |
| COLUMN_STATISTICS | 其他列的直方图                                          |
| COMMIT            | 阻塞的提交，相关全局读锁                                 |
| EVENT             | 存储的事件                                             |
| FOREIGN_KEY       | 外键名                                                |
| GLOBAL            | FLUSH TABLES WITH READ LOCK的全局锁锁                  |
| FUNCTION          | 存储函数                                               |
| LOCKING_SERVICE   | 对于使用锁定服务接口获取的锁。                            |
| PROCEDURE         | 存储过程                                               |
| RESOURCE_GROUPS   | 资源组                                                |
| SCHEMA            | 对于模式/数据库，这些锁与表的元数据锁类似，但它们是针对模式的。 |
| SRID              | 空间参照系统（Srids）。                                 |
| TABLES            | 对于表和视图，这包括本章讨论的元数据锁。                    |
| TABLESPACE        | 表空间                                                |
| TRIGGER           | 触发器                                                |
| USER_LEVEL_LOCK   | 用户锁                                                |

性能模式表中的数据是原始锁数据。通常，在调查锁问题或监控锁问题时，确定是否存在任何锁等待更有意思。要获取该信息，
需要使用 sys 模式。

*** sys模式

系统模式有两个视图，它们利用性能模式表中的信息，返回因另一个锁而无法授予一个锁的锁对。因此，这两个视图可以显示哪
些地方存在锁等待问题。这两个视图分别是 innodb_lock_waits 和 schema_table_lock_waits。

innodb_lock_waits 视图使用性能模式中的 data_locks 和 data_lock_waits 视图来返回 InnoDB 记录锁的所有锁
定等待情况。它显示的信息包括连接试图获取的锁以及涉及的连接和查询。如果需要不带格式化的信息，该视图也可以
x$innodb_lock_waits 的形式存在。

schema_table_lock_waits 视图的工作方式与此类似，但它使用 metadata_ locks 表来返回与模式对象相关的锁等待
信息。这些信息也可以在 x$schema_table_lock_waits 视图中以未格式化的方式提供。

*** 状态计数器和 InnoDB 指标

有几种状态计数器和 InnoDB 指标可以提供有关锁定的信息。这些指标主要用于全局（实例）级别，可用于检测锁问题的总体
增加情况。使用 sys.metrics 视图是一起监控所有这些指标的好方法。清单 18-15 展示了一个检索指标的示例。

#+begin_src sql
  select Variable_name, Variable_value as Value,
         Enabled
  from sys.metrics
  where Variable_name like 'innodb_row_lock%'
      or Variable_name like 'Table_locks%'
      or Type='InnoDB Metrics - lock';
#+end_src

如你所见，并非所有指标都是默认启用的。未启用的指标可以使用第7章讨论的innodb_monitor_enable选项启用。
innodb_row_lock_%、lock_deadlocks和lock_timeouts是最有趣的指标。行锁指标显示当前有多少个锁在等待，以
及等待获取 InnoDB 记录锁所花费的时间（以毫秒为单位）。lock_deadlocks（死锁）和 lock_timeouts 指标分别显示
死锁和锁等待超时的次数。

*** InnoDB 锁定监控器和死锁日志

长期以来，InnoDB 都有自己的锁监控器，并在 InnoDB 监控器输出中返回锁信息。默认情况下，InnoDB监控器包括最新死
锁以及锁等待中涉及的锁的信息。启用 innodb_ status_output_locks 选项（默认禁用）后，所有锁信息都会被列出；这
与性能模式 data_locks 表中的信息类似。

要演示死锁和事务信息，可以创建清单 18-12 中的死锁，并创建一个新的持续事务，该事务已按主键更新了 world.city 表
中的一条记录：

#+begin_src sql
  start transaction;
  update world.city
  set Population=Population+1
  where ID=130;
#+end_src

可以使用 SHOW ENGINE INNODB STATUS 语句生成 InnoDB 锁监控输出。清单 18-16 展示了一个启用所有锁信息并生成
监控输出的示例。本书 GitHub 代码库中的 listing_18_16.txt 文件也提供了完整的 InnoDB 监控输出。

#+begin_src sql
  set global innodb_status_ouput_locks=on;
  show engine innodb status\G
#+end_src

靠近顶部的部分是最新检测到的死锁，其中包括最新死锁涉及的事务和锁的详细信息以及发生时间。如果自上次重启 MySQL 以
来没有发生过死锁，则省略此部分。第 22 章将包括一个调查死锁的示例。

#+begin_comment
  innodB 监控输出中的死锁部分只包括涉及 innodB 记录锁的死锁信息。对于涉及用户级锁的死锁，则没有相应的信息。
#+end_comment

再往下一点，输出中的 TRANSACTIONS 部分列出了 InnoDB 事务。请注意，不持有任何锁的事务（如纯 SELECT 查询）不
包括在内。在示例中，world.city 表上有一个意向独占锁，主键等于 130 的记录上有一个独占锁（第一个字段的记录锁信息
中的 80000082 表示值为 0x82 的记录，十进制表示法与 130 相同）。

#+begin_comment
  现在，innodB 监控输出中的锁信息最好从 performance_schema.data_locks 和 performance_ schema.
  data_lock_waits 表中获取，但死锁信息仍然非常有用。
#+end_comment

你可以要求每 15 秒将监控输出转储到 stderr。通过启用 innodb_status_output 选项，就能启用转储。需要注意的是，
输出量相当大，所以如果你启用了它，就要做好错误日志快速增长的准备。InnoDB 监控输出也很容易隐藏更严重问题的信息。

如果想确保记录所有死锁，可以启用 innodb_print_ all_deadlocks 选项。这样，每次发生死锁时，InnoDB监控输出
中的死锁信息就会被打印到错误日志中。如果你需要调查死锁，这可能会很有用，但建议只在需要时启用，以避免错误日志变
得非常大，并可能隐藏其他问题。

#+begin_comment

  如果启用 innodB 监视器的定期输出或所有死锁信息，请务必小心。这些信息很容易掩盖记录在错误日志中的重要信息。
#+end_comment

** 总结

锁的话题庞大而复杂。希望本章能帮助你大致了解为什么需要锁和各种锁。

本章一开始就提出了为什么需要锁的问题。没有锁，并发访问模式和数据是不安全的。打个比方，数据库锁的作用就像交通灯和停
车标志在交通中的作用一样。它规范了对数据的访问，因此事务可以确保不会与其他事务发生碰撞，造成不一致的结果。

数据有两种访问级别：共享访问（也称为读取访问）和独占访问（也称为写入访问）。这些访问级别适用于不同的锁粒度，从全局
读锁到记录锁和间隙锁。此外，InnoDB 还在表级别使用意图共享锁和意图独占锁。

重要的是，要努力减少应用程序所需锁的数量，并降低所需锁的影响。减少锁问题的策略基本上可以归结为：通过使用索引和将大
型事务拆分成较小的事务，在事务中做尽可能少的工作，并在尽可能短的时间内保持锁。同样重要的是，应用程序中的不同任务应
尝试以相同的顺序访问数据；否则，可能会出现不必要的死锁。

本章最后一部分介绍了性能模式、系统模式、状态度量和 InnoDB 监控中的锁监控选项。大多数监控最好使用性能模式表和系统
模式视图。死锁是个例外，InnoDB 监控仍然是最佳选择。

这是第四部分的结论。现在是时候让查询分析变得更实用了，首先是找到需要优化的候选查询。

























* 分析查询

在上一章中，您学习了如何查找需要优化的查询。现在是时候进行下一步了--分析查询，以确定其执行效果不如预期的原因。分析过
程中的主要工具是 EXPLAIN 语句，它会显示优化器将使用的查询计划。与此相关的是优化器跟踪，可用于研究优化器最终使用查询
计划的原因。另一种方法是使用性能模式中的语句和阶段信息，查看存储过程或查询在哪里花费的时间最多。本章将讨论这三个主题。

对 EXPLAIN 语句的讨论是本章迄今为止篇幅最长的部分，共分为四节：

+ EXPLAIN 使用: EXPLAIN的基本用法
+ EXPLAIN格式: 查询计划可查看的每种格式的具体细节。这包括通过 EXPLAIN 语句明确选择的格式和 MySQL 工作台使用的
   Visual Explain 格式。
+ EXPLAIN输出: 对查询计划中可用信息的讨论。
+ EXPLAIN 示例: 使用 EXPLAIN 语句的一些示例，并讨论返回的数据。


** EXPLAIN使用

EXPLAIN 语句返回 MySQL 优化器将用于给定查询的查询计划概览。它既是非常简单的查询优化工具，也是比较复杂的查询优化
工具之一。说它简单，是因为你只需在要研究的查询前添加 EXPLAIN 命令；说它复杂，是因为要理解这些信息，需要对 MySQL
及其优化器的工作方式有一定的了解。EXPLAIN 既可以在你明确指定的查询中使用，也可以在另一个连接正在执行的查询中使用。
本节将介绍 EXPLAIN 语句的基本用法。

** 显式查询的用法

您可以在查询前面添加 EXPLAIN 来生成查询计划，也可以添加 FORMAT 选项来指定是否希望以传统表格格式、JSON 格式或树
形格式返回结果。查询支持 SELECT、DELETE、INSERT、REPLACE 和 UPDATE 语句。查询不会被执行（但例外情况请参见下
一小节的 EXPLAIN ANALYZE），因此获取查询计划是安全的。

如果需要分析复合查询（如存储过程和存储函数），首先需要将执行过程分割成单个查询，然后对每个需要分析的查询使用
EXPLAIN。确定存储程序中单个查询的一种方法是使用性能模式。本章稍后将举例说明如何实现这一点。

EXPLAIN 的最简单用法就是在指定 EXPLAIN 时，同时指定要分析的查询：

#+begin_src sql
  explain <query>;
#+end_src

在示例中，<query> 是要分析的查询。使用不带 FORMAT 选项的 EXPLAIN 语句会以传统表格格式返回结果。如果要指定格式，
可以添加 FORMAT=TRADITIONAL|JSON|TREE：

#+begin_src sql
  explain format=traditional <query>;

  explain format=json <query>;

  explain format=tree <query>;
#+end_src

首选哪种格式取决于您的需求。如果需要查询计划的概览、使用的索引以及查询计划的其他基本信息，传统格式更容易使用。JSON
格式提供了更多细节，更便于应用程序使用。例如，MySQL Workbench 中的 Visual Explain 使用 JSON 格式的输出。

树形格式是最新的格式，在 MySQL 8.0.16 及更高版本中受支持。它要求使用 Volcano 迭代器执行器执行查询，但在撰写本文
时，并非所有查询都支持该执行器。树形格式的一个特殊用途是用于 EXPLAIN ANALYZE 语句。

** EXPLAIN ANALYZE

EXPLAIN ANALYZE 语句是 MySQL 8.0.18 中新增的语句，是使用树格式对标准 EXPLAIN 语句的扩展。主要区别在于
EXPLAIN ANALYZE 会实际执行查询，并在执行过程中收集执行的统计数据。在执行语句时，查询的输出会被抑制，因此只会返
回查询计划和统计数据。与树形输出格式一样，需要使用火山迭代器执行器。


#+begin_comment
  在撰写本报告时，由于对 Volcano iterator 执行器的要求，可以使用 EXPLAIN ANALYZE 的查询仅限于 SELECT 语句
  的子集。预计随着时间的推移，支持的查询范围将会扩大。
#+end_comment

EXPLAIN ANALYZE 的用法与 EXPLAIN 语句非常相似：

#+begin_src sql
  explain analyze <query>
#+end_src

EXPLAIN ANALYZE 的输出结果将与树形格式输出结果一起在本章后面讨论。 将在本章稍后部分与树形格式输出一起讨论。

从本质上讲，EXPLAIN ANALYZE 只适用于显式查询，因为它需要自始至终监控查询。另一方面，普通 EXPLAIN 语句也可用于
正在进行的查询。

** 连接的用法

假设您正在调查一个性能不佳的问题，并注意到有一个查询已经运行了几个小时。你知道这是不应该发生的，所以你想分析一下查
询为什么这么慢。一种方法是复制查询并执行 EXPLAIN。但是，这可能无法提供所需的信息，因为索引统计信息可能在缓慢查询
开始后发生了变化，因此现在分析查询无法显示导致性能缓慢的实际查询计划。

更好的解决方法是请求用于慢查询的实际查询计划。可以使用 EXPLAIN 语句的 EXPLAIN FOR CONNECTION 变体来获取。如
果你想试试，举例来说，你需要一个长期运行的查询：

#+begin_src sql
  select * from world.city where id=130+sleep(0.1);
#+end_src

这将耗时约 420 秒（world.city 表中每行耗时 0.1 秒）。

您需要提供要调查的查询的连接 ID，并将其作为参数传递给 EXPLAIN。您可以从进程列表信息中获取连接 ID。例如，如果使用
sys.session 视图，连接 id 可以在 conn_id 列中找到：

#+begin_src sql
  select conn_id, current_statment,
    statement_latency
  from sys.session
  where command='Query'
  order by time
  desc limit 1\G
#+end_src

为了使输出简单明了，本示例中的输出仅限于感兴趣的连接。查询的连接 ID 是 8，可以用它获取查询的执行计划，如下所示：

#+begin_src sql
  explain for connection 8\G
#+end_src

与明确指定查询的方式相同，您可以选择添加所需的格式。如果使用的客户端与 MySQL Shell 不同，过滤后的列可能显示
100.00。在讨论输出的含义之前，值得先熟悉一下输出格式。

** EXAPLIN格式

当您需要检查查询计划时，可以在几种格式中进行选择。选择哪一种主要取决于您的偏好。也就是说，JSON 格式确实比传统格式
和树形格式包含更多信息。如果喜欢查询计划的可视化表示，MySQL 工作台的 Visual Explain 是一个不错的选择。

本节将讨论每种格式，并显示以下查询的查询计划输出：

#+begin_src sql
  select ci.ID, ci.Name, ci.District,
    co.Name as Country, ci.Population
  from world.city ci
    inner join
    (select Code, Name
     from world.country
     where Continent='Europe'
     order by SurfaceArea
     limit 10
     ) co on co.Code=ci.CountryCode
  order by ci.Population desc
  limit 5;
#+end_src

该查询按面积查找欧洲 10 个最小国家中最大的 5 个城市，并按城市人口降序排列。之所以选择这个查询，是因为它展示了各种
输出格式是如何表示子查询、排序和限制的。本节将不讨论 "解释 "语句返回的信息；这部分内容将在 "解释示例 "部分讨论。

#+begin_comment
  EXPLAIN 语句的输出取决于优化器开关的设置、索引统计信息以及 mysql.engine_ cost 表和 mysql.server_cost 表
  中的值，因此您看到的输出可能与示例中的不同。示例输出使用了默认值和新加载的世界示例数据库，并在加载完成后对表执行了
  ANALYZE TABLE，这些输出是在 MysQl shell 中创建的，默认情况下会自动获取警告（但只有在讨论警告时才会将其包含在
  输出中）。
#+end_comment

查询计划的输出相当冗长。为了便于比较输出结果，我们将本节中的示例与查询结果合并到本书 GitHub 代码库中的
explain_formats.txt 文件中。对于树形输出格式（包括 EXPLAIN ANALYZE），在列名和查询计划之间新增了一行，以便更
清晰地显示树形层次结构：

** 传统格式

在执行 EXPLAIN 命令时，如果不使用 FORMAT 参数或将格式设置为 TRADITIONAL，输出结果将以表的形式返回，就像查询了
一个普通表一样。当您需要查询计划的概览，并且由人工数据库管理员或开发人员检查输出结果时，这种方法非常有用。

#+begin_comment
  在调用 mysql 命令行客户端时，你可以使用 --vertical 选项来要求获得垂直格式的输出，也可以使用 \G 来终止查询。
#+end_comment

输出中有 12 列。如果某个字段没有任何值，则使用 NULL。下一节将讨论每一列的含义。清单 20-1 显示了示例查询的传统输出
结果。

#+begin_src sql
  explain format=traditional
  select ci.ID, ci.Name, ci.District,
    co.Name as Country, ci.Population
  from world.city ci
  inner join (select Code, Name
              from world.country
              where Continent='Europe'
              order by SurfaceArea
              limit 10
              ) co on co.Code=ci.CountryCode
  order by ci.Population desc
  limit 5\G
#+end_src

请注意第一个表名为 <derived 2>。这是对国家表的子查询，数字 2 指的是执行子查询的 id 列的值。Extra 列包含查询是
否使用临时表和文件排序等信息。输出的末尾是优化器重写后的查询。在许多情况下不会有太大改动，但在某些情况下，优化器可
能会对查询做出重大改动。在重写后的查询中，请注意注释（例如：/* select#1 */）是如何用于显示查询的该部分使用了哪个
id 值的。重写查询中可能还有其他提示，说明查询是如何执行的。SHOW WARNINGS（默认情况下由 MySQL Shell 隐式执行）
会将重写的查询作为注释返回。

输出结果可能看起来令人不知所措，而且很难理解如何使用这些信息来分析查询。在讨论了其他输出格式、选择类型和连接类型的
详细信息以及额外信息后，我们将举例说明 EXPLAIN 信息的用途。

如果要以编程方式分析查询计划，该怎么办？您可以像处理普通 SELECT 查询一样处理 EXPLAIN 输出，也可以请求包含一些附
加信息的 JSON 格式信息。

** JSON格式

自 MySQL 5.6 起，可以使用 JSON 格式请求 EXPLAIN 输出。与传统的表格格式相比，JSON 格式的一个优势是，它增加了灵
活性，可以以更合理的方式对信息进行分组。

JSON 输出的基本概念是查询块。查询块定义了查询的一部分，并可反过来包含自己的查询块。这允许 MySQL 将查询执行的细节
指定给所属的查询块。这一点从清单 20-2 所示的示例查询输出中也可以看出。

#+begin_src sql
  explain format=json
  select ci.ID, ci.Name, ci.District,
    co.Name as Country, ci.Population
  from world.city ci
  inner join (select Code, Name
              from world.country
              where Continent='Europe'
              order by SurfaceArea
              limit 10
              ) co on co.Code=ci.CountryCode
  order by ci.Population desc
  limit 5\G
#+end_src

正如你所看到的，输出结果相当冗长，但这种结构可以让你比较容易地看出哪些信息属于同一个表，以及查询的各个部分是如何相
互关联的。在这个示例中，有一个嵌套循环，包括两个表（co 和 ci）。co 表本身包含一个新的查询块，该查询块是使用
country 表的实体化子查询。

JSON 格式还包括其他信息，如 cost_info 元素中每个部分的估计成本。成本信息可用于查看优化器认为查询中最昂贵的部分在
哪里。例如，如果您发现查询的某个部分的成本很高，但根据您对数据的了解，该部分的成本应该很低，这可能表明索引统计信息
不是最新的，或者需要使用直方图。

使用 JSON 格式输出的最大问题是信息量太大，输出行数太多。解决这个问题的一个非常方便的方法是使用 MySQL Workbench
中的 Visual Explain 功能，该功能将在讨论树形格式输出后介绍。

** 树形格式

树形格式侧重于从查询各部分之间的关系和各部分的执行顺序来描述查询是如何执行的。从这个意义上讲，它听起来可能与 JSON
输出类似；不过，树形格式更简单易读，而且没有那么多细节。树形格式在 MySQL 8.0.16 中作为实验功能引入，依赖于火山迭
代器执行器。从 MySQL 8.0.18 开始，树格式也用于 "解释分析器 "功能。

清单 20-3 显示了示例查询的树形格式输出。此输出为非分析版本。稍后将显示 EXPLAIN ANALYZE 对同一查询的输出示例，以
便您了解两者的区别。

#+begin_src sql
  EXPLAIN FORMAT=TREE
  SELECT ci.ID, ci.Name, ci.District,
      co.Name AS Country, ci.Population
  FROM world.city ci
  INNER JOIN (SELECT Code, Name
             FROM world.country
             WHERE Continent = 'Europe'
             ORDER BY SurfaceArea
             LIMIT 10 ) co ON co.Code = ci.CountryCode
  ORDER BY ci.Population DESC LIMIT 5\G
#+end_src

输出结果很好地概述了查询是如何执行的。通过从里到外阅读输出，可以在一定程度上更容易理解执行情况。在嵌套循环中，有两
张表，其中第一张表是对 co 的表扫描（缩进部分已缩小）：

[[./images/sPgrZ6.png]]

在这里，您可以看到 co 表是如何创建的物化子查询，首先对国家表进行表扫描，然后应用大洲过滤器，再根据表面积进行排序，
最后将结果限制为 10 行。

嵌套循环的第二部分比较简单，只需使用 CountryCode 索引对 ci 表（城市表）进行索引查找即可：

[[./images/nesOfZ.png]]

使用内连接解决嵌套循环后，结果会以流式（即非实体化）的方式进行排序，并返回前五行：

[[./images/re33K9.png]]

虽然没有 JSON 输出那么详细，但它仍然包含了查询计划的大量信息。其中包括每个表的估计成本和估计行数。例如，在国家表
面积的排序步骤中

[[./images/COgCrN.png]]

一个很好的问题是，这与查询表的实际成本有什么关系。您可以使用 EXPLAIN ANALYZE 语句来解决这个问题。清单 20-4 显
示了为示例查询生成的输出示例。

#+begin_src sql
  EXPLAIN ANALYZE
  SELECT ci.ID, ci.Name, ci.District,
     co.Name AS Country, ci.Population
  FROM world.city ci
  INNER JOIN (SELECT Code, Name
              FROM world.country
              WHERE Continent = 'Europe'
              ORDER BY SurfaceArea LIMIT 10
              ) co ON co.Code = ci.CountryCode
  ORDER BY ci.Population DESC LIMIT 5\G
#+end_src

这是与 FORMAT=TREE 相同的树形输出，但每一步都有关于性能的信息。如果查看 ci 表的一行，可以看到有两个计时信息：行
数和循环数（重新格式化以提高可读性）：

[[./images/Pnhld4.png]]

在这里，预计读取 18 行（每个循环）的估计成本为 4.69。实际统计数据显示，0.012 毫秒后读取第一行，0.013 毫秒后读取
所有行。共有十个循环（十个国家各一个），每个循环平均读取两行，总共读取 20 行。因此，在这种情况下，估计值不是很准确
（因为查询只选择小国家）。

#+begin_comment
  EXPLAIN ANALYZE 的行数是四舍五入为整数的每个循环的平均值。在本示例中，使用性能模式中的
  table_io_waits_ summary_by_table 表显示读取了 15 条记录。
#+end_comment

如果您的查询在 MySQL 8.0.18 及更高版本中使用了散列连接，则需要使用树形格式输出来确认何时使用了散列连接算法。例
如，如果使用散列连接将城市表与国家表连接起来

#+begin_src sql
  explain format=tree
  select CountryCode, country.Name as Country,
    city.Name as City, city.District
  from world.country ignore index(Primary)
  inner join world.city ignore index(CountryCode)
    on city.CountryCode=country.Code\G
#+end_src

请注意，连接是一个内部哈希连接，对country表的表扫描是使用哈希进行的。

到目前为止，所有示例都使用了基于文本的输出。特别是 JSON 格式的输出，可能很难用来概览查询计划。为此，Visual
Explain 是一个更好的选择。

** 可视化EXPLAIN

可视化解释功能是 MySQL 工作台的一部分，通过将 JSON 格式的查询计划转换成图形表示来工作。早在第 16 章研究向
sakila.film 表添加直方图的效果时，您就已经使用过 Visual Explain。

如图 20-1 所示，单击闪电符号前带放大镜的图标，即可获得 Visual Explain 图表。

[[./images/Wd1mIM.png]]

如果查询执行时间较长或查询修改了数据，这是生成查询计划的一种特别有用的方法。如果已经执行了查询，也可以单击结果网格
右侧的执行计划图标，如图 20-2 所示。

[[./images/l2cXRs.png]]

Visual Explain 图表是以流程图的形式创建的，每个查询块和表都有一个矩形。数据处理则使用其他形状（如表示连接的菱形）
来描述。图 20-3 展示了 Visual Explain 中使用的每种基本形状的示例。

[[./images/4odm7k.png]]

图中，查询块为灰色，而表的两个示例（子查询中的单行查找和全表扫描）分别为蓝色和红色。灰色块也用于联合等情况。表格框
下方的文本以标准文本显示表格名称或别名，以粗体文本显示索引名称。圆角矩形表示对行进行的操作，如排序、分组、区分操作
等。

左上方的数字是该表、操作或查询块的相对成本。表和连接右上方的数字是估计要结转的行数。操作的颜色用于显示应用该操作的
成本有多高。表格也使用基于表格访问类型的颜色，主要用于归类类似的访问类型，其次用于显示访问类型的成本。使用 Visual
Explain 估算的成本，颜色与成本之间的关系如图 20-4 所示。

[[./images/XbAzgB.png]]

蓝色 (1) 代表最便宜；绿色 (2)、黄色 (3) 和橙色 (4) 代表中低成本；最昂贵的接入类型和操作为红色，象征高成本 (5)
到极高成本 (6)。

颜色组之间存在大量重叠。每个成本估算都考虑了 "平均 "用例，因此不应将成本估算视为绝对真理。查询优化是一项复杂的工作，
有时对于某个特定查询来说，一种方法通常比另一种方法更便宜，但结果却能提供更好的性能。

#+begin_comment
  本书作者曾经决定改进一个查询，该查询的查询计划看起来很糟糕：内部临时表、文件排序、访问方法不佳等等。经过长时间重
  写查询并验证表是否有正确的索引后，查询计划看起来很漂亮--但结果却发现查询的性能比原来的更差。
#+end_comment

对于表而言，成本与访问类型相关联，访问类型是传统 EXPLAIN 输出中 type 列的值，也是 JSON 格式输出中 access_type
字段的值。图 20-5 显示了 Visual Explain 如何表示当前存在的 12 种访问类型。对访问类型的解释将推迟到下一节。

[[./images/6LojWd.png]]

此外，Visual Explain 还有一个黑色的 "未知 "访问类型，以防遇到未知的访问类型。访问类型从左到右排序，然后根据颜色
和大致成本从上到下排序。

图 20-6 将所有这些整合在一起，显示了本节中使用的示例查询的查询计划。

[[./images/8dY6VQ.png]]

读图时，要从左下角读到右上角，然后再向上读。因此，从图中可以看出，首先对国家表进行全表扫描的子查询，然后对物化的 co
表进行另一次全表扫描，并使用非唯一索引查找将 ci（城市）表上的行连接起来。最后，使用临时表和文件排序对结果进行排序。

如果想了解比图表显示更多的详细信息，可以将鼠标悬停在查询计划中想了解的部分。图 20-7 显示了 ci 表所含详细信息的
示例。

[[./images/vPP7yh.png]]

弹出框架不仅会显示 JSON 输出中的其余详细信息，还会提供提示，帮助理解数据的含义。所有这些都意味着，Visual Explain
是通过查询计划开始分析查询的好方法。随着经验的积累，你可能会更喜欢使用基于文本的输出，尤其是如果你喜欢使用 shell
工作的话，但不要因为你认为使用基于文本的输出格式更好，就放弃 Visual Explain。即使对专家来说，Visual Explain
也是了解查询如何执行的绝佳工具。

希望通过对输出格式的讨论，您能了解 EXPLAIN 可以提供哪些信息。然而，要完全理解并利用它，有必要深入了解信息的含义。

** EXPLAIN 输出

解释输出中有很多信息，因此值得深入了解这些信息的含义。本节首先概述了传统输出和 JSON 格式输出中包含的字段；然后将
更详细地介绍选择类型、访问类型和额外信息。

** EXPLAIN字段

要在工作中建设性地使用 EXPLAIN 语句来改进查询，第一步就是要了解有哪些信息可用。这些信息包括对查询部分进行引用的
id，以及可用于查询的索引的详细信息、使用的索引和应用的优化器功能。

如果您在第一次阅读定义后无法回忆起所有细节，请不要担心。大多数字段都是不言自明的，因此您可以有条件地猜测它们所代表
的数据。当你自己分析一些查询时，也会很快熟悉这些信息。表 20-1 列出了传统格式中包含的所有字段以及 JSON 格式中的一
些常用字段。

| 传统格式        | json格式                         | 描述                                                                                                                                                           |
|---------------+---------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------|
| id            | select_id                       | 顶级表的 id = 1，第一个子查询的 id = 2，以此类推。如果是联合查询，id 将为 NULL，表值设置为 <unionM,N>（另请参阅表列），该行表示联合查询结果的汇总。                              |
| select_type   |                                 | 这表明表格将如何包含在整个语句中。已知的选择类型将在后面的 "选择类型 "部分讨论。对于 JsOn 格式，选择类型是由 JsOn 文档的结构以及依赖和可缓存等字段暗示的。                           |
|               | dependent                       | 是否是从属子查询，即取决于查询的外部部分。                                                                                                                           |
|               | cacheable                       | 子查询的结果是否可以缓存，还是必须对外部查询中的每一条记录重新进行评估。                                                                                                  |
| table         | table_name                      | 特殊情况包括联合表、派生表和具体化子查询，其中表名分别为 <unionM,N>、<derivedN> 和 <subqueryN>，其中 n 和 M 指的是查询计划早期部分的 id。                                     |
| partitions    | partitions                      | 您可以用它来确定分区剪枝是否按预期应用。                                                                                                                             |
| type          | access_type                     | 这表明优化程序如何决定限制在表中检查的行数，这些类型将在 "访问类型 "部分讨论。                                                                                             |
| possible_keys | possible_keys                   | 表的候选列表，使用模式 <auto_key0> 的键名表示自动生成的索引可用。                                                                                                      |
| key           | key                             | 使用模式 <auto_key0> 的键名表示使用自动生成的索引。                                                                                                                  |
| key_len       | key_length                      | 使用索引的字节数。对于由多列组成的索引，优化器可能只能使用列的子集。在这种情况下，可以使用键长度来确定有多少索引对该查询有用。如果索引中的列支持 NULL 值，则长度会比非 NULL 列增加 1 个字节。 |
|               | used_key_parts                  | 索引中使用的列。                                                                                                                                                 |
| ref           | ref                             | 例如，可以是条件常量，如 <table>.<column> = 'abc'，也可以是连接时另一个表中的列名。                                                                                     |
| rows          | rows_examined_per_scan          | 行数的估计值，是包含表的结果。对于与先前表连接的表，它是每次连接后估计能找到的行数。一种特殊情况是，当引用是表的主键或唯一键时，在这种情况下，行数估计值正好是 1。                       |
|               | rows_produced_per_join_filtered | 连接后的估计行数。将预期的循环数乘以每次扫描所检查的行数（rows_examined_per_scan）和已过滤行的百分比，即可得出有效结果。                                                       |
| filtered      | filtered                        | 该值以百分比为单位，因此值为 100.00 时将返回所有检查过的记录。100.00 是最佳值，最差值为 0。例如，MysQl shell 将返回 100，而 mysql 命令行客户端将返回 100.00。                    |
|               | cost_info                       | 一个 JsOn 对象，包含查询部分的费用明细。                                                                                                                            |
| Extra         |                                 | 有关优化器决策的附加信息。这些信息包括所使用的排序算法、是否使用覆盖索引等。最常见的支持值将在 "额外信息 "一节中讨论。                                                           |
|               | message                         | 在 JsOn 输出中没有专用字段的传统输出额外列中的信息，例如 Impossible WHERE。                                                                                            |
|               | using_filesort                  | 是否使用文件排序。                                                                                                                                               |
|               | using_index                     | 是否使用覆盖索引。                                                                                                                                               |
|               | using_temporary_table           | 子查询或排序等操作是否需要内部临时表。                                                                                                                               |
|               | attached_condition              | 与查询部分相关的 WHERE 子句                                                                                                                                       |
|               | used_columns                    | 表中所需的列。这对查看是否接近使用覆盖索引非常有用。                                                                                                                   |

有些信息初看起来似乎在 JSON 格式中缺失，因为该字段只存在于传统格式中。事实并非如此；相反，这些信息可以通过其他方式
获得，例如，Extra 中的几条信息在 JSON 格式中都有自己的字段。其他 Extra 消息则使用消息字段。本节后面讨论 Extra
列中的信息时，将提到 JSON 输出表中未包含的一些字段。

一般来说，如果值为 false，JSON 格式输出中的布尔字段将被省略；但可缓存的情况例外，因为与可缓存的情况相比，不可缓存
的子查询或联合表示成本较高。

对于 JSON 输出，还有一些字段用于对操作信息进行分组。操作的范围从访问表格到组合多个操作的复杂操作。一些常见操作及其
触发示例如下

+ table: 访问表格。这是最低级别的操作。
+ query_block: 最高级别的概念，有一个查询块与传统格式的 id 相对应。所有查询都至少有一个查询块。
+ nested_loop: 连接操作
+ grouping_operation: 例如，由 GROUP BY 子句产生的操作。
+ ordering_operation: 例如，该操作会导致 ORDER BY 子句。
+ duplicates_removal: 例如，使用 DISTINCT 关键字时产生的操作。
+ windowing: 从窗口函数获取操作结果
+ materialized_from_subquery: 执行子查询并将结果具体化。
+ attached_subqueries: 附加到查询其他部分的子查询。例如，IN（SELECT ...）子查询中的 IN 子查询。
+ union_result: 用于使用 UNION 合并两个或多个查询结果的查询。在 union_result 块中，有一个
  query_specifications 块，其中包含 union 中每个查询的定义。


表 20-1 中的字段和复杂操作列表对于 JSON 格式来说并不全面，但可以让你对可用信息有一个很好的了解。一般来说，字段名
称本身就包含信息，结合上下文通常就能理解字段的含义。不过，有些字段的值值得更多关注--首先是选择类型。

** 选择类型

选择类型显示查询的每个部分属于哪种查询块。在这种情况下，查询的一部分可以包括多个表。例如，如果您有一个连接表列表的
简单查询，但没有使用子查询等结构，那么所有表都将在查询的同一（且唯一）部分中。查询的每个部分都有自己的 id（JSON
输出中的 select_id）。

有多种选择类型。对于其中的大多数类型，JSON 输出中没有直接的字段；不过，可以从结构和其他一些字段推导出选择类型。表
20-2 列出了当前存在的选择类型，并提示了如何从 JSON 输出中导出类型。在表中，"选择类型 "列的值是传统输出格式中用于
select_type 列的值。

#+NAME: EXPLAIN select类型
| select类型            | JSON                       | 描述                                                        |
|----------------------+----------------------------+------------------------------------------------------------|
| SIMPLE               |                            | 适用于不使用派生表、子查询、联合或类似功能的 SELECT 查询。          |
| PRIMARY              |                            | 对于使用子查询或联合查询的查询，主查询部分是最外层的部分。           |
| INSERT               |                            | 针对INSERT语句                                               |
| DELETE               |                            | 针对DELETE语句                                               |
| UPDATE               |                            | 针对 UPDATE语句                                              |
| REPLACE              |                            | 针对REPLACE语句                                              |
| UNION                |                            | 对于联合语句，第二个或更后的 SELECT 语句。                       |
| DEPENDENT UNION      | dependent=true             | 对于联合语句，第二个或后面的 SELECT 语句依赖于外部查询。           |
| UNION RESULT         | union_result               | 查询中汇总联合 SELECT 语句结果的部分。                          |
| SUBQUERY             |                            | 对于子查询中的 SELECT 语句。                                   |
| DEPENDENT SUBQUERY   | dependent=true             | 对于从属子查询，第一条 SELECT 语句。                            |
| DERIVED              |                            | 派生表 - 通过查询创建的表，但在其他方面与普通表类似。               |
| DEPENDENT DERIVED    | dependent=true             | 依赖于另一个表的派生表。                                       |
| MATERIALIZED         | materialized_from_subquery | 物化子查询                                                   |
| UNCACHEABLE SUBQUERY | cacheable=false            | 子查询，其结果必须针对外部查询中的每一行进行评估。                  |
| UNCACHEABLE UNION    | cacheable=false            | 对于联合语句，第二条或更后一条 SELECT 语句是不可缓存子查询的一部分。 |

有些选择类型可以只作为信息使用，以便于理解查询的哪个部分。例如，这包括 PRIMARY 和 UNION。但是，有些选择类型表明
它是查询的昂贵部分。这尤其适用于不可缓存类型。依赖类型还意味着优化程序在决定在执行计划中的哪个位置添加表时灵活性较
低。如果您的查询速度较慢，并且看到了不可缓存或依赖的部分，那么值得考虑是否可以重写这些部分或将查询一分为二。

** 访问类型

在讨论 Visual Explain 时，我们已经遇到过表访问类型。它们显示查询是否使用索引、扫描和类似方式访问表。由于与每种
访问类型相关的成本差异很大，因此这也是在 EXPLAIN 输出中需要查找的重要值之一，以便确定查询的哪些部分需要改进以提
高性能。

本小节的其余部分总结了 MySQL 中的访问类型。标题是传统格式的类型列中使用的值。每种访问类型都有一个使用该访问类型的
示例。

*** system

system访问类型适用于只有一行的表。这意味着可以将表视为常数。可视化解释成本、信息和颜色如下：

+ 代价: 很低
+ 消息: 单行
+ 颜色: 蓝色

使用system访问类型进行查询的示例如下  

#+begin_src sql
  select * from (select 1) my_table;
#+end_src

system访问类型是常量访问类型的特例。

*** const

例如，当对主键的单个值或唯一索引进行过滤时，最多只能匹配表中的一条记录。可视化解释成本、信息和颜色如下：

+ 代价: 很低
+ 消息: 单行
+ 颜色: 蓝色
  
使用const访问类型进行查询的示例如下

#+begin_src sql
  select * from world.city where id=130;
#+end_src

*** eq_ref

该表是连接表中的右侧表，连接表的条件是主键或非空唯一索引。可视化解释成本、信息和颜色如下：

+ 代价: 很低
+ 消息: Unique Key Lookup
+ 颜色: 绿色

使用 eq_ref 访问类型进行查询的示例如下  

#+begin_src sql
  select *
  from world.city
    straight_join world.country on CountryCode=Code;
#+end_src

eq_ref 访问类型是 ref 访问类型的一种特殊情况，每次查找只能返回一条记录。

*** ref

该表通过非唯一二级索引进行过滤。视觉解释成本、信息和颜色如下：

+ 代价: 低至中等
+ 消息: Non-Unique Key Lookup
+ 颜色: 绿色

使用 ref 访问类型进行查询的示例如下

#+begin_src sql
  select * from world.city where CountryCode='AUS';
#+end_src

*** ref_or_null

与 ref 相同，但过滤的列也可以是 NULL。可视化解释成本、信息和颜色如下：

+ 代价: 低至中等
+ 消息: Key Lookup + Fetch NULL Values
+ 颜色: 绿色

使用 ref_or_null 访问类型进行查询的示例如下  

#+begin_src sql
  select *
  from sakila.payment
  where rental_id=1 or rental_id is null;
#+end_src

*** index_merge

优化器会选择两个或更多索引的组合来解决包含不同索引中列之间 OR 或 AND 的过滤器。可视化解释成本、信息和颜色如下：

+ 代价: 中等
+ 消息: Index Merge
+ 颜色: 绿色

虽然成本被列为中等，但更常见的严重性能问题之一是查询通常使用单个索引或进行全表扫描，而索引统计变得不准确，因此优
化器选择了索引合并。如果使用索引合并的查询性能不佳，可以尝试让优化器忽略索引合并优化或使用的索引，看看是否有帮助，
或者分析表以更新索引统计信息。或者，也可以将查询重写为两个查询的联合，每个查询使用过滤器的一部分。第 24 章将举例
说明。  

*** fulltext

优化器选择全文索引来过滤表格。视觉解释成本、信息和颜色如下：

+ 代价: 低
+ 消息: Fulltext Index Search
+ 颜色: 黄色

使用全文访问类型进行查询的示例是

#+begin_src sql
  select *
  from sakila.film_text
  where match(title, description)
       against('Circus' in boolean mode);
#+end_src

*** unique_subquery

用于 IN 操作符内的子查询，其中子查询返回主键或唯一索引的值。在 MySQL 8 中，这些查询通常由优化器重写，因此
unique_subquery 需要禁用实体化和半连接优化器开关。Visual Explain 成本、信息和颜色如下：

+ 代价: 低
+ 消息: Unique Key Lookup into table of subquery
+ 颜色: 橙色

使用 unique_subquery 访问类型的查询示例如下

#+begin_src sql
  set optimizer_switch='materialization=off,semijoin=off';

  select *
  from world.city
  where CountryCode in (
    select Code
    from world.country
    where Continent='Oceania'
  );

  set optimizer_switch='materialization=on,semijoin=on';
#+end_src

unique_subquery 访问方法是 index_subquery 方法的一种特例，适用于使用主索引或唯一索引的情况。

*** index_subquery

用于 IN 操作符内的子查询，其中子查询返回二级非唯一索引的值。在 MySQL 8 中，这些查询通常由优化器重写，因此
unique_subquery 需要禁用实体化和半连接优化器开关。Visual Explain 成本、信息和颜色如下：

+ 代价: 低
+ 消息: Nonunique Key Lookup into table of subquery
+ 颜色: 橙色

使用索引子查询访问类型进行查询的示例如下

  #+begin_src sql
    set optimizer_switch='materialization=off,semijoin=off';
    select *
    from world.country
    where Code in (
       select CountryCode
       from world.city
       where Name='Sydney'
    );
    set optimizer_switch='materialization=on,semijoin=on';
  #+end_src

*** range

当索引用于按顺序或分组查找多个值时，就会使用范围访问类型。它既可用于显式范围（如 ID BETWEEN 1 AND 10），也可
用于 IN 子句，或用于同一列上的多个条件用 OR 分隔的情况。Visual Explain 的成本、信息和颜色如下：

+ 代价: 中等
+ 消息: Index Range Scan
+ 颜色: 橙色

使用范围访问类型进行查询的示例如下

#+begin_src sql
  select *
  from world.city
  where id in (130, 3805);
#+end_src

使用范围访问的成本主要取决于范围内包含多少行。一种极端情况是，范围扫描只使用主键匹配一条记录，因此成本很低。另一
种极端情况是，范围扫描包括使用二级索引的表的很大一部分，在这种情况下，执行全表扫描的成本可能会更低。

范围访问类型与索引访问类型相关，区别在于需要部分扫描还是完全扫描。

*** index

优化器选择执行全索引扫描。这可以与使用覆盖索引相结合。视觉解释成本、信息和颜色如下：

+ 代价: 高
+ 消息: Full Index Scan
+ 颜色: 红

使用索引访问类型进行查询的示例如下

#+begin_src sql
  select ID, CountryCode
  from world.city;
#+end_src

由于索引扫描需要使用主键进行第二次查找，因此成本会变得非常高，除非该索引是查询的覆盖索引，以至于最终执行全表扫描
会更省钱。

*** ALL

最基本的访问类型是扫描表的所有行。这也是最昂贵的访问类型，因此该类型以大写字母书写。Visual Explain 的成本、信
息和颜色如下：

+ 代价: 很高
+ 消息: Full Table Scan
+ 颜色: 红色

使用 ALL 访问类型进行查询的示例如下

#+begin_src sql
  select * from world.city;
#+end_src

如果看到第一个表以外的表使用全表扫描，这通常是一个红旗信号，表明表上缺少一个条件，或者没有可以使用的索引。对于第
一个表来说，"全部 "是否是合理的访问类型取决于查询需要多少表；需要的表的部分越大，全表扫描就越合理。

#+begin_comment
  虽然全表扫描被认为是最昂贵的访问类型，但它与主键查找一起是每行最便宜的访问类型。因此，如果确实需要访问表的大部
  分或全部，全表扫描是读取行的最有效方法。
#+end_comment

关于访问类型的讨论到此为止。在本章后面的 EXPLAIN 示例以及本书后面的优化查询（如第 24 章）中，访问类型将再次被
引用。与此同时，让我们来看看 "Extra "栏中的信息。

** Extra信息

传统输出格式中的 "额外"（Extra）列是一个万能箱，用于存放没有自己列的信息。在引入 JSON 格式时，没有理由保留它，因
为引入额外字段很容易，而且没有必要在每次输出中都包含所有字段。因此，JSON 格式没有额外字段，而是有一系列字段。通用信
息字段保留了一些剩余信息。

#+begin_comment
  在某些情况下，"Extra"栏中的信息与存储引擎有关，或者只在极少数情况下使用。本讨论只涉及最常遇到的信息。有关信息的
  完整列表，请参阅 MysQl 参考手册
  https://dev.mysql.com/doc/refman/en/explain-output.html#explain-extra-information。
#+end_comment

比较常见的信息包括

+ Using index: 使用覆盖索引时。对于 JSON 格式，using_index 字段设置为 true。
+ Using index condition: 当使用索引测试是否有必要读取整行时。例如，当索引列上有范围条件时，就会使用这种方法。
  对于 JSON 格式，index_condition 字段与筛选条件一起设置。
+ Using where: 在不使用索引的情况下对表应用 WHERE 子句。这可能表明表上的索引不是最佳索引。在 JSON 格式中，
  attached_condition 字段与过滤条件一起设置。
+ Using index for group-by: 当使用松散索引扫描来解决 GROUP BY 或 DISTINCT 时。在 JSON 格式中，
  using_index_for_group_by 字段设置为 true。
+ Using join buffer (Block Nested Loop): 这意味着连接时无法使用索引，因此使用了连接缓冲区。带有此信息的表是
  添加索引的候选表。对于 JSON 格式，using_join_buffer 字段设置为 Block Nested Loop。需要注意的一点是，当使
  用哈希连接时，传统格式和 JSON 格式的输出仍会显示使用了块嵌套循环。要查看是实际的块嵌套循环连接还是哈希连接，需要
  使用树形格式输出。
+ Using join buffer (Batched Key Access): 这意味着连接正在使用分批键访问（BKA）优化。要启用分批密钥访问优
  化，必须启用 mrr（默认为开）和 batch_key_access（默认为关），并禁用基于成本的 mrr_cost_based（默认为开）
  优化器开关。该优化需要一个用于连接的索引，因此与在块嵌套循环中使用连接缓冲区不同，使用批量密钥访问算法并不表示对
  表的访问代价高昂。对于 JSON 格式，using_ join_buffer 字段设置为分批键访问。
+ Using MRR: 使用多范围读取 (MRR) 优化。这有时用于减少二级索引上范围条件下的随机 I/O 量，因为在这些条件下需要
  完整的行。该优化由 mrr 和 mrr_cost_based 优化器开关控制（默认情况下均已启用）。对于 JSON 格式，using_MRR
  字段设置为 true。
+ Using filesort: MySQL 会使用额外的传递来决定如何以正确的顺序检索记录。例如，通过二级索引排序时就会出现这种情
  况；而且该索引不是覆盖索引。对于 JSON 格式，using_filesort 字段设置为 true。
+ Using temporary: 内部临时表用于存储子查询的结果、排序或分组。对于排序和分组，有时可以通过添加索引或重写查询来
  避免使用内部临时表。对于 JSON 格式，using_temporary_table 字段设置为 true。
+ sort_union(...), Using union(...), Using intersect(...): 这三条信息与索引合并一起使用，用于说明如何执
  行索引合并。无论是哪种信息，括号内都包含索引合并所涉及的索引信息。对于 JSON 格式，关键字段指定了使用的方法和索引。
+ Recursive: 表是递归通用表表达式（CTE）的一部分。对于 JSON 格式，递归字段设置为 true。
+ Range checked for each record(index map: 0x1): 这种情况发生在连接时，第二个表的索引列上有一个取决于第一
  个表的列值的条件，例如，在 t2.val2 上有一个索引： SELECT * FROM t1 INNER JOIN t2 WHERE t2.val2 <
   t1.val1; 这就是触发性能模式语句事件表中 NO_GOOD_INDEX_USED 计数递增的原因。索引映射是一个位掩码，表示哪些
   索引是范围检查的候选索引。索引编号以 1 为基础，如 SHOW INDEXES 所示。写出位掩码时，设置了该位的索引号就是候
   选索引。对于 JSON 格式，range_checked_for_ each_record 字段被设置为索引映射。
+ Impossible WHERE: 如果过滤器中的值超出了数据类型支持的范围，例如，对于微小整数数据类型，WHERE ID = 300。对
  于 JSON 格式，信息会添加到信息字段中。
+ Impossible WHERE noticed after reading const tables: 与 Impossible WHERE 相同，但它适用于使用系统或
   const 访问方法解析表之后。例如，SELECT * FROM (SELECT 1 AS ID) a INNER JOIN city USING (ID) WHERE
   a.id = 130; 对于 JSON 格式，信息会添加到信息字段中。
+ Impossible HAVING: 除适用于 HAVING 子句外，与 Impossible WHERE 相同。对于 JSON 格式，信息会添加到信息字
  段中。
+ Using index for skip scan: 当优化器选择使用类似于松散索引扫描的多重范围扫描时。例如，它可用于覆盖索引，其中
  索引的第一列不用于过滤条件。此方法在 MySQL 8.0.13 及更高版本中可用。对于 JSON 格式，
  using_index_for_skip_scan 字段设置为 true。
+ Select tables optimized away: 该消息表示 MySQL 能够从查询中删除表，因为查询结果只有一条记录，而且该记录可
  以从一组确定的记录中生成。这种情况通常发生在只需要表中索引的最小值和/或最大值时。对于 JSON 格式，信息会添加到信
  息字段中。
+ Not tables used: 对于不涉及任何表的子查询，例如 SELECT 1 FROM dual；对于 JSON 格式，信息会添加到信息字段
  中。
+ no matching row in const table: 对于可以使用系统或 const 访问类型但没有符合条件的行的表。对于 JSON 格式，
  信息会添加到信息字段中。


#+begin_comment
  在撰写本文时，您需要使用树形格式输出来查看不使用索引的连接是否使用了哈希连接算法。
#+end_comment

关于 EXPLAIN 语句输出含义的讨论到此结束。剩下的就是开始使用它来检查查询计划了。

** EXPLAIN示例

在结束查询计划的讨论之前，我们不妨看几个示例，以便更好地了解如何将所有计划组合在一起。这里的示例只是一个引子。更多
示例将出现在本书的其余部分，尤其是第 24 章。

*** 单表, 全表扫描

作为第一个示例，考虑对world示例数据库中的city表进行查询，条件是查询非索引列 Name。由于没有可以使用的索引，
因此需要全表扫描来评估查询。 符合这些要求的查询示例如下

#+begin_src sql
  select *
  from world.city
  where Name='London';
#+end_src

输出的访问类型设置为 ALL，这也是预料之中的，因为在有索引的列上没有条件。预计将检查 4188 条记录（实际为 4079
条），每条记录都将应用 WHERE 子句中的一个条件。预计在检查的记录中，10% 的记录将与 WHERE 子句相匹配（注意，根
据所用客户端的不同，过滤列的输出可能是 10 或 10.00）。回顾第 17 章中关于优化器的讨论，优化器使用默认值来估计各
种条件的过滤效果，因此不能直接使用过滤值来估计索引是否有用。

相应的 Visual Explain 图表见图 20-8。

[[./images/bv6LKm.png]]

全表扫描用红色的 "全表扫描 "框显示，可以看到成本估计为 425.05。

该查询只返回两行（表中有一个伦敦在英国，一个在加拿大安大略省）。如果查询的是一个国家的所有城市，会出现什么情况？

*** 单表，索引访问

第二个示例与第一个示例类似，只是将过滤条件改为使用具有二级非唯一索引的 CountryCode 列。这样访问匹配行的成本会
更低。在本例中，将检索所有德国城市：

#+begin_src sql
  select *
  from world.city
  where CountryCode='DEU';
#+end_src

这次的 possible_keys 列显示 CountryCode 索引可用于查询，key 列显示索引已被使用。访问类型是 ref，以反映表
访问中使用了非唯一索引。估计将访问 93 条记录，这是精确的，因为优化器会询问 InnoDB 有多少行会匹配。过滤列显示，
索引完美地完成了对表的过滤。相应的 Visual Explain 图表如图 20-9 所示。

[[./images/kztUKC.png]]

尽管返回的行数是第一个示例的 45 倍，但估计成本仅为 28.05，不到全表扫描成本的十分之一。

如果只使用 ID 和 CountryCode 列，会发生什么情况？

*** 两表和覆盖索引

如果有一个索引包含表中所需的所有列，那么它就被称为覆盖索引。MySQL 将使用它来避免检索整条记录。由于city表中
的 CountryCode 索引是一个非唯一索引，因此也包括 ID 列，因为它是主键。为了使查询更符合实际情况，查询还将包括
country表，并根据洲来筛选所包含的国家。这种查询的一个示例是

#+begin_src sql
  explain select ci.ID
  from world.country co
  inner join world.city ci on ci.CountryCode=co.Code
  where co.Continent='Asia';
#+end_src

查询计划显示，优化程序选择从 co（国家）表的全表扫描开始，并使用 CountryCode 索引连接 ci（城市）表。这里的特别
之处在于 Extra 列包含了 Using index。因此无需读取城市表的整行。还要注意的是，键长度为 3（字节），也就是
CountryCode 列的宽度。相应的 Visual Explain 图表见图 20-10。

[[./images/0whZRU.png]]

key_len 字段不包括索引的主键部分，即使它已被使用。不过，它有助于了解多列索引的使用情况。

*** 多列索引

countrylanguage 表的主键包括 CountryCode 和 Language 列。试想一下，如果您想查找单个国家使用的所有语言，那么
就需要根据 CountryCode 而不是 Language 进行筛选。索引仍可用于执行过滤，您可以使用 EXPLAIN 输出的 key_len 字
段查看索引的使用情况。可以用来查找中国所有语言的查询是

#+begin_src sql
  explain select *
  from world.countrylanguage
  where CountryCode='CHN'\G
#+end_src

主键的总宽度为 CountryLanguage 列的 3 字节和 Language 列的 30 字节。由于 key_len 列显示只使用了 3 个字节，
因此可以断定只有索引的 CountryLanguage 部分用于过滤（索引的使用部分总是最左边的部分）。在 Visual Explain 中，
您需要将鼠标悬停在相关表格上，以获取扩展信息，如图 20-11 所示。

[[./images/1UIveL.png]]

在图中，查找 Key/Index 下的 Used Key Parts 标签： PRIMARY。这直接表明只使用了索引的 CountryCode 列。

作为最后一个例子，让我们回到在使用 EXPLAIN 格式时作为示例的查询。

*** 两表子查询和排序

本章前面大量使用的示例查询将用于结束有关 EXPLAIN 的讨论。该查询混合使用了各种功能，因此会触发前面讨论过的几部分
信息。它也是一个包含多个查询块的查询示例。为了提醒大家，在此重复该查询。

#+begin_src sql
  explain select ci.ID, ci.Name, ci.District,
                 co.Name as Country, ci.Population
  from world.city ci
  inner join (
    select Code, Name
    from world.country
    where Continent='Europe'
    order by SurfaceArea
    limit 10
  ) co on co.Code=ci.CountryCode
  order by ci.Population desc
  limit 5\G
#+end_src

图 20-12 重复了该查询的 Visual Explain 图表。在继续阅读输出分析之前，建议您先自行研究一下。

[[./images/btT9m1.png]]


查询计划以子查询开始，该子查询使用国家表按面积查找最小的十个国家。该子查询的表标签是 <derived2>，因此需要查找
id = 2 的行（其他查询可能有多条行），也就是本例中的第 3 行。第 3 行的选择类型设置为 DERIVED，因此它是一个派
生表；这是一个通过查询创建的表，但在其他方面与普通表类似。派生表是通过对每一行应用 WHERE 子句的全表扫描（类型 =
ALL）生成的，然后进行文件排序。生成的派生表被具体化（从 Visual Explain 中可见），并被称为 co。

一旦构建了派生表，它就会被用作与 ci（城市）表连接的第一个表。从行的排序可以看出，<derived2> 在第 1 行，ci 在第
2 行。对于派生表中的每一行，估计将使用 CountryCode 索引在 ci 表中检查 18 行。从 Visual Explain 中的表框标
签可以看出，CountryCode 索引是一个非唯一索引，类型列的值为 ref。据估计，连接将返回 180 条记录，即派生表中的
10 条记录乘以 ci 表中每次索引查找 18 条记录的估计值。

最后，使用内部临时表和文件排序对结果进行排序。该查询的总成本估计为 247.32。

到目前为止，我们一直在讨论查询计划的最终结果。如果想知道优化器是如何达到这一结果的，就需要检查优化器跟踪。

** 优化器跟踪

优化器跟踪并不是经常需要的，但有时遇到意外的查询计划时，看看优化器是如何实现的会很有用。这就是优化器跟踪所显示的内
容。

#+begin_comment
  当查询计划与预期不符时，通常是因为缺少或错误的 WHERE 子句、缺少或错误的连接条件、查询中的其他错误或索引统计不正
  确。在深入了解优化器决策过程的细节之前，请先检查这些问题。
#+end_comment

通过将 optimizer_trace 选项设置为 1，优化器就会启用跟踪功能。 这样，优化器就会为后续查询记录跟踪信息（直到
optimizer_trace 再次被禁用），这些信息可通过 information_schema.OPTIMIZER_TRACE 表获取。保留跟踪信息的最
大数量由 optimizer_trace_limit 选项配置（默认为 1）。

您可以选择执行需要优化器跟踪的查询，或者使用 EXPLAIN 获取查询计划。后者非常有用，因为它可以同时提供查询计划和优化
器跟踪。获取查询的优化器跟踪的典型工作流程如下：

1. 在当前会话启用optimizer_trace选项
2. 在想要查看的SQL新增EXPLAIN
3. 禁用optimizer_trace选项
4. 从information_schema.optimizer_trace表中查看优化器跟踪情况

information_schema.optimizer_trace表中包含的列:

+ QUERY: 原始查询。
+ TRACE: 包含跟踪信息的 JSON 文档。稍后会有更多关于跟踪的信息。
+ MISSING_BYTES_BEYOND_MAX_MEM_SIZE: 记录跟踪的大小（以字节为单位）受限于 optimizer_trace_max_mem_size
   选项的值（在 MySQL 8 中默认为 1 MiB）。这一列显示记录完整跟踪所需的内存容量。如果该值大于 0，则增加
   optimizer_trace_max_mem_size 选项的值。
+ INSUFFICIENT_PRIVILEGES: 是否缺少生成优化器跟踪的权限。

该表是作为临时表创建的，因此跟踪对于会话来说是唯一的。

清单 20-10 显示了获取查询（与前几节中的重复示例查询相同）的优化器跟踪的示例。由于优化器跟踪输出超过 15000 个字符，
几乎有 500 行之长，因此此处对其进行了截断。同样，EXPLAIN 语句的输出也被省略了，因为它与之前显示的相同，而且对本讨
论并不重要。完整输出包含在本书 GitHub 代码库中的 listing_20_10.txt 文件中，跟踪本身包含在 listing_20_10.

#+begin_src sql
  set session optimizer_trace=1;
  explain select ci.ID, ci.Name, ci.District,
    co.Name as Country, ci.Population
  from world.city ci
  inner join (select Code, Name from world.country
              where Continent='Europe'
              order by SurfaceArea
              limit 10
              ) co on co.Code=ci.CountryCode
  order by ci.Population desc
  limit 5\G
  set session optimizer_trace=0;
  select * from information_schema.optimizer_trace\G
#+end_src

跟踪是结果中最有趣的部分。虽然有很多信息，但幸运的是，这些信息在很大程度上是不言自明的，如果你熟悉 JSON 格式的
EXPLAIN 输出，就会发现其中有一些相似之处。大部分信息都与执行查询的各个部分的成本估算有关。在有多个可能选项的情况
下，优化器会计算每个选项的成本，并选择最便宜的选项。本跟踪中的一个例子是访问 ci（城市）表。这可以通过 CountryCode
索引或表扫描完成。该决策的跟踪部分如清单 20-11 所示（缩进部分有所减少）。


[[./images/iCldVe.png]]

这表明，在使用 CountryCode 索引（"access_type"："ref"）时，估计平均要检查 18 行多一点，费用为 63.181。对于
全表扫描（"access_type"："scan"），预计需要检查 4188 行，总费用为 4194.3。选择 "元素表明已选择 ref 访问类型。

虽然很少有必要深入研究优化器如何得出查询计划的细节，但了解优化器如何工作可能会很有用。有时，查看查询计划的其他选项
的估计成本也很有用，可以了解为什么不选择这些选项。

如果您有兴趣了解更多有关优化器跟踪的信息，请访问
https://dev.mysql.com/doc/internals/en/optimizer-tracing.html 查阅 MysQl 内部手册。

到目前为止，除了 EXPLAIN ANALYZE 之外，整个讨论都是关于在执行查询之前的阶段对查询进行分析。如果要检查实际性能，
EXPLAIN ANALYZE 通常是最佳选择。另一种方法是使用性能模式。

** 性能模式事件分析

通过性能模式，您可以分析在每个被检测的事件上花费了多少时间。您可以利用它来分析执行查询时花费时间的位置。本节将探讨
如何使用性能模式分析存储过程，以了解存储过程中哪些语句花费的时间最长，以及如何使用阶段事件分析单个查询。本节最后将
介绍如何使用 sys.ps_trace_thread() 存储过程创建线程工作图，以及如何使用 ps_trace_ statement_digest() 收
集具有给定摘要的语句的统计数据。

*** 检查存储过程

要检查存储过程所做的工作可能很困难，因为你不能直接在存储过程上使用 EXPLAIN，而且存储过程执行哪些查询可能并不明
显。相反，你可以使用性能模式。它可以记录执行的每一条语句，并将历史记录保存在 events_statements_history 表中。

除非需要在每个线程中存储超过 10 个查询，否则不需要做任何事情就可以开始分析。如果存储过程产生的语句事件超过 10
个，则需要增加 performance_schema_events_statements_history_size 选项的值（需要重新启动），使用
events_statements_history_long 表，或者使用 sys.ps_trace_thread() 存储过程（稍后将解释）。本讨论的其
余部分假定你可以使用 events_statements_ history 表。

以清单 20-12 中的存储过程为例，说明如何检查存储过程执行的查询。该存储过程也可在文件 listing_20_12.sql 中使
用，该文件可在任何模式中使用。

#+begin_src sql
  create schema if not exists chapter_20;

  delimiter $$

  create procedure chapter_20.testproc()
    sql security invoker
    not deterministic
    modifies sql data
  begin
    declare v_iter, v_id int unsigned default 0;
    declare v_name char(35) charset latin1;

    set v_id=ceil(rand()*4079);
    select Name
    into v_name
    from world.city
    where id=v_id;
    select * from world.city
    where Name=v_name;
  end $$
  delimiter ;
#+end_src

存储过程执行三个查询。第一个查询将 v_id 变量设置为介于 1 和 4079 之间的整数（world.city 表中的可用 ID 值）。
第二个查询获取具有该 ID 的城市名称。第三个查询找出所有与第二个查询中找到的名称相同的城市。

如果在连接中调用该存储过程，随后就可以分析该存储过程触发的查询以及这些查询的性能。例如

#+begin_src sql
  select ps_current_thread_id();
#+end_src

#+begin_src sql
  call chatper_20.testproc();
#+end_src

存储过程的输出是随机的，因此每次执行都会不同。然后，可以使用 PS_CURRENT_THREAD_ID() 函数（在 MySQL 8.0.15
及更早版本中使用 sys.ps_thread_id(NULL)）找到的线程 ID 来确定执行了哪些查询。

清单 20-13 展示了如何进行这种分析。您必须在不同的连接中进行此分析，将 THREAD_ID = 83 改为使用找到的线程 ID，
并在第二个查询中将 NESTING_EVENT_ID = 64 改为使用第一个查询中的事件 ID。输出中的一些细节已被删除，以便将重
点放在最感兴趣的值上。

#+begin_src sql
  select *
  from performance_schema.events_statements_history
  where thread_id=ps_current_thread_id()
    and event_name='statement/sql/call_procedure'
   order by event_id desc
   limit 1\G
#+end_src

#+begin_src sql
  select *
  from performance_schema.events_statements_history
  where thread_id=ps_current_thread_id()
    and nesting_event_id=64
  order by event_id\G
#+end_src

分析包括两个查询。第一个是确定存储过程的总体信息，方法是查询语句statement/sql/call_procedure事件（调用存
储过程的事件）的最新发生时间（按 EVENT_ID 排序）。

第二个查询请求同一线程的事件，嵌套事件 id 为 statement/sql/call_procedure 事件的事件 id。这些是存储过程执
行的语句。通过按 EVENT_ID 排序，语句将按其执行顺序返回。

第二个查询的结果显示，存储过程一开始有四条 SET 语句。其中有些是预期的，但也有一些是通过隐式设置变量触发的。最后
两行最值得讨论，因为它们显示执行了两个查询。首先，通过 ID 列（主键）查询了城市表。不出所料，它检查了一条记录。由
于查询结果保存在 v_name 变量中，因此 ROWS_AFFECTED 计数器被递增，而不是 ROWS_SENT。

第二个查询的结果并不理想。它也是查询城市表，但查询的是没有索引的名称。结果是检查了 4080 条记录，只返回了一条记
录。NO_INDEX_USED 列设置为 1，以反映执行了全表扫描。

使用这种方法检查存储过程的一个缺点是--如你所见--会很快用完历史记录表中的所有十行。另一种方法是启用
events_statements_history_long 消费者，并在闲置的测试系统上测试存储过程，或禁用其他连接的历史记录。这样就
可以分析执行多达 10000 条语句事件的存储过程。另一种方法是使用 sys.ps_trace_thread()存储过程，它也使用长历
史记录，但支持在存储过程执行时进行轮询，因此即使表不够大，无法在存储过程期间保存所有事件，它也可以收集事件。

本示例使用语句事件来分析性能。有时，您需要知道在更细粒度的级别上发生了什么，在这种情况下，您需要开始查看阶段事件。

*** 分析事件

如果需要更详细地了解查询花费的时间，第一步就是查看阶段事件。也可以选择包括等待事件。由于处理等待事件的步骤与处理
阶段事件的步骤基本相同，因此读者可自行分析查询的等待事件。

#+begin_comment
  因此，在生产系统中启用阶段和等待事件时要谨慎。某些等待事件，尤其是与互斥相关的等待事件，也会对查询产生影响，以
  至于影响分析结论。使用等待事件分析查询通常只有性能架构师和使用 MysQl 源代码的开发人员才需要做。
#+end_comment

生成的阶段事件数远远大于语句事件数。这意味着，为了避免阶段事件从历史表中消失，建议在空闲的测试系统上执行分析，并
使用 events_stages_history_long 表。该表默认为禁用；要启用它，请启用相应的消费者：

#+begin_src sql
  update performance_schema.setup_consumers
  set enabled='YES'
  where name in ('events_stages_current',
                 'events_stages_history_long');

#+end_src

events_stages_history_long 消费者依赖于 events_stages_ current 消费者，因此需要同时启用这两个消费者。默
认情况下，只启用与进度信息相关的阶段事件。对于一般分析，您需要启用所有阶段事件：

#+begin_src sql
  update performance_schema.setup_instruments
  set enabled='YES', timed='YES'
  where name like 'stage/%';
#+end_src

此时的分析方法与分析存储过程时的方法基本相同。例如，假设性能模式线程 ID 为 83 的连接正在执行以下查询：

#+begin_src sql
  select *
  from world.city
  where  Name='Sydney';
#+end_src

假设这是最后执行的查询，则可以获得每个阶段所花费的时间，如清单 20-14 所示。您需要在一个单独的连接中执行此操作，
并更改 SET @thread_id = 83 以使用您连接的线程 ID。除了时间明显不同外，查询所经历的阶段列表也可能不同。

#+begin_src sql
  select event_id, SUBSTRING_INDEX(event_name, '/', -1) as Event,
    FORMAT_PIC_TIME(timer_wait) as Latency
  from performance_schema.events_stages_history_long
  where thread_id=ps_current_thread_id() and
    nesting_event_id=(select event_id
                      from performance_schema.events_statements_history
                      where thread_id=ps_current_thread_id()
                      order by event_id desc
                      limit 1);
#+end_src

从 events_ stages_history_long 表中选取事件 ID、阶段名称（为简洁起见，去掉了事件全名的前两个部分）和使用
FORMAT_PICO_TIME() 函数格式化的延迟时间（在 MySQL 8.0.15 及更早版本中使用 sys.format_time() 函数）。
WHERE 子句根据执行查询的连接的线程 ID 和嵌套事件 ID 进行筛选。嵌套事件 id 设置为线程 id 等于 83 的连接最近
执行的语句的事件 id。结果显示，查询中最慢的部分是发送数据，也就是存储引擎查找和发送行的阶段。

用这种方法分析查询的主要问题是，要么受限于默认情况下每个线程只能保存 10 个事件，要么在完成检查之前，事件有可能从
历史长表中删除。为了解决这个问题，我们创建了 sys.ps_trace_thread() 存储过程。

*** 分析sys.ps_trace_thread()过程

当你需要分析一个复杂的查询或一个执行超过几条语句的存储程序时，你可以使用一个能在执行过程中自动收集信息的工具。在
sys 模式中，ps_trace_thread()存储过程就是一个这样的选项。

存储过程会循环一段时间，轮询历史长表中的新事务、语句、阶段和等待事件。作为选项，存储过程还可以设置性能模式以包含
所有事件，并使消费者能够记录事件。不过，由于包含所有事件通常太过繁琐，因此建议自行设置性能模式，以记录和消耗分析
所需的事件。

另一个可选功能是在监控开始时重置性能模式表。如果可以接受删除长期历史表的内容，这将非常有用。

调用存储过程时，必须提供以下参数：

+ Thread ID：要监控的性能模式线程 ID。
+ Out File: 将结果写入的文件。结果使用点图描述语言创建。这要求已设置 secure_file_ priv 选项，允许向目标目
  录写入文件，且文件不存在，执行存储过程的用户拥有 FILE 权限。
+ Max Runtime: 以秒为单位的最长监控时间。支持指定精度为 1/100 秒的值。如果该值设置为空，运行时间将设置为 60
   秒。
+ Poll Interval: 轮询历史记录表格的时间间隔。该值的设置精度为 1/100 秒。如果该值设置为空，则轮询间隔将设置为
  一秒。
+ Refresh: 布尔值，表示是否重置用于分析的性能模式表。
+ Auto Setup: 布尔值，表示是否启用程序可以使用的所有仪器和用户。启用后，程序完成时将恢复当前设置。
+ Debug: 布尔值，表示是否包含附加信息，如事件是在源代码的哪个位置触发的。这在包含等待事件时非常有用。

使用存储过程 ps_trace_thread() 的示例见清单 20-15。在执行存储过程时，先前的 testproc() 存储过程将从被监控
的线程中调用。该示例假定一开始使用的是默认的性能模式设置。  

#+begin_src sql
  update performance_schema.setup_consumers
  set enabled='YES'
  where name='events_statements_history_long';
#+end_src

#+begin_src sql
  select ps_current_thread_id();
#+end_src

#+begin_src sql
  call sys.ps_trace_thread(32, '/mysql/files/thread32.gv',
                           10, 0.1, False, False, False);
#+end_src

#+begin_src sql
  call testproc();
#+end_src

在本例中，只启用了 events_statements_history_long 消费者。这样就可以记录调用 testproc() 存储过程所产生的
所有语句事件，就像之前手动完成的那样。使用 PS_CURRENT_THREAD_ID() 函数（在 MySQL 8.0.15 及更早版本中，使
用 sys.ps_thread_id(NULL)）可以获得要监控的线程 ID。

针对线程 ID 32 调用 ps_trace_thread()存储过程，并将输出写入 /mysql/files/thread_32.gv。存储过程每 0.1
秒轮询一次，持续 10 秒，并且禁用所有可选功能。

您需要一个能理解点格式的程序将其转换成图像。其中一个选择是 Graphviz 工具集，它可以通过软件包仓库从多个 Linux
发行版中获取。也可从项目主页 www.graphviz.org/ 下载，适用于 Linux、Microsoft Windows、macOS、Solaris
和 FreeBSD。程序的输出显示了如何将带有点图定义的文件转换为 PDF 或 PNG 文件的示例。图 20-13 显示了 CALL
testproc() 语句生成的图形。

[[./images/n5iGWg.png]]


语句图包含的信息与手动分析存储过程时相同。对于像 testproc() 这样简单的存储过程，生成语句图的优势有限，但对于更
复杂的存储过程或分析启用了低级事件的查询，语句图是可视化执行流程的好方法。

另一个可以帮助你分析查询的系统模式存储过程是 ps_trace_ statement_digest()存储过程。


*** ps_trace_statement_digest过程分析

作为使用性能模式分析查询的最后一个示例，将演示 sys 模式中的存储过程 ps_trace_statement_digest()。该存储过
程获取摘要，然后监控 events_statements_history_long 表和 events_stages_history_long 表，查找与该摘
要中的语句相关的事件。分析结果包括摘要数据和详细信息，如运行时间最长的查询的查询计划。

程序包含五个参数，均为必选参数。这些参数是

+ Digest: 要监控的摘要。如果语句摘要与所提供的摘要一致，则无论默认模式如何，都将对其进行监控。
+ Runtime: 监控多长时间，以秒为单位。不允许使用小数。
+ Poll Interval: 轮询历史记录表格的时间间隔。该值可按 1/100 秒的精度设置，且必须小于 1 秒。
+ Refresh: 布尔值，表示是否重置用于分析的性能模式表。
+ Auto Setup: 布尔值，表示是否启用程序可以使用的所有仪器和用户。启用后，程序完成时将恢复当前设置。

例如，可以使用存储过程 sys.ps_trace_statement_digest() 启动监控，并在监控过程中执行以下查询（监控示例如
下）：

#+begin_src sql
  SELECT * FROM world.city WHERE CountryCode = 'AUS';
  SELECT * FROM world.city WHERE CountryCode = 'USA';
  SELECT * FROM world.city WHERE CountryCode = 'CHN';
  SELECT * FROM world.city WHERE CountryCode = 'ZAF';
  SELECT * FROM world.city WHERE CountryCode = 'BRA';
  SELECT * FROM world.city WHERE CountryCode = 'GBR';
  SELECT * FROM world.city WHERE CountryCode = 'FRA';
  SELECT * FROM world.city WHERE CountryCode = 'IND';
  SELECT * FROM world.city WHERE CountryCode = 'DEU';
  SELECT * FROM world.city WHERE CountryCode = 'SWE';
  SELECT * FROM world.city WHERE CountryCode = 'LUX';
  SELECT * FROM world.city WHERE CountryCode = 'NZL';
  SELECT * FROM world.city WHERE CountryCode = 'KOR';
#+end_src

在这些查询中，哪个查询速度最慢，执行情况可能各不相同。

清单 20-16 展示了一个示例，说明如何使用存储过程监控选择某一国家所有城市的查询。在示例中，摘要是通过
STATEMENT_DIGEST() 函数找到的，但也可能是通过基于 events_statements_summary_by_digest 表的监控找到的。
将由存储过程启用所需的工具和消费者，并重置受监控的表，以避免包含监控开始前执行的语句。轮询频率设置为每 0.5 秒轮
询一次。为减少输出宽度，删除了阶段事件名称的 stage/sql/ 前缀，并缩短了 EXPLAIN 输出的虚线。本书 GitHub 代
码库中的 listing_20_16.txt 文件提供了未经修改的输出结果。

#+begin_src sql
  set @digest=STATEMENT_DIGEST('select * from world.city where CountryCode=''AUS''');
  call sys.ps_trace_statement_digest(@digest, 60, 0.5, true, true);
#+end_src

输出首先是分析过程中发现的所有查询的摘要。共检测到 13 次执行，耗时共计 7.29 毫秒。总体摘要还包括各阶段所用时间
的汇总。输出的下一部分是 13 次执行中最慢的一次的详细信息。输出的最后部分是最慢查询的 JSON 格式查询计划。

在生成查询计划时，您需要注意一个限制。执行 EXPLAIN 语句时，默认模式将设置为与存储过程执行时相同的模式。这意味
着，如果查询是在不同的模式下执行的，并且没有使用完全限定的表名（即包括模式名），那么 EXPLAIN 语句就会失败，存储
过程也不会输出查询计划。

** 总结

本章介绍了如何分析你认为可能需要优化的查询。本章的主要内容集中在 EXPLAIN 语句，它是分析查询的主要工具。本章的其余
部分介绍了优化器跟踪以及如何使用性能模式分析查询。

EXPLAIN 语句支持几种不同的格式，可帮助您以最适合自己的格式获取查询计划。传统格式使用标准表格输出，JSON 格式返回
详细的 JSON 文档，树格式显示相对简单的执行树。树形格式仅在 MySQL 8.0.16 及更高版本中支持，并且要求使用 Volcano
iterator 执行器来执行查询。JSON 格式是 MySQL 工作台中的 Visual Explain 功能用来创建查询计划图表的格式。

EXPLAIN 输出中提供了大量有关查询计划的信息。会议讨论了传统格式的字段以及 JSON 中最常遇到的字段。这包括详细讨论选
择类型、访问类型和额外信息。最后，通过一系列示例展示了如何使用这些信息。

优化器跟踪可用于获取信息，了解优化器如何最终生成 EXPLAIN 语句返回的查询计划。对于最终用户来说，通常没有必要使用优
化器跟踪，但如果想更多地了解优化器以及产生查询计划的决策过程，使用优化器跟踪可能会很有用。

本章的最后一部分展示了如何使用性能模式事件来确定语句耗费的时间。首先展示了如何将存储过程分解为单个语句，然后再将语
句分解为多个阶段。最后，使用存储过程 ps_trace_thread()自动进行分析并创建事件图表，使用存储过程
ps_trace_statement_digest()收集给定语句摘要的统计数据。

本章分析了查询。有时有必要考虑整个事务。下一章将介绍如何分析事务。

* 事务

事务是报表的老大哥。它们将多个变更组合在一起，无论是在单个报表中还是在多个报表中，因此它们作为一个整体被应用或放弃。
大多数情况下，事务不过是事后才想到的，只是在需要同时应用多个报表时才会考虑。这种考虑事务的方式很糟糕。事务对于确保数
据完整性非常重要，如果使用不当，可能会导致严重的性能问题。

本章首先通过回顾事务对锁和性能的影响，讨论为什么需要从性能角度认真对待事务。本章其余部分的重点是分析事务，首先使用信
息模式中的 INNODB_TRX 表，然后是 InnoDB 监控、InnoDB 指标，最后是性能模式。

** 事务的影响

如果把事务看作是用于分组查询的容器，那么它似乎是一个很单纯的概念。但是，我们必须明白，由于事务为查询组提供了原子性
因此事务活动的时间越长，与查询相关的资源被保留的时间就越长，事务中完成的工作越多，需要的资源就越多。在事务提交之前，
查询一直在使用哪些资源？主要是锁和撤销日志。

#+begin_comment
  InnoDB 支持只读事务，这种事务的开销比读写事务低。对于自动提交的单语句事务，InnoDB 会尝试自动判断语句是否为只读
  事务。对于多语句事务，可以在启动时明确指定该事务为只读事务： start transaction read only；
#+end_comment

*** 锁
查询执行时会获取锁，使用默认事务隔离级别--REPEATABLE READ--时，所有锁都会被保留，直到事务提交。使用 READ
COMMITTED 事务隔离级别时，一些锁可能会被释放，但至少涉及已更改记录的锁会被保留。锁本身是一种资源，但也需要内存
来存储有关锁的信息。在正常工作负载中，你可能不会考虑太多，但庞大的事务可能会占用大量内存，以至于事务失败时出现
ER_LOCK_TABLE_FULL 错误：

#+begin_comment
  ERROR: 1206: The total number of locks exceeds the lock table size
#+end_comment

从记录在错误日志中的警告信息（稍后会详细说明）可以看出，锁所需的内存来自缓冲池。因此，持有的锁越多，持有的时间越
长，用于缓存数据和索引的内存就越少。

#+begin_comment
  如果一个事务因为使用了所有锁内存而中止，就会造成四重打击。首先，要更新足够多的行，使用足够多的锁内存来触发错误，
  需要花费一段时间。这些工作都白费了。其次，由于所需的更改次数较多，回滚事务可能需要很长时间。第三，在使用锁内存
  时，InnoDB 实际上处于只读模式（可能会有一些小事务），直到回滚完成后才会释放锁内存。第四，缓冲池中用于缓存数据
  和索引的空间所剩无几。
#+end_comment

出错前，错误日志中会出现警告，称超过 67% 的缓冲池被用于锁或自适应哈希索引：

#+begin_comment
  2019-07-06T03:23:04.345256Z 10 [Warning] [MY-011958] [InnoDB] Over 67 percent of the buffer
  pool is occupied by lock heaps or the adaptive hash index! Check that your transactions do
  not set too many row locks. Your buffer pool size is 7 MB. Maybe you should make the buffer
  pool bigger?. Starting the InnoDB Monitor to print diagnostics, including lock heap and hash
  index sizes.
#+end_comment

警告之后是 InnoDB 监控程序的定期重复输出，因此可以确定哪些事务是罪魁祸首。有关事务的 InnoDB 监控输出将在 "
InnoDB 监控 "一节中讨论。

元数据锁是事务中经常被忽视的一种锁类型。当语句查询表时，会使用共享元数据锁，该元数据锁会一直保留到事务结束。当表
上有元数据锁时，任何连接都不能对表执行任何 DDL 语句（包括 OPTIMIZE TABLE）。如果一个 DDL 语句被一个长期运行
的事务阻塞，它将反过来阻塞使用该表的所有新查询。第 22 章将举例说明如何使用本章的某些方法来调查此类问题。

*** Undo日志

如果选择回滚事务，则还必须按要求保存事务处理期间所做的更改。这一点很容易理解。更令人吃惊的是，即使是没有做出任何
更改的事务，也会使其他事务中的撤销信息继续存在。当事务需要读视图（一致快照）时，就会出现这种情况，在使用
REPEATABLE READ 事务隔离级别时，事务的持续时间就是这种情况。读视图意味着，无论其他事务是否更改数据，事务都将
返回与事务启动时间相对应的行数据。为了做到这一点，有必要保留在事务生命周期内发生变化的行的旧值。在 MySQL 5.7
及更早版本中，这可能意味着 ibdata1 文件最终会变得很大。(在 MySQL 8 中，撤销日志总是存储在可以截断的单独撤销
表空间中）。

#+begin_comment
  READ COMMITTED 事务隔离级别更不容易产生大量撤销日志，因为读视图只在查询期间保留。
#+end_comment

撤销日志活动部分的大小用历史列表长度来衡量。历史记录列表长度是撤销日志尚未清除的已提交事务的数量。这意味着你不能
使用历史列表长度来衡量行更改的总量。它能告诉你的是，在执行查询时必须考虑的变化的链接列表中有多少单位的旧行（每个
事务一个单位）。链接列表越长，查找每条记录的正确版本的成本就越高。最后，如果历史记录列表很大，就会严重影响所有查
询的性能。

#+begin_comment
  历史列表长度问题是使用逻辑备份工具（如 mysqlpump 和 mysqldump）创建大型数据库备份的最大问题之一，这些工具
  使用单个事务来获得一致的备份。如果备份期间提交的事务很多，备份会导致历史列表长度变得非常大。
#+end_comment

什么是大历史清单长度？这没有明确的规定，只是说越小越好。通常情况下，当历史记录列表的事务长度达到一千到一百万时，
性能问题就会开始显现，但何时成为瓶颈取决于撤销日志中提交的事务以及历史记录列表长度较大时的工作量。

当不再需要最旧的部分时，InnoDB 会在后台自动清除历史列表。有两个选项可以控制清除，还有两个选项可以影响无法清除时
的情况。这两个选项是

+ innodb_purge_batch-size: 每批清除的撤销日志页数。该批次由清除线程分担。生产系统中不允许更改此选项。默认值
  为 300，有效值介于 1 和 5000 之间。

+ innodb_purge_threads: 并行使用的清除线程数。如果数据更改跨越多个表，并行度越高越有用。另一方面，如果所有更
  改都集中在少数几个表上，则最好使用较低的数值。更改清除线程数需要重启 MySQL。默认值为 4，有效值在 1 到 32 之
  间。

+ innodb_max_purge_lag: 当历史列表的长度超过 innodb_max_purge_lag 的值时，就会在更改数据的操作中添加延
  迟，以降低历史列表的增长速度，但代价是增加语句延迟。默认值为 0，这意味着不会增加延迟。有效值为 0-4294967295。

+ innodb_max_purge_lag_delay: 当历史列表长度大于 innodb_max_purge_lag 时，DML 查询的最大延迟。

通常没有必要更改这些设置；但在特殊情况下，更改这些设置可能会有用。如果清除线程跟不上，可以尝试根据被修改表的数量
来更改清除线程的数量；被修改的表越多，清除线程的数量就越多。在更改清除线程数量时，必须从更改前的基线开始监控效
果，以便了解更改后是否有所改善。

最大清除滞后选项可用于降低修改数据的 DML 语句的速度。当写入仅限于特定连接，且延迟不会导致为保持相同吞吐量而创建
额外写入线程时，该选项就非常有用。

如何监控事务的时间、锁使用的内存以及历史列表的长度？可以使用信息模式、InnoDB 监控和性能模式来获取这些信息。

** INNODB_TRX

信息模式中的 INNODB_TRX 表是有关 InnoDB 事务的最专用信息源。它包含的信息有：事务何时开始、有多少行被修改、有多
少个锁。sys.innodb_lock_waits 视图也使用 INNODB_TRX 表来提供锁等待问题所涉及的事务的一些信息。表 21-1 总结
了表中的列。

#+information_schema.innodb_trx
| 数据类型                     | 描述                                                                                                               |
|----------------------------+-------------------------------------------------------------------------------------------------------------------|
| trx_id                     | 事务 ID。在引用事务或与 InnoDB 监控程序的输出进行比较时，这个 id 会很有用。否则，这个 id 应被视为纯粹的内部 id，不具有任何意义。该 id 只分配给修改了数据或锁定了记录的事务；只执行了只读 SELECT 语句的事务会有一个类似 421124985258256 的虚拟 id，如果事务开始修改或锁定记录，该 id 就会改变。 |
| trx_state                  | 事务的状态。可以是运行（RUNNING）、锁定等待（LOCK WAIT）、滚回（ROLLING BACK）和提交（COMMITTING）。                         |
| trx_started                | 使用系统时区启动事务的时间。                                                                                           |
| trx_requested_lock_id      | 当 trx_state 为 LOCK WAIT 时，这一列显示事务正在等待的锁的 id。                                                          |
| trx_wait_started           | 当 trx_state 为 LOCK WAIT 时，此列显示锁定等待开始的时间（使用系统时区）。                                                 |
| trx_weight                 | 以修改的行和持有的锁来衡量事务完成了多少工作。在发生死锁时，该权重用于决定回滚哪个事务。权重越高，说明已完成的工作越多。              |
| trx_mysql_thread_id        | 执行事务的连接的连接 ID（与性能模式线程表中的 PROCESSLIST_ID 列相同）。                                                    |
| trx_query                  | 事务当前执行的查询。如果事务处于空闲状态，则查询为空。                                                                     |
| trx_operation_state        | 事务执行的当前操作。即使正在执行查询，该值也可能为空。                                                                     |
| trx_tables_in_use          | 事务使用的表格数量。                                                                                                  |
| trx_tables_locked          | 事务持有行锁的表数量。                                                                                                |
| trx_lock_structs           | 事务创建的锁结构数。                                                                                                  |
| trx_lock_memory_bytes      | 事务持有的锁使用的内存量（以字节为单位）。                                                                               |
| trx_rows_locked            | 事务持有的记录锁数量。虽然称为行锁，但也包括索引锁。                                                                       |
| trx_rows_modified          | 事务修改的行数。                                                                                                     |
| trx_concurrency_tickets    | 当 innodb_thread_concurrency 不为 0 时，一个事务会被分配到 innodb_concurrency_tickets 票，它可以在允许另一个事务执行工作前使用这些票。这一列显示还剩多少张票。 |
| trx_isolation_level        | 事务使用的事务隔离级别。                                                                                              |
| trx_unique_checks          | 连接是否启用了 unique_checks 变量。                                                                                   |
| trx_foreign_key_checks     | 连接是否启用了外键检查变量。                                                                                           |
| trx_last_foreign_key_error | 事务遇到的最后一次（如果有）外键错误的错误信息。                                                                          |
| trx_adaptive_hash_latched  | 事务是否锁定了自适应哈希索引的一部分。共有 innodb_adaptive_hash_index_parts 部分。这一列实际上是一个布尔值。                   |
| trx_adaptive_hash_timeout  | 是否在多次查询中保持对自适应哈希索引的锁定。如果自适应散列索引只有一个部分，且不存在竞争，那么超时会倒计时，当超时为 0 时，锁就会被释放。 如果存在竞争或有多个部分，那么每次查询后锁都会被释放，超时值为 0。 |
| trx_is_read_only           | 事务是否为只读事务。事务可以通过明确声明或启用自动提交的单语句事务（InnoDB 可以检测到查询只读取数据）来实现只读。                   |
| trx_autocommit_non_locking | 当事务是单语句非锁定 SELECT 且自动提交选项已启用时，这一列被设为 1。当这一列和 trx_is_read_only 都为 1 时，InnoDB 可以优化事务以减少开销。 |

通过 INNODB_TRX 表中的信息，可以确定哪些事务影响最大。清单 21-1 显示了两个事务返回信息的示例。

#+begin_src sql
  select *
  from information_schema.innodb_trx;
#+end_src

第一行显示的是一个修改数据的事务示例。在检索信息时，已经修改了 4,544,497 条记录，并且还有一些记录锁。你还可以看
到，该事务仍在执行查询（UPDATE 语句）。

第二行是启用自动提交后执行 SELECT 语句的示例。由于启用了自动提交，事务中只能有一条语句（显式 START TRANSACTION
禁用了自动提交）。trx_query 列显示这是一条 SELECT COUNT(*) 查询，没有任何锁定子句，因此是一条只读语句。这意味
着 InnoDB 可以跳过一些准备工作，比如为事务保存锁和撤销信息，从而减少事务的开销。为了反映这一点，
trx_autocommit_non_locking 列被设置为 1。

您应该担心哪些事务，这取决于系统的预期工作量。如果是 OLAP 工作负载，预计会有相对长时间运行的 SELECT 查询。对于纯
粹的 OLTP 工作负载，任何运行时间超过几秒钟、修改记录超过几条的事务都可能是问题的征兆。例如，要查找超过一分钟的事
务，可以使用以下查询：

#+begin_src sql
  select *
  from information_schema.innodb_trx
  where trx_started < now() - interval 1 minute;
#+end_src

** innodb监控

InnoDB 监控程序是 InnoDB 信息的瑞士军刀，其中也包括事务信息。InnoDB 监控器输出中的 "事务"（TRANSACTIONS）部
分专门用于显示事务信息。这些信息不仅包括事务列表，还包括历史列表长度。清单21-2显示了InnoDB监视器的摘录，其中
的事务部分是在INNODB_TRX表之前的输出之后的示例。

#+begin_src sql
  show engine innodb status\G
#+end_src

事务 "部分的顶部显示事务 id 计数器的当前值，然后是已从撤消日志中清除的信息。其中显示，事务 id 小于 5894 的撤销
日志已被清除。清除时间越靠后，历史列表长度（本节第三行）就越大。从 InnoDB 监控输出中读取历史列表长度是获取历史列
表长度的传统方法。下一节将介绍如何以更好的方式获取用于监控目的的值。

该部分的其余部分是事务列表。请注意，虽然输出生成了与 INNODB_TRX 中相同的两个活动事务，但事务列表只包括一个活动事
务（UPDATE 语句的事务）。在 MySQL 5.7 及更高版本中，只读非锁定事务不包括在 InnoDB 监控事务列表中。因此，如果需
要包含所有活动事务，最好使用 INNODB_TRX 表。

** innodb_metrics和sys.metrics

InnoDB 监控报告对于数据库管理员了解 InnoDB 的总体情况非常有用，但对于监控来说，它就没有那么有用了，因为它需要进
行解析，才能以监控可以使用的方式获取数据。在本章的前面部分，我们已经介绍了如何从 information_schema 表中获取事
务信息。INNODB_TRX 表中获取事务信息，那么历史列表长度等度量指标又如何呢？

InnoDB 度量系统包含多个度量，可在 information_schema.INNODB_METRICS 视图中显示有关事务的信息。这些度量指标
都位于事务子系统中。清单 21-3 列出了事务度量的列表、默认情况下是否启用这些度量，以及解释度量内容的简短注释。

#+begin_src sql
  select name, count, status, comment
  from information_schema.innodb_metrics
  where subsystem='transaction'\G
#+end_src

其中最重要的指标是 trx_rseg_history_len，即历史列表长度。这也是唯一一个默认启用的指标。与提交和回滚相关的指标可
用于确定有多少读写、只读和非锁定只读事务，以及它们的提交和回滚频率。回滚次数多说明存在问题。如果怀疑重做日志是瓶颈，
可以使用 trx_on_log_% 指标来衡量事务提交期间有多少事务在等待重做日志。

#+begin_comment
  使用 innodb_monitor_enable 选项可以启用 InnoDB 指标，使用 innodb_monitor_disable 选项则可以禁用 InnoDB
  指标。这可以动态完成。
#+end_comment

查询 InnoDB 指标的另一种便捷方法是使用 sys.metrics 视图，该视图还包括全局状态变量。清单 21-4 展示了一个使用
sys.metrics 视图获取当前值和是否启用度量的示例。

#+begin_src sql
  select Variable_name as name,
         Variable_value as value,
         Enabled
  from sys.metrics
  where Type='InnoDB Metrics - transaction';
#+end_src

这表明历史记录列表长度为 45，是一个很好的低值，因此几乎没有撤销日志的开销。其余指标均已禁用。

到目前为止，有关事务信息的讨论都是关于所有事务或单个事务的汇总统计信息。如果想更深入地了解事务完成了哪些工作，就需
要使用性能模式。

** Performance Schema中的事务

性能模式支持 MySQL 5.7 及更高版本中的事务监控，而且在 MySQL 8 中默认已启用。 除了与 XA 事务和保存点相关的事务
细节外，性能模式中可用的事务细节不多，无法从信息模式中的 INNODB_TRX 表中获取。不过，性能模式事务事件的优势在于，
可以将它们与其他事件类型（如语句）结合起来，以获取有关事务工作的信息。这也是本节的重点。此外，性能模式还提供包含汇
总统计信息的汇总表。

*** 事务事件和语句

性能模式中调查事务的主要表是事务事件表。有三个表用于记录当前或最近的事务：events_transactions_current、
events_transactions_history 和 events_transactions_history_long。它们的列汇总如表 21-2 所示。

| 列名                             | 描述                                                                                          |
|---------------------------------+----------------------------------------------------------------------------------------------|
| thread_id                       | 执行事务的连接的性能模式线程 ID。                                                                 |
| event_id                        | 事件的事件 ID。您可以使用事件 ID 为线程中的事件排序，或将其与线程 ID 一起作为事件表之间的外键。            |
| end_event_id                    | 事务完成时的事件 ID。如果事件 id 为空，则表示事务仍在进行中。                                         |
| event_name                      | 事务事件名称。目前该列的值总是 transaction。                                                       |
| state                           | 事务的状态。可能的值是 ACTIVE、COMMITTED 和 ROLLED BACK。                                         |
| trx_id                          | 目前尚未使用，将始终为空。                                                                        |
| gtid                            | 事务的 GTID。如果 GTID 是自动确定的（通常），则返回 AUTOMATIC。这与执行事务的连接的 gtid_next 变量相同。 |
| xid_format_id                   | xa事务，格式id                                                                                 |
| xid_gtrid                       | xa事务，gtrid                                                                                 |
| xid_bqual                       | xa事务，bqual值                                                                                |
| xa_state                        | 对于 xa 事务，指事务的状态。可以是 "ACTIVE"、"IDLE"、"PREPARED"、"ROLLEDBACK"或 "COMMITED"。        |
| source                          | 记录事件的源代码文件和行号。                                                                      |
| timer_start                     | 事件开始的时间（皮秒）。                                                                         |
| timer_end                       | 事件完成的时间（皮秒）。如果事务尚未完成，则该值对应当前时间。                                          |
| timer_wait                      | 事件完成的时间（皮秒）。如果事务尚未完成，则该值对应当前时间。                                          |
| access_mode                     | 事务是只读模式（READ ONLY）还是读写模式（READ WRITE）。                                            |
| isolation_level                 | 事务隔离级别                                                                                   |
| autocommit                      | 事务是否根据自动提交选项自动提交，以及是否已启动显式事务，可能的值是 "NO"和 "YES"。                      |
| number_of_savepoints            | 事务中创建的保存点数量。                                                                         |
| number_of_rollback_to_savepoint | 事务回滚到保存点的次数。                                                                         |
| number_of_release_savepoint     | 事务释放保存点的次数。                                                                           |
| object_instance_begin           | 该字段目前未使用，始终设置为空。                                                                   |
| nesting_event_id                | 触发事务的事件 ID。                                                                             |
| nesting_event_type              | 触发事务的事件类型。                                                                                             |

如果使用 XA 事务，事务事件表在需要恢复事务时非常有用，因为格式 id、gtrid 和 bqual 值可直接从表中获取，而不像
XA RECOVER 语句那样需要解析输出。同样，如果使用保存点，也可以获得保存点使用情况的统计信息。其他信息与
INNODB_TRX 表中的信息非常相似。

以使用 events_transactions_current 表为例，可以启动两个事务。第一个事务是普通事务，更新几个城市的人口：

#+begin_src sql
  start transaction;
  UPDATE world.city SET Population = 5200000 WHERE ID = 130;
  UPDATE world.city SET Population = 4900000 WHERE ID = 131;
  UPDATE world.city SET Population = 2400000 WHERE ID = 132;
  UPDATE world.city SET Population = 2000000 WHERE ID = 133;
#+end_src

第二事务中的XA事务

#+begin_src sql
  xa start 'abc', 'def', 1;
  update world.city set Population=900000 where id=3805;
#+end_src

清单 21-5 显示了 events_transactions_current 表的输出示例，其中列出了当前活动的事务。

#+begin_src sql
  select *
  from performance_schema.events_transaction_current
  where state='ACTIVE'\G
#+end_src

第 1 行中的事务是普通事务，而第 2 行中的事务是 XA 事务。两个事务都是由一条语句启动的，这可以从嵌套事件类型中看
出来。如果要查找触发事务的语句，可以使用该语句查询 events_statements_history 表，如

#+begin_src sql
  select sql_text
  from performance_schema.events_statements_history
  where thread_id=54 and event_id=38;
#+end_src

这表明 THREAD_ID = 54 执行的事务是使用 START TRANSACTION 语句启动的。由于 events_statements_history
表只包括连接的最后十条语句，因此不能保证启动事务的语句仍在历史表中。如果要查看单语句事务或自动提交被禁用时的第一
条语句（仍在执行中），则需要查询 events_statements_current 表。

事务和语句之间的关系也是反过来的。如果给定了事务事件 ID 和线程 ID，就可以使用语句事件历史和当前表查询该事务执行
的最后十条语句。清单 21-6 显示了 THREAD_ID = 54 和事务 EVENT_ID = 39（来自清单 21-5 的第 1 行）的示例，
其中包括开始事务的语句和后续语句。

#+begin_src sql
  set @thread_id=54;
      @event_id=39;
      @nesting_event_id=38;
  select event_id, sql_text,
         format_pico_time(timer_wait) as Latency,
         if(end_event_id is null, 'YES', 'NO') as IsCurrent
  from (( select event_id, end_event_id,
                 timer_wait,
                 sql_text, nesting_event_id,
                 nesting_event_type
          from performance_schema.events_statements_current
          where thread_id=@thread_id
          ) union (
          select event_id, end_event_id,
                 timer_wait,
                 sql_text, nesting_event_id,
                 nesting_event_type
          from performance_schema.events_statements_history
          where thread_id=@thread_id
          )
          ) events
  where (nesting_event_type='TRANSACTION'
         and nesting_event_id=@event_id)
         or event_id=@nesting_event_id
  order by event_id desc\G
#+end_src

子查询（派生表）从 events_statements_current 和 events_statements_history 表中查找线程的所有语句事件。
有必要包括当前事件，因为事务中可能有正在执行的语句。筛选语句的条件是语句是事务的子事务或事务的嵌套事件（EVENT_ID
= 38）。这将包括从事务开始的所有语句。如果有正在进行的语句，则最多有 11 条语句，否则最多有 10 条。

END_EVENT_ID 用于确定语句当前是否正在执行，而语句的顺序则使用 EVENT_ID 倒序排列，因此最新的语句在第 1 行，
最旧的语句（START TRANSACTION 语句）在第 5 行。

这类查询不仅有助于调查仍在执行查询的事务。当你遇到一个空闲事务，并想知道该事务在被遗弃前做了什么时，它也非常有用。
查找活动事务的另一种相关方法是使用 sys.session 视图，该视图使用 events_ transactions_current 表来包含每
个连接的事务状态信息。清单 21-7 显示了一个查询活动事务的示例，其中不包括执行查询的连接的记录。

#+begin_src sql
  select *
  from sys.session
  where trx_state='ACTIVE'
        and conn_id <> connection_id()\G
#+end_src

这表明第一行中的事务活动时间已超过 11 分钟，距离最后一次执行查询的时间是 690 秒（11.5 分钟）（您的值会有所不
同）。last_statement 可以用来确定连接执行的最后一次查询。这是一个放弃事务的示例，它阻止了 InnoDB 清除撤销日
志。导致放弃事务的最常见原因是数据库管理员以交互方式启动事务时走神了，或者自动提交功能被禁用，没有意识到事务已经
启动。

#+begin_comment
  如果禁用自动提交功能，请务必在工作结束时提交或回滚。有些连接器默认情况下会禁用自动提交功能，因此请注意您的应用
  程序可能没有使用服务器默认设置。
#+end_comment

您可以回滚事务，以避免更改任何数据。对于第一个（正常）事务

#+begin_src sql
  rollback;
#+end_src

#+begin_src sql
  xa end 'abc', 'def', 1;
  xa rollback 'abc', 'def', 1;
#+end_src

性能模式表有助于分析事务的另一种方法是使用汇总表获取汇总数据。

*** 事务汇总表

报表汇总表可以用来获取执行报表的报告，同样，事务汇总表也可以用来分析事务的 使用情况。虽然这些表不像报表那样有用，
但它们确实能让人了解哪些连接和账户以不同的方式使用事务。

共有五个事务汇总表，可按全局或账户、主机、线程或用户对数据进行分组。所有汇总表也都按事件名称分组，但由于目前只有
一个事务事件（事务），所以操作为空。这些表格是

+ events_transactions_summary_global_by_event_name: 所有事务汇总。该表只有一行。
+ events_summary_by_account_by_event_name: 按用户名和主机名分组的事务。
+ events_transactions_summary_by_host_event_name: 按账户主机名分组的事务。
+ events_transactions_summary_by_thread_by_event_name: 按线程分组的事务。只包括当前存在的线程。
+ events_transactions_summary_by_user_by_event_name: 按账户用户名分组的事件。

每个表格包括事务统计分组列和三组列：总计、读写事务和只读事务。这三组列中的每一组都包含事务总数以及总延迟、最小延
迟、平均延迟和最大延迟。清单 21-8 显示了 events_transactions_summary_global_by_ event_name 表中的数据
示例。  

#+begin_src sql
  select *
  from performance_schema.events_transaction_summary_global_by_event_name\G
#+end_src

当你研究输出结果时，可能会惊讶于事务数量之多，尤其是读写事务。请记住，在查询 InnoDB 表时，即使没有明确指定事务，
所有事务都是事务。因此，即使是查询单行的简单 SELECT 语句也算作一个事务。关于读写事务和只读事务之间的分配问题，
只有在明确启动只读事务的情况下，性能模式才会将其视为只读事务：

#+begin_src sql
  start transaction read only;
#+end_src

当 InnoDB 确定自动提交的单语句事务可被视为只读事务时，该事务仍将计入性能模式中的读写统计数据。

** 总结

事务是数据库中的一个重要概念。它有助于确保您可以将更改作为一个单元应用到多条记录中，并确保您可以选择是应用更改还是
回滚更改。

本章首先讨论了为什么必须了解事务的使用方式。虽然事务本身可以被视为更改的容器，但锁会一直保留到事务提交或回滚为止，
而且会阻碍撤销日志的清除。锁和大量撤销日志都会影响查询的性能，即使查询不是在造成大量锁或大量撤销日志的事务中执行。
锁使用的内存来自缓冲池，因此可用于缓存数据和索引的内存较少。以历史列表长度衡量的大量撤销日志意味着 InnoDB 在执行
语句时必须考虑更多的行版本。

本章其余部分将介绍如何分析正在进行的事务和过去的事务。信息模式中的 INNODB_TRX 表是当前事务的最佳信息来源。InnoDB
监控和 InnoDB 指标是对其的补充。对于 XA 事务和使用保存点的事务，或者需要调查哪些语句作为事务的一部分被执行时，需
要使用性能模式的事务事件表。性能模式还包括汇总表，可用于获取更多信息，了解谁在读写事务和只读事务上花费了时间。

* 诊断锁争用

在第 18 章中，我们向你介绍了 MySQL 中的锁世界。如果你还没有阅读第 18 章，强烈建议你现在就阅读，因为本章与之密切相
关。如果你已经有一段时间没有阅读本章了，你甚至可以温习一下。锁问题是导致性能问题的常见原因之一，其影响可能非常严重。
在最糟糕的情况下，查询可能会失败，连接会堆积起来，从而无法建立新的连接。因此，了解如何调查锁问题并对问题进行补救非常
重要。

本章将讨论四类锁问题：

+ FLUSH锁
+ 元数据和模式锁
+ 记录锁包括间隙锁
+ 死锁

每一类锁都使用不同的技术来确定锁争用的原因。在阅读示例时，应注意类似的技术也可用于调查与示例不完全匹配的锁问题。

对于每个锁类，讨论分为六个部分：

+ 症状： 这描述了您如何确定遇到了这种锁问题。
+ 原因： 遇到这类锁问题的根本原因。这与第 18 章中关于锁的一般性讨论有关。
+ 设置： 这包括设置锁问题的步骤，如果你想自己尝试的话。由于锁争用需要多个连接，提示符（如连接 1>）用于告知哪些语句应
  使用哪个连接。如果你想在没有更多信息的情况下完成调查，可以跳过这一部分，在完成调查后再回头查看。
+ 调查： 调查细节。这部分借鉴了第 18 章中的 "监控锁 "部分。
+ 解决方案： 如何立即解决锁具问题，从而最大限度地减少由此造成的停机时间。
+ 预防： 讨论如何降低遇到问题的几率。这与第 18 章中的 "减少锁定问题 "部分密切相关。


** FLUSH锁

在 MySQL 中遇到的常见锁问题之一是刷新锁。发生此问题时，用户通常会抱怨查询无法返回，监控结果可能会显示查询堆积如
山，最终导致 MySQL 连接耗尽。围绕刷新锁的问题有时也是最难调查的锁问题之一。

*** 症状

刷新锁问题的主要症状是数据库停止运行，所有使用某些或所有表的新查询都在等待刷新锁。需要注意的蛛丝马迹包括以下几点：

+ 新查询的查询状态是 "Waiting for table flush"。这可能发生在所有新查询中，也可能只发生在访问特定表的查询中。
+ 越来越多的连接建立
+ 最终，新连接会因为 MySQL 没有连接而失败。新连接收到的错误信息是 ER_CON_COUNT_ERROR："ERROR 1040
  (HY000)： 使用传统 MySQL 协议（默认端口为 3306）时，连接数过多"；使用传统 MySQL 协议（默认端口为 3306）
  时，错误信息为 "MySQL 错误 5011： 使用 X 协议（默认端口 33060）时，则会出现 "MySQL Error 5011: Could
   not open session"（MySQL 错误 5011：无法打开会话）。
+ 至少有一个查询的运行时间晚于最早的刷新锁请求。
+ 进程列表中可能有 FLUSH TABLES 语句，但并非总是如此。
+ 当 FLUSH TABLES 语句等待 lock_wait_ 超时时，会出现 ER_LOCK_WAIT_TIMEOUT 错误： ERROR: 1205： 超过
  锁等待超时；尝试重启事务。由于 lock_wait_timeout 的默认值是 365 天，因此只有在超时时间缩短的情况下才有可
  能发生这种情况。
+ 如果使用设置了默认模式的 mysql 命令行客户端进行连接，连接可能会在进入提示符前挂起。如果在连接打开的情况下更改
  默认模式，也会发生同样的情况。

#+begin_comment
  如果使用 -A 选项启动客户端，禁止收集自动完成信息，就不会出现 mysql 命令行客户端阻塞的问题。更好的解决方案是
  使用 MysQL shell，以不会因刷新锁而阻塞的方式获取自动完成信息。
#+end_comment

*** 原因

当连接请求刷新表时，需要关闭对该表的所有引用，这意味着不能使用该表进行任何活动查询。因此，当刷新请求到达时，必须
等待使用要刷新的表的所有查询结束。请注意，除非明确指定要刷新哪些表，否则必须完成的只是查询，而不是整个事务。显然，
所有表都被刷新的情况最严重，例如，由于使用了带读锁的刷新表（FLUSH TABLES WITH READ LOCK），这意味着在刷新语
句继续执行之前，所有活动查询都必须完成。

当等待刷新锁成为一个问题时，这意味着有一个或多个查询阻止了 FLUSH TABLES 语句获得刷新锁。由于 FLUSH TABLES
语句需要独占锁，因此它反过来又阻止了后续查询获取它们需要的共享锁。

在备份过程中，备份进程需要刷新所有表并获得读锁，以便创建一致的备份。

当 FLUSH TABLES 语句超时或被杀死，但后续查询没有进行时，可能会出现一种特殊情况。出现这种情况的原因是底层表定
义缓存（TDC）版本锁没有释放。这种情况可能会造成混乱，因为不清楚为什么后续查询仍在等待表刷新。

*** 引发

要调查的锁情况涉及三个连接（不包括用于调查的连接）。第一个连接执行慢速查询，第二个连接使用读锁刷新所有表，最后一
个连接执行快速查询。语句如下

#+begin_src sql
  select city.*, sleep(180) from world.city where id=130;
#+end_src

#+begin_src sql
  flush tables with read lock;
#+end_src

#+begin_src sql
  select * from world.city where id=3805;
#+end_src

在第一个查询中使用 SLEEP(180) 意味着您有三分钟（180 秒）的时间来执行其他两个查询并进行调查。如果需要更长的时
间，可以延长睡眠时间。现在可以开始调查了。

*** 调查

调查刷新锁需要查看实例上运行的查询列表。与其他锁争议不同，没有性能模式表或 InnoDB 监控报告可以用来直接查询阻塞
查询。

清单 22-1 显示了使用 sys.session 视图的输出示例。使用其他方法获取查询列表也会产生类似的结果。线程和连接 ID
以及语句延迟会有所不同。

#+begin_src sql
  select thd_id, conn_id, state,
         current_statement,
         statement_latency
  from sys.session
  where command='Query'\G
#+end_src

输出中有四个查询。默认情况下，sys.session 和 sys.processlist 视图会根据执行时间降序排列查询。这使得调查诸
如围绕刷新锁的争用等问题变得容易，因为在查找原因时，查询时间是首要考虑因素。

首先要查找 FLUSH TABLES 语句（稍后将讨论没有 FLUSH TABLES 语句的情况）。在本例中，就是 thd_id = 53（第二
行）。注意 FLUSH 语句的状态是 "等待表刷新"。然后查找运行时间较长的查询。在这种情况下，只有一个查询：thd_id =
30 的查询。就是这个查询阻止了 "FLUSH TABLES WITH READ LOCK "完成。一般来说，查询可能不止一个。

其余两个查询分别是被 FLUSH TABLES WITH READ LOCK 语句阻塞的查询和获取输出的查询。前三个查询共同构成了一个典
型的例子，即一个长期运行的查询阻塞了 FLUSH TABLES 语句，而 FLUSH TABLES 语句又反过来阻塞了其他查询。

您也可以从 MySQL 工作台获取进程列表，在某些情况下还可以从监控解决方案中获取。图 22-1 显示了如何从 MySQL
Workbench 获取进程列表。

[[./images/EUbIpJ.png]]

要在 MySQL Workbench 中获取进程列表报告，请在屏幕左侧导航窗格中选择 "管理 "下的 "客户端连接 "项。您不能选择
包含哪些列，为了使文本可读，屏幕截图中只包含了报告的一部分。Id 列对应 sys.session 输出中的 conn_id，Thread
（最右边一列）对应 thd_id。完整截图以 figure_22_1_workbench_flush_lock.png 的形式包含在本书的 GitHub
代码库中。

图 22-2 显示了 MySQL 企业监控器 (MEM) 在相同锁定情况下的进程报告示例。

[[./images/ulaybe.png]]

进程报告可在各实例的 "度量 "菜单项下找到。您可以选择输出中要包含的列。在本书的 GitHub 代码库中，可以找到该报告
的示例以及更多细节，如 figure_22_2_mem_flush_lock.png。

MySQL Workbench 和 MySQL Enterprise Monitor 等报告的一个优势是，它们使用现有连接来创建报告。如果锁问题
导致所有连接都被使用，那么使用监控解决方案获取查询列表就非常有价值。

如前所述，FLUSH TABLES 语句并不总是出现在查询列表中。仍有查询等待刷新表的原因是底层 TDC 版本锁。调查的原则保
持不变，但可能会让人感到困惑。清单 22-2 显示了这样一个示例，使用了相同的设置，但在调查之前杀死了执行 flush 语
句的连接（在执行 FLUSH TABLES WITH READ LOCK 的连接中，可在 MySQL Shell 中使用 Ctrl+C）。

#+begin_src sql

  select thd_id, conn_id, state,
         current_statement,
         statement_latency
  from sys.session
  where command='Query'\G
#+end_src

除了 FLUSH TABLES 语句消失之外，这种情况与前一种情况相同。在这种情况下，找出等待时间最长、状态为 "Waiting
for table flush." 的查询。运行时间比该查询等待时间长的查询是阻止释放 TDC 版本锁的查询。在本例中，这意味着
thd_id = 30 是阻塞查询。

一旦确定了问题和所涉及的主要疑问，就需要决定如何解决问题。

*** 解决方案

解决问题有两个层面。首先，需要解决查询无法执行的直接问题。其次，需要努力避免将来出现该问题。本小节将讨论直接的
解决方案，下一小节将考虑如何降低问题发生的几率。

要解决眼前的问题，可以选择等待查询完成或开始杀死查询。如果能在刷新锁争用正在进行时将应用程序重定向到使用另一个实
例，那么就可以通过让长期运行的查询完成来自行解决问题。如果正在运行或等待的查询中存在数据更改查询，那么在这种情
况下，就需要考虑在所有查询完成后，系统是否会处于一致的状态。一种选择是继续以只读模式运行，在不同的实例上执行读
取查询。

如果决定终止查询，可以尝试终止 FLUSH TABLES 语句。如果有效，这是最简单的解决方案。但是，正如我们讨论过的那样，
这并不总是有用的，在这种情况下，唯一的解决办法就是杀死阻止 FLUSH TABLES 语句完成的查询。如果长期运行的查询看起
来像失控查询，而且执行这些查询的应用程序/客户端不再等待它们，那么你可能想杀死这些查询，而不首先尝试杀死 FLUSH
TABLES 语句。

杀死查询时的一个重要考虑因素是数据被更改的程度。对于纯粹的 SELECT 查询（不涉及存储例程）来说，改变的数据总是很
少，从已完成工作的角度来看，杀死查询是安全的。但对于 INSERT、UPDATE、DELETE 和类似查询，如果杀死查询，则必须
回滚已更改的数据。回滚更改所需的时间通常要比一开始进行更改所需的时间长，因此如果更改较多，回滚需要等待很长时间。
你可以使用 information_schema.INNODB_TRX 表，通过查看 trx_rows_modified 列来估算工作量。如果需要回滚
的工作很多，通常最好让查询完成。

#+begin_comment
  当 DML 语句被杀死时，它所做的工作必须回滚。回滚所需的时间通常比创建更改所需的时间长，有时甚至更长。如果考虑杀
  死一个长期运行的 DML 语句，就需要考虑到这一点。
#+end_comment

*** 预防

发生刷新锁争用的原因是长时间运行的查询和 FLUSH TABLES 语句的组合。因此，要防止出现这一问题，需要研究如何避免
这两种情况同时出现。

本书其他章节将讨论查找、分析和处理长时间运行的查询。其中一个特别值得关注的选项是为查询设置超时。使用
max_execution_time 系统变量和 MAX_EXECUTION_TIME(N) 优化器提示，可以为 SELECT 语句设置超时，这是防止查
询失控的好方法。某些连接器也支持定时查询。

#+begin_comment
  为了避免 SELECT 查询长时间运行，可以配置 max_ execution_time 选项或设置 MAX_EXECUTION_TIME(N) 优化器
  提示。
#+end_comment

有些长时间运行的查询是无法避免的。这可能是报告任务、构建缓存表或其他必须访问大量数据的任务。在这种情况下，你能做
的就是尽量避免它们在需要刷新表时运行。一种方法是将长期运行的查询安排在与需要刷新表时不同的时间运行。另一种方法是
让长期运行的查询在与需要刷表的作业不同的实例上运行。

备份是一项需要刷新表的常见任务。在 MySQL 8 中，可以通过使用备份和日志锁来避免这个问题。例如，MySQL Enterprise
Backup (MEB) 在 8.0.16 及更高版本中就是这样做的，因此 InnoDB 表永远不会被刷新。另外，你也可以在使用率较低的
时段执行备份，这样发生冲突的可能性就会降低，或者甚至可以在系统处于只读模式时执行备份，从而完全避免使用 "FLUSH
TABLES WITH READ LOCK"。

** 元数据和模式锁

在 MySQL 5.7 及更早版本中，元数据锁经常引起混淆。问题在于，谁拥有元数据锁并不明显。在 MySQL 5.7 中，元数据锁的
检测被添加到性能模式中，而在 MySQL 8.0 中，它被默认启用。启用工具后，就很容易确定是谁阻塞了试图获取锁的连接。

*** 症状

元数据锁争用的症状与刷新锁争用类似。在典型情况下，会有一个长期运行的查询或事务、一个等待元数据锁的 DDL 语句，以
及可能的查询堆积。需要注意的症状如下：

+ DDL 语句和可能的其他查询卡在 "Waiting for table metadata lock" 状态。
+ 查询可能堆积如山。正在等待的查询都使用同一个表。(如果有多个表的 DDL 语句在等待元数据锁，则可能有不止一组查询
  在等待）。
+ 当 DDL 语句已等待 lock_wait_timeout 时，会出现 ER_LOCK_WAIT_TIMEOUT 错误： ERROR: 1205： 超过锁等
  待超时；尝试重启事务。由于 lock_wait_timeout 的默认值是 365 天，因此只有在超时时间缩短的情况下才有可能发
  生这种情况。
+ 有一个长期运行的查询或一个长期运行的事务。在后一种情况下，事务可能处于空闲状态，或正在执行查询，而查询并不使用
  DDL 语句所作用的表。

最后一点可能会让情况变得扑朔迷离：可能没有任何长期运行的查询是导致锁问题的明确候选。那么，元数据锁争用的原因是什
么呢？

*** 原因

请记住，元数据锁的存在是为了保护Performance Schema（以及与显式锁一起使用）。只要事务处于活动状态，模式保护
就会一直存在，因此当事务查询表时，元数据锁将持续到事务结束。因此，你可能看不到任何长时间运行的查询。事实上，持有
元数据锁的事务可能根本不会做任何事情。

简而言之，元数据锁存在的原因是，一个或多个连接可能依赖于特定表的模式不发生变化，或者它们使用 LOCK TABLES 或
FLUSH TABLES WITH READ LOCK 语句显式锁定了表。

*** 引发

元数据锁的示例调查与前一个示例一样使用了三个连接。第一个连接正在进行事务处理，第二个连接试图为事务处理使用的表添
加索引，第三个连接试图对同一表执行查询。这些查询是

#+begin_src sql
  start transaction;
  select * from world.city where id=3805\G
#+end_src

#+begin_src sql
  alter table world.city add index(Name);
#+end_src

#+begin_src sql
  select * from world.city where id=130;
#+end_src

此时，您可以开始调查。这种情况不会自行解决（除非 lock_wait_timeout 的值很低，或者您准备等待一年），因此您有
足够的时间。当你想解决阻塞时，可以开始终止连接 2 中的 ALTER TABLE 语句，以避免修改 world.city 表。然后在连
接 1 中提交或回滚事务。

*** 调查

如果启用了 wait/lock/metadata/sql/mdl 性能模式工具（MySQL 8 中的默认设置），就可以直接调查元数据锁问题。
你可以使用性能模式中的 metadata_locks 表来列出已授权和待授权的锁。不过，获取锁情况摘要的更简单方法是使用系统
模式中的 schema_table_lock_waits 视图。

以清单 22-3 中涉及三个连接的元数据锁等待问题为例。我们选择的 WHERE 子句只包含本次调查所关注的记录。

#+begin_src sql
  select thd_id, conn_id, state,
         current_statement,
         statement_latency
  from sys.session
  where command='Query' or trx_state='ACTIVE';
#+end_src

有两个连接正在等待元数据锁（在 world.city 表上）。还有第三个连接（conn_id = 9）处于空闲状态，这可以从语句延
迟的 NULL 中看出（在某些早于 8.0.18 的版本中，还可以看出当前语句为 NULL）。在这种情况下，查询列表仅限于有活动
查询或活动事务的查询，但通常一开始会有一个完整的进程列表。不过，为了便于关注重要部分，输出是经过过滤的。

一旦知道存在元数据锁问题，就可以使用 sys.schema_table_ lock_waits 视图来获取有关锁竞争的信息。清单 22-4
显示了与刚才讨论的进程列表相对应的输出示例。

#+begin_src sql
  select *
  from sys.schema_table_lock_waits\G
#+end_src

输出结果显示，有四个查询正在等待和阻塞。这可能令人吃惊，但这是因为涉及多个锁，而且存在一连串的等待。每一行都是一
对等待和阻塞连接。输出使用 "pid "表示进程列表 id，这与前面输出中使用的连接 id 相同。信息包括锁在哪个进程上、等
待连接的详细信息、阻塞连接的详细信息，以及可用于杀死阻塞查询或连接的两个查询。

第一行显示进程列表 id 7090 正在等待自己。这听起来像是死锁，但其实不是。原因是 ALTER TABLE 首先获取了一个可以
升级的共享锁，然后试图获取正在等待的独占锁。因为没有明确的信息说明哪个现有锁实际上阻塞了新锁，所以这些信息最终被
包含在内。

第二行显示 SELECT 语句正在等待进程列表 id 7090，也就是 ALTER 表。这就是连接开始堆积的原因，因为 DDL 语句需
要独占锁，所以会阻止共享锁请求。

第三行和第四行揭示了锁争用的根本问题。进程列表 id 9 阻塞了其他两个连接，这表明这是阻塞 DDL 语句的罪魁祸首。因
此，在调查类似问题时，请查找等待独占元数据锁的连接是否被其他连接阻塞。如果输出中有大量记录，还可以查找造成阻塞最
多的连接，并以此为起点。清单 22-5 举例说明了如何做到这一点。

#+begin_src sql
  select *
  from sys.schema_table_lock_waits
  where waiting_lock_type='EXCLUSIVE'
    and waiting_pid <> blocking_pid\G
#+end_src

#+begin_src sql
  select blocking_pid, count(*)
  from sys.schema_table_lock_waits
  where waiting_pid <> blocking_pid
  group by blocking_pid
  order by count(*) desc;
#+end_src

第一个查询是查找等待独占元数据锁的情况，其中阻塞进程列表 id 并非其自身。在这种情况下，会立即给出主阻塞争用。第
二个查询会确定每个进程列表 id 引发的阻塞查询的数量。这可能不像本例中显示的那么简单，但使用如图所示的查询将有助于
缩小锁竞争的范围。

确定了锁争用的源头后，就需要确定事务在做什么。在本例中，锁争用的根源是连接 9。回过头来看进程列表输出，你会发现在
这种情况下它什么也没做：

[[./images/81thGM.png]]

这个连接做了什么来获取元数据锁？当前语句中没有涉及 world.city 表的语句，这表明该连接正在打开一个活动事务。在这
种情况下，事务处于空闲状态（如 statement_ latency = NULL 所示），但也可能是正在执行与 world.city 表元数据
锁无关的查询。无论哪种情况，都需要确定事务在当前状态之前正在做什么。为此，您可以使用性能模式和信息模式。清单
22-6 展示了一个调查事务状态和最近历史的示例。

#+begin_src sql
  select *
  from information_schema.innodb_trx
  where trx_mysql_thread_id=9\G
#+end_src

#+begin_src sql
  select *
  from performance_schema.events_transaction_current
  where thread_id=30;
#+end_src

#+begin_src sql
  select event_id, current_schema,
         sql_text
  from performance_schema.events_statements_history
  where thread_id=30
        and nesting_event_id=113
        and nesting_event_type='TRANSACTION'\G
#+end_src

#+begin_src sql
  select attr_name, attr_value
  from performance_schema.session_connect_attrs
  where processlist_id=9;
#+end_src

第一个查询使用信息模式中的 INNODB_TRX 表。例如，该表显示了事务的启动时间，因此可以确定该事务活动了多长时间。
trx_rows_modified列也很有用，如果决定回滚事务，可以知道事务修改了多少数据。请注意，InnoDB所谓的MySQL线
程id（trx_mysql_thread_id列）实际上是连接id。

第二个查询使用性能模式中的 events_transactions_current 表来获取更多事务信息。可以使用 TIMER_WAIT 列来确
定事务的时间。该值的单位是皮秒，因此使用 FORMAT_PICO_TIME() 函数更容易理解该值：

#+begin_src sql
  select format_pico_time(5749876163371648) as age;
#+end_src

如果使用的是 MySQL 8.0.15 或更早版本，请使用 sys.format_time() 函数。

第三个查询使用 events_statements_history 表查找事务中以前执行的查询。NESTING_EVENT_ID 列被设置为
events_transactions_current 表输出中的 EVENT_ID 值，NESTING_EVENT_TYPE 列被设置为与事务匹配。这样可以
确保只返回正在进行的事务的子事件。结果按（语句的）EVENT_ID 排序，以获得语句的执行顺序。默认情况下，
events_statements_history 表最多包含连接的十个最新查询。

在本例中，调查显示事务执行了两个查询：一个是从 world.city 表中选择，另一个是从 world.country 表中选择。正是
第一个查询导致了元数据锁争用。

第四个查询使用 session_connect_attrs 表查找连接提交的属性。并非所有客户端和连接器都会提交属性，或者它们可能
被禁用，因此这些信息并不总是可用的。当属性可用时，它们有助于找出违规事务是从哪里执行的。在本例中，可以看到连接来
自 MySQL Shell (mysqlsh)。如果要提交空闲事务，这可能会很有用。

*** 解决方案

对于元数据锁争用，基本上有两种解决方法：完成阻塞事务或终止 DDL 语句。要完成阻塞事务，需要提交或回滚。如果杀死连
接，就会触发事务回滚，因此需要考虑有多少工作需要回滚。要提交事务，必须找到执行连接的位置并提交。不能提交由其他连
接拥有的事务。

杀死 DDL 语句将允许其他查询继续进行，但如果锁被一个已放弃但仍在运行的事务持有，则从长远来看，这并不能解决问题。
对于有一个废弃事务持有元数据锁的情况，可以选择同时杀死 DDL 语句和废弃事务的连接。这样就可以避免 DDL 语句在事务
回滚时继续阻塞后续查询。回滚完成后，就可以重试 DDL 语句。

*** 预防

避免元数据锁争用的关键在于，在需要为事务使用的表执行 DDL 语句的同时，避免长时间运行的事务。例如，可以在知道没有
长期运行事务的情况下执行 DDL 语句。还可以将 lock_wait_timeout 选项设置为一个较低的值，使 DDL 语句在
lock_wait_timeout 秒后放弃。虽然这并不能避免锁问题，但可以避免 DDL 语句阻止其他查询的执行，从而减轻问题。这
样你就可以找到根本原因，而不必担心应用程序的大部分无法运行。

还可以减少事务的活动时间。一种方法是，如果不要求所有操作都作为原子单元执行，则将一个大事务拆分成几个较小的事务。
此外，还应确保事务处于活动状态时不进行交互工作、文件 I/O、向最终用户传输数据等操作，从而避免不必要地长时间打开
事务。

造成事务长期运行的一个常见原因是，应用程序或客户端根本没有提交或回滚事务。在禁用自动提交选项时，这种情况尤其容易
发生。禁用自动提交选项后，任何查询（即使是普通的只读 SELECT 语句）都会在没有活动事务的情况下启动新事务。这意味
着，一个看似无害的查询可能会启动一个事务，而如果开发人员没有意识到自动提交已被禁用，那么开发人员可能不会考虑显式
地结束该事务。在 MySQL 服务器中，自动提交设置默认是启用的，但有些连接器会默认禁用它。

** 行级锁

记录锁争用是最常遇到的情况，但通常也是干扰最小的情况，因为默认锁等待超时时间只有 50 秒，所以不会出现查询堆积的情况。
尽管如此，在某些情况下，记录锁会导致 MySQL 停止运行，这一点将在下文中说明。本节将研究InnoDB记录锁的一般问题和
更详细的锁等待超时问题。对死锁的具体调查将推迟到下一节。

*** 症状

InnoDB 记录锁争用的症状通常很微妙，不易察觉。在严重情况下，会出现锁等待超时或死锁错误，但在很多情况下，可能没有
直接的症状。相反，症状是查询比正常速度慢。这种情况从慢几分之一秒到慢好几秒不等。

在锁等待超时的情况下，会出现类似下面示例中的 ER_LOCK_WAIT_TIMEOUT 错误：

#+begin_comment
  ERROR: 1205: Lock wait timeout exceeded; try restarting transaction
#+end_comment

当查询比没有锁竞争的情况下要慢时，最可能的方法是通过监控来发现问题，可以使用类似于 MySQL 企业监控器中的查询分析
器，或者使用 sys.innodb_lock_waits 视图来检测锁竞争。图 22-3 显示了查询分析器中的一个查询示例。在讨论调查记
录锁竞争时，将使用 sys 模式视图。本书 GitHub 代码库中的 figure_22_3_quan.png 也提供了该图的全尺寸。

[[./images/nz6Zch.png]]

在图中，请注意查询的延迟图是如何在期末增加，然后又突然下降的。在正常化查询的右侧还有一个红色图标--该图标表示查询
返回了错误。本例中的错误是锁等待超时，但从图中无法看出。规范化查询左侧的甜甜圈形图表也显示了红色区域，表明该查询
的查询响应时间指数有时很差。顶部的大图显示了一个小的下降，表明实例中存在足够多的问题，导致实例性能普遍下降。

还有一些实例级指标可以显示实例的锁定情况。这些指标对于监控一段时间内的总体锁争用情况非常有用。清单 22-7 显示了
使用 sys.metrics 视图的可用指标。

#+begin_src sql
  select Variable_name,
         Variable_value as Value,
         Enabled
  from sys.metrics
  where Variable_name like 'innodb_row_lock%'
        or Type='InooDB Metrics - lock';
#+end_src

在本次讨论中，innodb_row_lock_% 和 lock_timeouts 指标最引人关注。这三个时间变量的单位都是毫秒。可以看到，
只有一个锁等待超时，这本身并不一定值得担心。你还可以看到，有 354 次锁不能立即被授予（innodb_row_lock_waits）
而且等待时间超过了 51 秒（innodb_row_lock_time_max）。当锁争用的总体水平上升时，这些指标也会随之上升。

比手动监控指标更好的方法是，确保您的监控解决方案记录指标，并能将其绘制成时间序列图。图 22-4 显示了图 22-3 中发
现的同一事件的指标绘制示例。

[[./images/ZOybgi.png]]

[[./images/cxIMLx.png]]

比手动监控指标更好的方法是，确保您的监控解决方案记录指标，并能将其绘制成时间序列图。图 22-4 显示了图 22-3 中发
现的同一事件的指标绘制示例。

*** 原因

InnoDB 在行数据、索引记录、间隙和插入意图锁上使用共享锁和独占锁。当有两个事务试图以相互冲突的方式访问数据时，其
中一个查询将不得不等待，直到所需的锁可用。简而言之，可以同时批准两个共享锁请求，但一旦有了独占锁，任何连接都不能
获取同一记录上的锁。

由于排他锁最有可能导致锁竞争，因此通常是更改数据的 DML 查询导致 InnoDB 记录锁竞争。另一个原因是 SELECT 语句
通过添加 FOR SHARE（或 LOCK IN SHARE MODE）或 FOR UPDATE 子句来进行抢占式锁定。

*** 引发

这个示例只需要两个连接就能设置调查场景，第一个连接有一个正在进行的事务，第二个连接试图更新第一个连接持有锁的记录。
由于等待 InnoDB 锁的默认超时时间是 50 秒，因此可以选择增加第二个连接的超时时间，以便有更多时间执行调查。设置为

#+begin_src sql
  start transaction;
  update world.city
         set Population=5000000
         where id=130;
#+end_src

#+begin_src sql
  set session innodb_lock_wait_timeout=300;
  start transaction;
  update world.city
         set Population=Population*1.10
         where CountryCode='AUS';
#+end_src

在此示例中，连接 2 的锁定等待超时设置为 300 秒。连接 2 的 START TRANSACTION 不是必需的，但可以在完成后回滚
两个事务，以避免更改数据。

*** 调查

记录锁的调查与元数据锁的调查非常相似。你可以查询性能模式中的 data_locks 表和 data_lock_waits 表，它们将分别
显示原始锁数据和待处理锁。innodb_lock_waits 视图可以查询这两个表，找到其中一个被另一个阻塞的锁对。

#+begin_comment
  在 MysQL 5.7 及更早版本中，信息模式中有两个类似的表，分别命名为 INNODB_LOCKS 和 INNODB_LOCK_WAITS。使
  用 innodb_lock_waits 视图的一个好处是，它在不同版本的 MysQL 中都能正常工作（但在 MysQL 8 中增加了一些信
  息）。
#+end_comment

在大多数情况下，最简单的方法是使用 innodb_lock_waits 视图开始调查，然后再根据需要深入研究性能模式表。清单
22-8 显示了 innodb_lock_waits 视图在锁等待情况下的输出示例。

#+begin_src sql
  select * from sys.innodb_lock_waits\G
#+end_src

根据列名的前缀，输出中的列可分为五个部分。这些组是

+ wait_: 这些列显示了锁定等待时间的一些一般信息。
+ locked_: 这些列显示了从模式到索引的锁定内容以及锁定类型。
+ waiting_: 这些列显示了等待授予锁的事务的详细信息，包括请求的查询和锁模式。
+ blocking_: 这些列显示了阻塞锁请求的事务的详细信息。请注意，在示例中，阻塞查询为 NULL。这意味着在生成输出时，
  事务处于空闲状态。即使列出了阻塞查询，该查询也可能与争夺的锁无关，只是查询是由持有锁的同一事务执行的。
+ sql_kill_: 这两列提供了 KILL 查询，可用于杀死阻塞查询或连接。

#+begin_comment
  blocking_query 列是阻塞事务当前执行的查询（如果有）。这并不意味着查询本身一定会导致锁请求阻塞。
#+end_comment

blocking_query 列为 NULL 的情况很常见。这意味着阻塞事务当前没有执行查询。这可能是因为它处于两个查询之间。如果
这段时间较长，则表明应用程序正在执行本应在事务之外完成的工作。更常见的情况是，事务不执行查询是因为它被遗忘了，要
么是在交互会话中人忘记结束事务，要么是应用程序流无法确保事务提交或回滚。  

*** 解决方案

解决方案取决于锁等待的程度。如果只是几个查询的锁等待时间很短，那么让受影响的查询等待锁可用就可以了。请记住，锁的
存在是为了确保数据的完整性，因此锁本身并不是一个问题。只有当锁对性能造成重大影响或导致查询失败到无法重试的程度时，
锁才会成为问题。

如果锁状态持续时间较长，尤其是阻塞事务已被放弃，则可以考虑杀死阻塞事务。与往常一样，如果阻塞事务执行了大量工作，
则需要考虑回滚可能会耗费大量时间。

对于因锁等待超时错误而失败的查询，应用程序应重试。请记住，默认情况下，锁等待超时只会回滚超时发生时正在执行的查询。
事务的其他部分将保持查询前的状态。因此，未能处理超时可能会使未完成的事务带有自己的锁，从而导致更多的锁问题。是只
回滚查询还是回滚整个事务，由 innodb_rollback_on_timeout 选项控制。

#+begin_comment
 对于因锁等待超时错误而失败的查询，应用程序应重试。请记住，默认情况下，锁等待超时只会回滚超时发生时正在执行的查
 询。事务的其他部分将保持查询前的状态。因此，未能处理超时可能会使未完成的事务带有自己的锁，从而导致更多的锁问题。
 是只回滚查询还是回滚整个事务，由 innodb_rollback_on_timeout 选项控制。
#+end_comment

*** 预防

防止严重的记录级锁竞争主要遵循第 18 章 "减少锁定问题 "一节中讨论的指导原则。概括讨论内容，减少锁等待争用的方法
主要是减少事务的大小和持续时间，使用索引减少访问记录的数量，以及可能将事务隔离级别切换为 READ COMMITTED，以提
前释放锁并减少间隙锁的数量。

** 死锁

死锁是数据库管理员最担心的锁问题之一。这一方面是因为它的名称，另一方面是因为它与讨论过的其他锁问题不同，总是会导致
错误。不过，与其他锁问题相比，死锁并没有什么特别令人担忧的地方。相反，死锁会导致错误，这意味着你可以更早地了解死锁，
并自行解决锁问题。

*** 症状

症状很简单。死锁的受害者会收到一个错误，而 lock_deadlocks InnoDB 指标会递增。被 InnoDB 选为受害者的事务将收
到的错误信息是 ER_LOCK_DEADLOCK：

#+begin_comment
  ERROR: 1213: Deadlock found when trying to get lock; try restarting transaction
#+end_comment

lock_deadlocks 指标对于监控死锁发生的频率非常有用。跟踪 lock_deadlocks 值的便捷方法是使用 sys.metrics
视图：

#+begin_src sql
  select *
  from sys.metrics
  where Variable_name='lock_deadlocks'\G
#+end_src

例如，你也可以通过执行 SHOW ENGINE INNODB STATUS 来检查 InnoDB 监控输出中的 LATEST DETECTED DEADLOCK
部分。这将显示上次死锁发生的时间，因此你可以用它来判断死锁发生的频率。如果启用了innodb_print_all_deadlocks
选项，错误锁就会有很多死锁信息输出。在讨论了死锁的原因和设置之后，我们将在 "调查 "中详细介绍 InnoDB 监控死锁输
出的细节。

*** 原因

死锁是由于两个或多个事务获得锁的顺序不同造成的。每个事务最终都会持有另一个事务需要的锁。这种锁可能是记录锁、间隙
锁、谓词锁或插入意图锁。图 22-5 显示了一个循环依赖引发死锁的示例。

[[./images/9i4VFg.png]]

图中所示的死锁是由于表的主键上有两个记录锁造成的。这是可能出现的最简单的死锁之一。正如在研究死锁时所显示的那样，
圆圈可能比这更复杂。

*** 引发

这个示例与上一个示例一样使用了两个连接，但这次两个连接都进行了更改，最后连接 1 被阻塞，直到连接 2 因错误回滚其
更改。连接 1 将澳大利亚及其城市的人口更新了 10%，而连接 2 将澳大利亚的人口更新为达尔文市的人口，并添加了该城市。
语句如下

#+begin_src sql
  start transaction;
  update world.city set Population=Population*1.10
  where CountryCode='AUS';
#+end_src

#+begin_src sql
  start transaction;
  update world.country set Population=Population+14600
  where Code='AUS';
#+end_src

#+begin_src sql
  update world.country set Population=Population*1.1
  where Code='AUS';
#+end_src

#+begin_src sql
  insert into world.city
  values(4080, 'Darwin', 'AUS', 'Northern Territory', 146000);
  rollback;
#+end_src

#+begin_src sql
  rollback;
#+end_src

关键是这两个事务都更新了城市和国家表，但顺序相反。设置完成后，显式回滚这两个事务，以确保表没有变化。

*** 调查

分析死锁的主要工具是 InnoDB 监控输出中有关最新检测到的死锁信息的部分。如果启用了 innodb_print_all_deadlocks
选项（默认情况下关闭），也可以从错误日志中获得死锁信息；不过，这些信息是一样的，因此不会改变分析结果。

死锁信息包含四个部分，分别描述死锁和结果。这些部分是

+ 死锁何时出现
+ 涉及死锁的第一个事务的信息。
+ 涉及死锁的第二个事务的信息。
+ 哪个事务被回滚。启用 innodb_print_all_deadlocks 时，错误日志中不会包含这些信息。

这两笔事务的编号是任意的，主要目的是为了能够参考其中一笔或另一笔事务。包含事务信息的两个部分是最重要的部分。它们
包括事务的活动时间、关于事务大小的一些统计信息（如占用的锁、撤销日志条目等）、阻塞等待锁的查询，以及死锁中涉及的
锁的信息。  

与使用 data_locks 表、data_lock_waits 表和 sys.innodb_lock_waits 视图相比，锁信息并不容易解释。不过，只
要尝试执行几次分析，就不会太难。

#+begin_comment
  在测试系统中故意制造一些死锁，然后研究由此产生的死锁信息。既然知道了查询，就更容易解释锁数据了。
#+end_comment

对于本次死锁调查，请参考清单 22-9 中显示的 InnoDB 监控程序的死锁部分。该清单较长，行数较多，因此本书的 GitHub
代码库中也提供了 listing_22_9_ deadlock.txt，你可以用自己选择的文本编辑器打开输出。

#+begin_src sql
  show engine innodb status\G
#+end_src

死锁发生在服务器时区 2019 年 11 月 6 日 18:29:07。您可以使用此信息查看该信息是否与用户报告的死锁相同。

有趣的是两个事务的信息。您可以看到，事务 1 正在更新 Code="AUS "的国家的人口：

#+begin_src sql
  update world.contry set Population=Population*1.1 where Code='AUS';
#+end_src

事务2尝试插入一个新的城市

#+begin_src sql
  INSERT INTO world.city VALUES (4080, 'Darwin', 'AUS', 'Northern Territory', 146000)
#+end_src

这是一个涉及多个表的死锁案例。虽然两个查询在不同的表上工作，但这本身并不能证明涉及了更多的查询，因为外键可以触发
一个查询在两个表上加锁。但在本例中，Code 列是国家表的主键，唯一涉及的外键是从城市表的 CountryCode 列到国家表
的 Code 列（读者可以使用世界示例数据库来显示这一点）。因此，两个查询本身不太可能陷入僵局。

#+begin_comment
  死锁输出来自 MysQL 8.0.18，该版本在输出中增加了额外信息。但是，如果您仍在使用较早的版本，升级将使调查死锁变
  得更容易。
#+end_comment

接下来要观察的是正在等待哪些锁。事务 1 等待country表主键的独占锁：

[[./images/Zba89p.png]]

主键的值可以在该信息后面的信息中找到。由于 InnoDB 包含了与记录相关的所有信息，因此看起来有点令人不知所措。因为
是主键记录，所以整条记录都包含在内。这对了解记录中的数据很有用，尤其是当主键本身不包含这些信息时，但第一次看到时
可能会感到困惑。国家表的主键是该表的第一列，因此记录信息的第一行就包含了锁所请求的主键值：

[[./images/Or7DDO.png]]

InnoDB 包含十六进制表示的值，但也会尝试将其解码为字符串，所以这里的值显然是 "AUS"，这并不奇怪，因为查询的
WHERE 子句中也有这个值。这并不总是那么明显，所以你应该总是从锁定输出中确认值。从信息中还可以看出，该列在索引中
是按升序排序的。

事务 2 等待城市表 CountryCode 索引上的插入意图锁：

[[./images/3sVApj.png]]

可以看到，锁定请求涉及记录前的空白。在这种情况下，由于 CountryCode 索引是非唯一二级索引，所以锁定信息更简单，
因为 CountryCode 索引中只有两列--CountryCode 列和主键（ID 列）。该索引实际上是（CountryCode，ID），记录
前间隙的值如下：

[[./images/4mWc2L.png]]

可以看到，锁定请求涉及记录前的空白。在这种情况下，由于 CountryCode 索引是非唯一二级索引，所以锁定信息更简单，
因为 CountryCode 索引中只有两列--CountryCode 列和主键（ID 列）。该索引实际上是（CountryCode，ID），记录
前间隙的值如下：

#+begin_src sql
  select *
  from world.city
  where CountryCode='AUT'
  order by CountryCode, ID
  limit 1;
#+end_src

到目前为止一切顺利。既然事务都在等待这些锁，当然可以推断出另一个事务持有锁。在 8.0.18 及更高版本中，InnoDB 包
含了两个事务持有的锁的完整列表；而在早期版本中，InnoDB 只为其中一个事务明确包含了该列表，因此你需要确定事务还执
行了哪些查询。

根据现有信息，您可以做出一些有根据的猜测。例如，INSERT 语句被 CountryCode 索引上的间隙锁阻塞。使用
CountryCode = "AUS "条件的查询就是使用该间隙锁的查询示例。死锁信息还包括拥有事务的两个连接的信息，这可能会对
你有所帮助：

[[./images/QvF1IR.png]]

可以看到这两个连接都是使用 root@localhost 账户进行的。如果确保每个应用程序和角色都有不同的用户，那么账户可能
有助于缩小执行事务的范围。

如果连接仍然存在，您还可以使用性能模式中的 events_statements_history 表查找连接执行的最新查询。这可能不是死
锁所涉及的查询，这取决于连接是否被用于更多查询，但仍可提供有关连接用途的线索。如果连接已不存在，原则上可以在
events_statements_history_long 表中找到这些查询，但需要将 "MySQL 线程 ID"（连接 ID）映射到 Performance
Schema 线程 ID，而要做到这一点并不容易。此外，events_statements_history_long 消费者默认未启用。

在这种特殊情况下，两个连接都仍然存在，除了回滚事务外，它们没有做任何其他事情。清单 22-10 显示了如何查找事务中涉
及的查询。请注意，查询可能会返回比此处显示更多的记录，这取决于您使用的是哪个客户端，以及在连接中执行了哪些其他查
询。

#+begin_src sql
  select sql_text, nesting_event_id,
         nesting_event_type
  from performance_schema.events_statements_history
  where thread_id=ps_thread_id(61)
  order by event_id\G
#+end_src

请注意，连接 ID 62（第二个事务）包含了 MySQL 错误编号，第三行的错误编号设置为 1213--死锁。当遇到错误时，MySQL
Shell 会自动执行 SHOW WARNINGS 语句，也就是第 4 行中的语句。 还请注意，事务 2 的 ROLLBACK 嵌套事件为空，
但事务 1 的 ROLLBACK 嵌套事件却不是空。这是因为死锁触发了整个事务的回滚（所以事务 2 的 ROLLBACK 没有做任何事
情）。

触发死锁的原因是事务 1 首先更新了城市表的人口，然后又更新了国家表的人口。事务 2 首先更新了国家表的人口，然后试
图在城市表中插入一个新城市。这是两个工作流以不同顺序更新记录从而容易造成死锁的典型例子。

总的来说，调查包括两个步骤：

1. 分析 InnoDB 的死锁信息，确定死锁涉及的锁，并尽可能多地获取连接信息。
2. 使用性能模式等其他来源查找有关事务中查询的更多信息。通常需要分析应用程序才能获得查询列表。


*** 解决方案

死锁是最容易解决的锁定情况，因为 InnoDB 会自动选择其中一个事务作为受害者，并将其回滚。在前面讨论的死锁中，事务
2 被选为受害者，这可以从死锁输出中看出：

[[./images/kIrYrn.png]]

这意味着对于事务 1 来说，没有任何事情可做。事务 2 回滚后，事务 1 可以继续并完成其工作。

对于事务 2，InnoDB 已经回滚了整个事务，所以你需要做的就是重试事务。记住要重新执行所有查询，而不是依赖第一次尝试
时返回的值；否则，你可能会使用过时的值。

#+begin_comment
  始终做好处理死锁和锁等待超时的准备。对于死锁或锁等待超时后事务回滚的情况，应重试整个事务。对于只有查询回滚的锁
  等待超时，重试查询可能会增加延迟。
#+end_comment

如果死锁发生的次数相对较少，您就不需要再做什么了。死锁是一个不争的事实，因此不要因为遇到几次死锁而惊慌失措。如果
死锁造成的影响很大，你就需要考虑进行修改，以防止一些死锁的发生。

*** 预防

减少死锁与减少记录锁争用非常相似，但在整个应用程序中以相同的顺序获取锁非常重要。建议再次阅读第 18 章中的 "减少
锁定问题 "一节。减少死锁的要点是减少锁的数量和持有时间，并以相同的顺序获取锁：

+ 通过将大型事务拆分成几个较小的事务，并添加索引以减少锁的数量，从而减少每个事务的工作量。
+ 如果您的应用适合使用 READ COMMITTED 事务隔离级别来减少锁的数量和锁定时间，请考虑使用 READ COMMITTED 事务
  隔离级别。
+ 确保事务的开启时间越短越好。
+ 必要时，通过执行 SELECT ... FOR UPDATE 或 SELECT ... FOR SHARE 查询，以相同顺序访问记录，从而抢先锁定。

关于如何调查锁的讨论到此结束。你可能会遇到与本章讨论的情况不完全相同的锁定情况，但调查问题的技巧是相似的。


** 总结

本章向你展示了如何使用 MySQL 中的可用资源来调查与锁相关的问题。本章包括调查四种不同类型锁问题的示例：刷新锁、元数
据锁、记录锁和死锁。每种问题类型都使用了 MySQL 的不同功能，包括进程列表、性能模式中的锁表和 InnoDB 监控输出。

还有许多其他锁类型也会导致锁等待问题。本章讨论的方法对于调查其他锁类型引起的问题也有很大帮助。归根结底，要成为调查
锁的专家，唯一的方法就是积累经验，但本章的技术提供了一个良好的起点。


* 提高查询性能

在本书的第四部分中，举了几个配置选项影响 MySQL 行为的例子。这些选项包括字符集和校对的选择、如何创建索引统计、优化器
应如何工作等。还有其他一些选项会直接或间接地影响查询的性能。本章将讨论其他章节未涉及的最常用选项，以及配置 MySQL 时
的一些一般注意事项。

本章首先介绍了有关更改配置的一些 "最佳实践"。接下来的章节分别介绍 InnoDB、查询缓冲区和内部临时表。

** 最好的做法

当你开始进行配置更改时，值得牢记一些原则，它们能让你在进行配置更改时更加成功。我们将讨论的最佳实践包括以下内容：

+ 警惕最佳做法。
+ 使用监测来验证效果。
+ 每次更改一个选项。
+ 进行相对较小的渐进式修改。
+ 越少越好。
+ 确保您了解该选项的作用。
+ 考虑副作用。

最佳实践清单的第一条就是要警惕最佳实践，这听起来可能有些拗口。意思是说，当你看到一些建议时，你不应该直接跳过并应用
这些建议。  

没有两个系统是完全相同的，因此，尽管建议总体上可能是好的，但你仍然需要考虑它是否也适用于你的系统。另一个陷阱是查看
适用于旧版本 MySQL 或 8 GiB 内存很大的时候的建议。如果你在 Google 上搜索某些设置，有可能会看到很多年前编写的建
议。同样，一段时间前对你的系统很有效的建议，可能会因为应用工作量的变化而不再有效。最后，即使某项建议能提高系统性能，
也可能会产生副作用，例如可能会丢失已提交的更改，这对你来说是不可接受的。

警惕最佳实践的建议也适用于本书中的建议。请务必考虑这些建议如何适用于您的系统。

那么应该如何进行配置更改呢？应用第 2 章所述的原则。图 23-1 复述了这些步骤。

[[./images/VQyCLe.png]]

首先要确定问题是什么，然后通过监控系统或定时查询或类似方法收集基线。基线也可以是观测值的组合。然后，您就可以确定优化
的目标。确定什么是足够好的目标非常重要，否则将永远无法完成。接下来的步骤是确定原因，并以此找到解决方案。最后，实施解
决方案，并通过与基线进行比较来验证效果。如果问题没有得到解决，或者发现了多个问题，就可以重新开始。

在此过程中，监控非常重要，因为它既可用于定义问题、收集基线，也可用于验证效果。如果跳过这些步骤，就很难知道自己的解
决方案是否奏效，是否对其他查询也产生了影响。

在确定解决方案时，要尽可能减少改动。这既适用于旋转旋钮的配置选项数量，也适用于旋转旋钮的幅度。如果一次更改多个选项，
就无法衡量每次更改的效果。例如，两个更改可能会相互抵消，因此你会认为解决方案不起作用，但其中一个更改确实很有效，而另
一个更改却使情况变得更糟。

配置选项通常也有一个甜蜜点。如果设置太小，选项所代表的功能就无法被充分使用，从而产生重大影响。如果设置过大，功能的
开销就会比好处更多。在这两者之间，你可以获得功能优点的最佳组合，同时又能限制开销。如图 23-2 所示。

[[./images/yKpBgC.png]]

通过循序渐进的微小改变，你就能最大限度地找到这个最佳点。

这与下一点有关：通常情况下，越小越好。举例来说，你有足够的内存来增加每个查询或每个连接的缓冲区，但这并不意味着增加
缓冲区大小就能加快查询速度。当然，这一原则在多大程度上适用取决于选项。就 InnoDB 缓冲池的大小而言，最好有一个相对
较大的缓冲区，因为这有助于减少磁盘 I/O，并从内存中提供数据。关于缓冲池，还需要记住的一点是，内存分配只在 MySQL
启动和动态增加缓冲池大小时才会发生。

但是，对于像连接缓冲区这样的缓冲区，可能会为一次查询分配多次缓冲区，分配缓冲区的开销可能会成为一个问题。这将在 "查
询缓冲区 "一节中进一步讨论。在任何情况下，对于与资源相关的选项，都需要记住分配给某个功能的资源不能用于其他功能。

少而精 "的概念既适用于配置选项的最佳值，也适用于调整选项的数量。在配置文件中设置的选项越多，配置文件就会变得越杂乱，
也就越难概览哪些选项被修改了以及修改的原因。(按功能对设置进行分组也有帮助，例如将所有 InnoDB 设置放在一起）。如果
你习惯于将选项设置为默认值，那么最好还是不要将它们包含在内，因为包含这些选项意味着你将错过默认值的更改，而这些更改
是优化默认配置以反映 MySQL 内部结构更改或标准硬件更改的一部分。

#+begin_comment
  在 MySQL 5.6 及更高版本中，MySQL 配置选项的默认值有了很大的改进。根据开发团队的测试以及 MySQL 支持团队、客户
  和社区成员的反馈，这些更改主要发生在两个主要版本之间。
#+end_comment

建议一开始设置尽可能少的选项。你很可能需要设置 InnoDB 缓冲池、重做日志以及表缓存的大小。你可能还想设置一些路径和
端口，并要求启用一些功能，如全局事务标识符（GTID）或组复制。除此之外，只能根据观察结果进行更改。

#+begin_comment
  一开始，只需设置 innoDB 缓冲池和重做日志的大小、路径和端口，并启用所需的功能、 和端口，并启用所需的功能、 只根
  据观察结果更改配置。
#+end_comment

清单中的最后两点是相关的：确保你了解方案的作用并考虑其副作用。了解选项的作用有助于确定该选项对你的情况是否有用，以
及该选项可能产生的其他影响。例如，考虑一下 sync_binlog 选项。该选项告诉你二进制日志的更新应该多久同步一次到磁盘。
在 MySQL 8 中，默认值是每次提交都同步一次，这对于同步性能较差的磁盘来说会严重影响查询性能。因此，将 sync_binlog
设置为 0（禁用强制同步）可能很诱人；但是，这样做的副作用可以接受吗？如果不同步更改，这些更改就只能保留在内存中，直
到出现其他情况（如内存需要用于其他用途）才会强制同步。这意味着如果 MySQL 崩溃，更改就会丢失，如果有副本，就必须重
建副本。这可以接受吗？

即使可以接受可能丢失二进制日志事件的情况，使用 sync_binlog = 0 还有一个更微妙的副作用。 同步不在事务提交时发生，
并不意味着它永远不会发生。二进制日志的最大大小是 1 GiB（max_binlog_size 选项）加上最后一个事务的大小，旋转二进
制日志意味着旧的二进制日志会被刷新到磁盘上。如今，这通常意味着 MySQL 最终会写入 1 GiB，然后一次性全部刷新。即使在
快速磁盘上，写出一千兆字节的数据也需要相当长的时间。在此期间，MySQL 不能执行任何提交，因此任何发出提交（无论是隐式
还是显式）的连接都会停滞，直到同步完成。这可能会让人大吃一惊，而且停滞的时间可能会很长，足以让最终用户（可能是客户）
感到不安。本书作者曾见过因二进制日志轮转而导致的提交停滞，时间从几秒到半分钟不等。简而言之，sync_binlog = 0 可以
提供最高的总体吞吐量和平均提交延迟，而 sync_binlog = 1 则可以提供最好的数据安全性和最可预测的提交延迟。

** InnoDB概览

鉴于所有涉及表的查询都与 InnoDB 存储引擎交互，因此花些时间查看 InnoDB 参数的配置非常重要。这包括 InnoDB 缓冲池
的大小和重做日志的大小--大多数生产系统都需要调整这两项配置。

在讨论配置选项之前，有必要回顾一下数据如何在表空间和缓冲池之间流动，以及如何通过重做日志系统返回表空间。图 23-3 显
示了这一流程的简单概览。

[[./images/g05pII.png]]

当查询请求数据时，总是从缓冲池中读取数据。如果数据不在缓冲池中，就会从表空间中获取。InnoDB 将缓冲池分为两部分：旧
区块子列表和新区块子列表。数据总是以整页为单位读入旧块子列表的头部（顶部）。如果再次需要同一页面的数据，则将数据移
至新区块子列表。当需要为新页面腾出空间时，两个子列表都会使用最近最少使用（LRU）原则来决定驱逐哪些页面。页面是从旧
区块子列表的缓冲池中驱逐出来的。由于新页面在升级到新区块子列表之前会在旧区块子列表中停留一段时间，这就意味着如果一
个页面被使用过一次，但之后又被闲置，那么它很快就会再次被从缓冲池中驱逐出去。这可以防止备份等罕见的大型扫描污染缓冲
池。

当查询更新发生变化时，这些变化会被写入内存中的日志缓冲区，然后再写入重做日志，重做日志至少由两个文件组成。重做日志
文件以循环方式使用，因此写入从一个文件的开头开始，然后填满文件，文件填满后，InnoDB 继续写入下一个文件。文件大小固
定，文件数量也固定。当日志写到最后一个文件的末尾时，InnoDB 会移回第一个文件的起点。

这些更改也会被写回缓冲池，并标记为 "脏"，直到它们被刷新到表空间文件中。InnoDB 使用双写缓冲区，以确保在崩溃时能检
测到写入是否成功。双写缓冲区是必要的，因为大多数文件系统都不能保证原子写入，因为InnoDB页面的大小大于文件系统块的
大小。在撰写本文时，唯一可以安全禁用双写缓冲区的文件系统是 ZFS。

#+begin_comment
  即使文件系统可以处理 innoDB 页面的原子写入，在实际操作中也可能行不通。启用了日志功能的 eXT4 文件系统就是一个
  例子，理论上没有双写缓冲区应该是安全的，但实际上却可能导致数据损坏。
#+end_comment

** innodb缓冲池

InnoDB 缓冲池是 InnoDB 缓存数据和索引的地方。由于所有数据请求都要经过缓冲池，因此从性能角度来看，缓冲池自然成为
MySQL 非常重要的一部分。这里将讨论缓冲池的几个重要参数。

表 23-1 总结了为优化查询性能而可能需要更改的缓冲池相关配置选项。

| 选项名                        |               默认值 | 注释                                                                                                                      |
|------------------------------+---------------------+--------------------------------------------------------------------------------------------------------------------------|
| innodb_buffer_pool_size      |              128MiB | 缓冲池的总大小                                                                                                              |
| innodb_buffer_pool_instances |          Auto-sized | 缓冲池被分割成多少部分。如果总大小小于 1 GB，默认值为 1，否则为 8。对于 32 位 Windows，如果小于 1.3 GB，默认值为 1；否则，每个实例的大小为 128 MiB。实例的最大数量为 64。 |
| innodb_buffer_pool_dump_pct  |                  25 | 在转储缓冲池内容（备份）时，缓冲池中最近使用的页面所占的百分比。                                                                    |
| innodb_old_blocks_time       |                1000 | 以毫秒为单位，页面在旧区块子列表中停留多长时间后，新读取的页面才会将其提升到新区块子列表。                                              |
| innodb_old_blocks_pct        |                  37 | 旧区块子列表占整个缓冲池的百分比。                                                                                             |
| innodb_io_capacity           |                 200 | 允许 innoDB 在非紧急状态下每秒进行多少次 i/o 操作。                                                                             |
| innodb_io_capacity_max       |                2000 | 允许 innoDB 在紧急状态下每秒进行多少次 i/o 操作。                                                                              |
| innodb_flush_method          | unbuffered or fsync | innoDB 用于将更改写入磁盘的方法。在微软视窗系统中，默认值为unbuffered；在Linux/unix系统中，默认值为fsync。                            |
|                              |                     |                                                                                                                          |

这些选项将在本节的其余部分详细讨论，首先是与缓冲池大小有关的选项。

#+begin_comment
  key_buffer_size选项与缓存innoDB索引无关。这个选项的名字来源于 MySQL 的早期，当时 MyiSAM 存储引擎是主要
  的存储引擎，所以不需要在选项前加上 mysiam。
#+end_comment

*** 缓冲池大小

这些选项中最重要的是缓冲池的大小。默认的 128 MiB 大小很适合在笔记本电脑上建立测试实例，不会耗尽内存（这也是默认
值这么小的原因），但对于生产系统，你很可能需要分配更多内存。增加内存大小，直到工作数据集能容纳进缓冲池，这样做对
你有好处。工作数据集是执行查询所需的数据。通常情况下，这是整个数据集的一个子集，因为有些数据是不活动的，例如，它
与过去的事件有关。

#+begin_comment
  如果缓冲池较大并启用了核心转储，则禁用 innodb_buffer_pool_in_core_file 选项，以避免在发生核心转储时转储
  整个缓冲池。该选项在 MySQL 8.0.14 及更高版本中可用。
#+end_comment

缓冲池命中率是指不从磁盘读取数据，而直接从缓冲池满足页面请求的频率：

$$ Hit Rate=100-(\frac{Innodb_pages_read}{Innodb_buffer_pool_read_requests}) $$

Innodb_pages_read 和 Innodb_buffer_pool_read_requests 这两个变量是状态变量。清单 23-1 展示了一个如何
计算缓冲池命中率的示例。

#+begin_src sql
  select Variable_name, Variable_value
  from sys.metrics
  where Variable_name in ('Innodb_pages_read', 'Innodb_buffer_pool_read_requests')\G
#+end_src

#+begin_src sql
  select 100-(100*1028/141319) as HitRate;
#+end_src

在该示例中，99.3% 的页面请求由缓冲池满足。这个数字涉及所有缓冲池实例。如果要确定给定时间段内的命中率，需要收集该
时间段开始和结束时的状态变量值，并使用它们之间的差值进行计算。你也可以从信息模式中的 INNODB_BUFFER_POOL_STATS
视图或 InnoDB 监控中获取命中率。在这两种情况下，速率都是以每千次请求为单位返回的。清单 23-2 展示了这方面的示
例。要获得有意义的结果，需要确保执行了一些查询，以产生一些缓冲池活动。

#+begin_src sql
  select pool_id, number_pages_read,
         number_pages_get, hit_rate
  from information_schema.buffer_tool_status\G
#+end_src

#+begin_src sql
  show engine innodb status\G
#+end_src

需要注意的是，InnoDB直接返回的命中率是自上次检索缓冲池统计数据以来的时间段，而且是针对每个缓冲池实例的。如果你
想完全控制命中率所针对的时间段，就需要使用状态变量或 INNODB_BUFFER_POOL_STATS 视图中的 NUMBER_PAGES_READ
和 NUMBER_ PAGES_GET 来自行计算。

缓冲池的命中率应尽可能接近 100% 或 1000/1000。尽管如此，在某些情况下这是不可能的，因为数据量不可能装入内存。在
这种情况下，缓冲池命中率仍然很有用，因为它可以让你监控缓冲池在一段时间内的有效性，并与一般查询统计数据进行比较。如
果缓冲池命中率随着查询性能的下降而开始下降，则应考虑增加缓冲池的容量。

*** 缓冲池实例

MySQL 从 5.5 版开始就支持多缓冲池实例。引入该功能的原因是，随着每台主机的 CPU 越来越多，典型的数据库工作负载
中并行运行的查询也越来越多。这导致在访问缓冲池中的数据时出现互斥争用。

减少争用的解决方案之一，是允许将缓冲池拆分成多个实例，每个实例有不同的互斥。实例的数量由
innodb_buffer_pool_instances 选项控制。用 innodb_buffer_pool_size 指定的缓冲池总量会平均分配给各个实
例。除 32 位 Windows 系统外，缓冲池大小小于 1 gigabyte 时，默认情况下只有一个实例。对于较大的缓冲池，默认为
8 个实例。实例的最大数量为 64 个。

对于单线程工作负载来说，最好将所有内存都放在一个缓冲池中。工作负载的并行程度越高，额外的实例越有助于减少争用。增
加缓冲池数量的具体效果取决于并行查询请求存储在不同页面中的数据的程度。如果所有请求都是针对不同页面的，那么增加并
发查询数量的实例数量就会使你受益。如果所有查询都请求同一页面中的数据，则增加实例数量没有任何好处。一般来说，注意
不要让每个缓冲池实例太小。如果没有监控数据证明，则允许每个实例的容量为 1 千兆字节或更大，缓冲池的容量至少为 8
千兆字节。

*** 清空缓冲池

重启数据库的常见问题之一是，在缓存预热之前，缓存暂时无法正常工作。这会导致查询性能非常差，最终用户满意度也很低。
解决这个问题的办法是在关机时在缓冲池中存储最常用页面的列表，并在重启后立即将这些页面读入缓冲池，即使还没有查询请
求这些页面。

这项功能默认是启用的，需要考虑的主要问题是要在转储中包含多少缓冲池。这可以通过 innodb_buffer_ pool_dump_pct
选项来控制。默认值是 25%。页面是从新块子列表的头部读取的，所以包含的是最近使用的页面。

转储只包括应读取页面的引用，因此每页的转储大小大约为 8 字节。如果你有一个 128 GiB 的缓冲池，并使用 16 KiB 的
页面，那么缓冲池中就有 8,388,608 个页面。如果缓冲池转储默认为 25%，则转储量约为 16MB。转储存储在数据目录下的
ib_buffer_pool 文件中。

#+begin_comment
  转储只包括应读取页面的引用，因此每页的转储大小大约为 8 字节。如果你有一个 128 GiB 的缓冲池，并使用 16 KiB
  的页面，那么缓冲池中就有 8,388,608 个页面。如果缓冲池转储默认为 25%，则转储量约为 16MB。转储存储在数据目
  录下的 ib_buffer_pool 文件中。
#+end_comment

如果遇到重启后查询速度变慢的问题，可以考虑增加 innodb_buffer_pool_dump_pct，在转储中包含缓冲池的更大一部分。
增加该选项的主要缺点是，由于会导出更多页面引用，关机时间会更长，ib_buffer_pool 文件也会变大，而且重启后加载页
面的时间也会更长。将页面加载回缓冲池是在后台进行的，但如果包含更多页面，可能需要更长时间才能将所有最重要的页面恢
复到缓冲池中。

*** 旧缓冲子列表

如果数据集大于缓冲池，一个潜在的问题是，一次大型扫描可能会拉入一些数据，而这些数据仅用于该次扫描，之后很长时间都
不会再使用。如果出现这种情况，缓冲池中使用频率较高的数据就有可能被排出，需要这些数据的查询就会受到影响，直到扫描
完成并恢复平衡为止。逻辑备份（如 mysqlpump 和 mysqldump 所做的备份）就是可能引发该问题的工作的很好例子。备份
过程需要扫描所有数据，但直到下一次备份时才会再次需要这些数据。

为了避免这个问题，缓冲池被分成两个子列表：新块子列表和旧块子列表。当从表空间读取页面时，它们首先被 "隔离 "在旧块
子列表中，只有当页面在缓冲池中的时间超过 innodb_ old_blocks_time 毫秒并再次被使用时，才会被移到新块子列表中。
这有助于提高缓冲池扫描的耐受性，因为单表扫描只会从页面中快速连续读取记录，然后就不会再使用该页面了。这样，一旦扫
描完成，InnoDB 就可以自由地删除页面。

innodb_old_blocks_time 的默认值是 1000 毫秒，对于大多数工作负载来说，这个值足以避免扫描污染缓冲池。如果有工
作在扫描后不久（但超过一秒）又返回到相同的行，那么如果不想让后续访问将页面提升到新块子列表，可以考虑增加
innodb_old_blocks_time 的值。

旧块子列表的大小由 innodb_old_blocks_pct 选项设定，该选项指定了用于旧块子列表的缓冲池的百分比。默认值为 37%。
如果缓冲池较大，可能需要减少 innodb_old_blocks_pct，以避免新加载的页面占用过多缓冲池。旧块子列表的最佳大小还
取决于向缓冲池加载瞬时页面的速度。

你可以监控新旧块子列表的使用情况，这与发现命中率的方法类似。清单 23-3 显示了使用 INNODB_BUFFER_POOL_STATS
视图和 InnoDB 监控的输出示例。

#+begin_src sql
  select pages_made_young,
         pages_not_made_young,
         pages_made_young_rate,
         pages_made_not_young_rate,
         young_make_per_thousand_gets
  from information_schema.innodb_buffer_bool_stats\G
#+end_src

#+begin_src sql
  show engine innodb status\G
#+end_src

*** 刷新页

InnoDB 需要平衡将变更合并到表空间文件的工作强度。如果过于懒惰，重做日志最终会满，需要强制刷新，但如果过于努力，
又会影响系统其他部分的性能。毋庸置疑，要想正确计算方程式是非常复杂的。除了在崩溃恢复期间或恢复物理备份（如使用
MySQL Enterprise Backup 创建的备份）之后，合并都是通过将缓冲池中的脏页面冲洗到表空间文件来完成的。

在最近的 MySQL 版本中，只要有足够的重做日志，InnoDB 使用的自适应刷新算法就能很好地实现平衡，因此一般不需要做
太多工作。主要有三个选项需要考虑：两个用于设置系统的 I/O 容量，一个用于设置刷新方法。

I/O 容量的两个选项是 innodb_io_capacity 和 innodb_io_capacity_max。innodb_io_capacity选项用于正常
的刷新，应该设置为InnoDB每秒允许的I/O操作次数。在实际操作中，要知道使用哪个值并不容易。默认值为 200，大致
相当于低端固态硬盘。通常情况下，将容量设置为几千，高端存储就能从中受益。最好一开始就设置一个相对较低的值，如果监
控结果显示刷新进度落后且有空余的 I/O 容量，再增加该值。

#+begin_comment
  innodb_io_capacity和innodb_io_capacity_max选项不仅用于确定innoDB向表空间文件刷新脏页面的速度，
  还包括其他i/o活动，比如合并变更缓冲区的数据。
#+end_comment

innodb_io_capacity_max选项告诉我们，如果刷新进度落后，允许InnoDB采取多大的推送力度。默认值最小为 2000，
最大为 innodb_io_capacity 的两倍。在大多数情况下，默认值都能很好地工作，但如果你使用的是低端磁盘，就应该考虑
将设置值降低到1000以下。如果出现异步刷新（这将与重做日志讨论），且监控显示InnoDB没有使用足够的I/O容量，
则应增加innodb_io_ capacity_max的值。

#+begin_comment
  i/o 容量设置过高会严重影响系统性能。
#+end_comment

刷新脏页的方式有多种，例如使用操作系统的 I/O 缓存或避免使用。这可以通过 innodb_flush_ 方法选项来控制。在微软
视窗系统中，可以在 "未缓冲"（默认值，推荐使用）和 "正常 "两种值之间进行选择。在 Linux 和 Unix 系统上，选择难
度更大，因为它们支持以下值：

+ fsync: 这是默认值。InnoDB 使用 fsync() 系统调用。数据也将缓存在操作系统的 I/O 缓存中。
+ O_DSYNC: InnoDB 在打开重做日志文件（同步写入）时使用 O_SYNC 选项，而在数据文件中使用 fsync。之所以使用
  O_SYNC，而不是 O_DSYNC，是因为 O_DSYNC 已被证明太不安全，所以才使用 O_SYNC。
+ O_DIRECT: 这与 fsync 类似，但绕过了操作系统的 I/O 缓存。它只适用于表空间文件。
+ O_DIRECT_NO_FSYNC: 除了跳过 fsync() 系统调用外，它与 O_DIRECT 相同。由于 EXT4 和 XFS 文件系统中的错
  误，在 MySQL 8.0.14 之前使用这种方法是不安全的，因为在 8.0.14 中实施了针对这些错误的解决方法。如果重做日
  志文件与表空间文件位于不同的文件系统上，则应使用 O_DIRECT 而不是 O_DIRECT_NO_FSYNC。在大多数生产系统中，
  这是最佳选择。

此外，还有一些实验性的冲洗方法只能用于性能测试。这里不涉及这些实验方法。

哪种刷新方法能带来最佳性能是非常复杂的。由于 InnoDB 自己会缓存数据，而且比操作系统做得更好（因为 InnoDB 知道数
据是如何被使用的），所以人们很自然地认为 O_DIRECT 选项中的一种效果最好。通常情况也是如此；然而，生活更加复杂，
在某些情况下，fsync更快。因此，你需要在自己的系统上进行测试，以确定哪种刷新方法最有效。还有一点是，在不重启操作
系统的情况下重启 MySQL 时，如果使用 fsync 冲洗方法，那么 InnoDB 在第一次读取数据时就能从 I/O 缓存中获益。


** redo日志

重做日志用于持久保存已提交的更改，同时提供顺序 I/O，以尽可能提高性能。为了提高性能，在将更改写入日志文件之前，会先
将其写入内存中的日志缓冲区。

然后，后台进程通过双写缓冲区将缓冲池中的更改合并到表空间中。尚未合并到表空间文件的页面不能从缓冲池中删除，因为它们
被视为脏页面。页面被视为脏页面意味着它的内容与表空间中相同页面的内容不同，因此 InnoDB 不允许读取表空间中的页面，
直到更改被合并。

表 23-2 总结了与重做日志相关的配置选项，您很可能需要更改这些选项来优化查询性能。

| 选项名                     | 默认值 | 注释                                                        |
|---------------------------+-------+------------------------------------------------------------|
| innodb_log_buffer_size    | 16MiB | 日志缓冲区的大小，重做日志事件写入磁盘上的重做日志文件前存储在内存中。 |
| innodb_log_file_size      | 48MiB | 每个redo日志文件的大小                                        |
| innodb_log_files_in_group | 2     | 重做日志中文件的数量。必须至少有两个文件。                        |


*** 日志缓冲

日志缓冲区是一个内存缓冲区，InnoDB 使用它来缓冲重做日志事件，然后再将其写入磁盘。这允许事务将更改保留在内存中，
直到缓冲区已满或更改已提交。日志缓冲区的默认大小为 16MB。

如果有大型事务或大量较小的并发事务，建议增大日志缓冲区的大小。你可以使用 innodb_log_buffer_size 选项来设置
日志缓冲区的大小。在 MySQL 8 中（与旧版本不同），可以动态更改日志缓冲区的大小。理想情况下，缓冲区应该足够大，
以便 InnoDB 只需在提交更改时将其写出即可；不过，这当然要与内存的其他用途相权衡。如果单个事务的缓冲区中有大量更
改，也会拖慢提交速度，因为所有数据都必须在提交时写入重做日志，所以这也是超大日志缓冲区需要考虑的另一个问题。

一旦日志缓冲区已满或事务已提交，重做日志事件就会写入重做日志文件。

*** 日志文件

重做日志的大小是固定的，由多个文件（至少两个）组成，每个文件的大小相同。配置重做日志时的主要考虑因素是确保日志足
够大，不会 "满"。实际上，"满 "意味着容量的 75%，因为此时会触发异步刷新。异步刷新会阻塞触发刷新的线程，而其他线
程原则上可以继续工作。实际上，异步刷新非常猛烈，通常会导致系统瘫痪。还有一种同步刷新，在容量达到 90% 时触发，并
阻塞所有线程。

你可以使用 innodb_log_file_size 和 innodb_log_files_in_group 这两个选项来控制日志大小。重做日志的总大
小是这两个值的乘积。 建议将文件大小设置为 1-2 GiB，然后调整文件数量，以获得所需的总大小，至少要有两个文件。不要
让每个重做日志文件变得非常大的原因是，它们会在操作系统的 I/O 缓存中缓冲（即使 innodb_flush_method =
O_DIRECT），文件越大，重做日志就越有可能占用 I/O 缓存中的大量内存。 重做日志的总大小不允许超过 512 GiB，最多
只能有 100 个文件。

#+begin_comment
  重做日志越大，可存储的尚未从缓冲池刷新到表空间的更改就越多。这可能会增加崩溃时的恢复时间和正常关机所需的时间。
#+end_comment

确定重做日志容量的最佳方法是通过监控解决方案来监控重做日志在一段时间内的满载情况。图 23-4 显示了重做日志文件的
I/O 速率和根据检查点滞后测量的重做日志使用情况的示例图。如果要创建类似的东西，就需要执行高强度的写工作日志；
employees数据库在这方面很有用。具体需要多少资源取决于硬件、配置、使用资源的其他进程等。

[[./images/JwezL5.png]]

确保未检查点的重做日志部分不超过 75%。在此示例中，最高峰值出现在重做日志 96 MiB 中的 73 MiB 左右（14:37），
这意味着近 76% 的重做日志被用于脏页面。这意味着在该时间前后有一次异步刷新，这可能会影响到当时正在运行的查询。您
可以使用重做日志文件的 I/O 速率来了解文件系统为重做日志执行 I/O 的压力有多大。

手动检查当前重做日志使用情况的最佳方法是启用 log_lsn_ current 和 log_lsn_last_checkpoint InnoDB 指标，
这样就可以查询当前日志序列号和上次检查点时的日志序列号。检查点滞后占整个重做日志的百分比计算公式为

$$ Log Pct=100*\frac{log_lsn_last_checkpoint-log_lsn_current}{#logfile*\log{file_size}} $$

你可以从information_schema的INNODB_METRICS表或sys.metrics视图中获取当前值。另外，无论是否启用了度
量指标，都可以从 InnoDB 监控程序的 LOG 部分获得日志序列号。  清单 23-4 展示了使用这些资源确定检查点滞后的示
例。

#+begin_src sql
  set global innodb_monitor_enable='log_lsn_current',
      global innodb_monitor_enable='log_lsn_last_checkpoint';
  select *
  from sys.metrics
  where Variable_name in ('log_lsn_current',
                          'log_lsn_last_checkpoint')\G
#+end_src

#+begin_src sql
  select round(
  100*(
  (select count from information_schema.innodb_metrics where name='log_lsn_current')-
  (select count from information_schema.innodb_metrics where name='log_lsn_last_checkpoint')/
  (@@global.innodb_log_file_size)*@@global.innodb_log_files_in_group
  ),2
  ) as logUsagePct;
#+end_src

#+begin_src sql
  show engine innodb status\G
#+end_src

首先启用所需的 InnoDB 指标。启用这些指标的开销很小，所以不启用也没关系。然后从 sys.metrics 视图中查询指标值，
然后使用 INNODB_METRICS 表直接计算滞后值。最后，日志序列号也可以在 InnoDB 监控输出中找到。日志序列号的变化非
常快，因此即使你连续查询，如果有任何工作正在进行，它们也会发生变化。这些值反映了 InnoDB 中以字节为单位的工作量，
因此在任何两个系统上都会有所不同。

** 并行查询执行

自 MySQL 8.0.14 起，InnoDB 对并行执行查询提供了有限的支持。这是通过使用多个读取线程对聚类索引或分区执行扫描来
实现的。在 8.0.17 中，该功能的实现得到了极大的改进。

并行扫描会根据要扫描的索引子树的数量自动进行。你可以通过设置 innodb_parallel_ read_threads 选项，配置 InnoDB
为跨所有连接并行执行而创建的线程的最大数量。这些线程是作为后台线程创建的，只有在需要时才会出现。如果所有并行线程都在
使用中，InnoDB 将恢复为单线程执行任何其他查询，直到线程再次可用。

从 MySQL 8.0.18 起，并行扫描用于 SELECT COUNT(*)（允许多个表），不带任何过滤条件，也用于 CHECK TABLE 执行的
两次扫描中的第二次扫描。

你可以从 performance_schema. threads 表中查找名称为 thread/innodb/parallel_read_ thread 的线程，查看
并行线程的当前使用情况。如果你想试试这个功能，可以使用 MySQL Shell 中的 Python 模式来计算 employees.salaries
表中的行数：

#+begin_src python
  for i in range(100): session.run_sql('select count(*) from employees.salaries')
#+end_src

在 innodb_parallel_ read_threads = 4（默认值）的情况下，performance_schema.threads 的输出示例如下

#+begin_src sql
  select thread_id, type, thread_os_id
  from performance_schema.threads
  where name='thread/innodb/parallel_read_thread';
#+end_src

您可以尝试使用较小的表，如世界数据库中的表，看看后台线程数量的差异。

如果发现大部分时间都在使用所有配置的读取线程，而且有空闲的 CPU，可以考虑增加 innodb_parallel_ read_threads
的值。支持的最大值是 256。切记为单线程查询留出足够的 CPU 资源。

如果你看到了 semaphore 等待，而且对 CPU 的监控显示，在有很多并行读取线程的情况下，CPU 资源出现了竞争，那么你可
以考虑减少 innodb_parallel_read_threads，以减少查询的并行性。

** 查询缓存

MySQL 在执行查询时使用多个缓冲区。这些缓冲区包括存储连接中使用的列值、排序缓冲区等。我们很容易认为这些缓冲区越多
越好，但一般情况并非如此。相反，缓冲区往往越少越好。本节将讨论为什么会这样。

当 MySQL 需要为查询或查询的一部分使用缓冲区时，有几个因素会决定对查询的影响。这些因素包括以下内容：

+ 缓冲区的大小是否足以满足工作需要？
+ 内存是否足够？
+ 分配缓冲区的成本是多少？

如果缓冲区不够大，算法就无法达到最佳性能，因为需要进行更多的迭代，或者必须溢出到磁盘。不过，在某些情况下，缓冲区的
配置值是最小值，而不是最大值。例如，由 join_buffer_size 设置大小的连接缓冲区就是这种情况。缓冲区的最小大小始终
会被分配，如果它的大小不足以容纳连接时单行所需的列，则会根据需要进行扩展。  

关于内存的问题也非常重要。MySQL 崩溃的最常见原因可能是操作系统内存不足，操作系统杀死了 MySQL。对于单个查询而言，
各种缓冲区所需的内存加起来似乎并不多，但如果将所有并发执行的查询相乘，再加上空闲连接和全局分配所需的内存，内存可能
会突然变得比你想象的还要多。这还可能导致交换，而交换是性能的主要杀手。

最后一点对大多数人来说更令人惊讶。分配内存是有成本的，通常需要的内存越多，每字节的成本就越高。例如，Linux 上有各种
阈值，在这些阈值下，分配方法会发生变化。这些阈值取决于 Linux 发行版本，但例如可能是 256 KiB 和 2 MiB。如果超过
其中一个阈值，分配方法就会变得更昂贵。这也是 join_buffer_size、sort_buffer_size 和 read_rnd_buffer_size
选项默认值为 256 KiB 的部分原因。这意味着有时缓冲区稍微小一点更好，因为最佳大小的缓冲区所带来的性能提升不足以补偿
分配更多内存的开销。

缓冲区的分配是需要改进的地方之一，因此在某些情况下，升级可以让您使用更大的缓冲区，而不会出现传统的缺点。例如，在
MySQL 8.0.12 及更高版本中，使用了一种新的排序缓冲区算法。这意味着，在 Linux/unix 和 Windows 上的非并发排序中，
内存是以增量方式分配的，因此从性能角度看，使用较大的 sort_buffer_size 值更安全。不过，您仍然需要考虑允许单个查询
使用多少内存。

结论是，在查询期间分配的缓冲区最好保守一些。保持较小的全局设置（默认值是一个很好的起点），仅在可以证明增加设置后会
有明显改善的查询中增加全局设置。

** 内部临时表

当查询需要存储子查询的结果、合并 UNION 语句的结果等类似内容时，会使用内部临时表。MySQL 8采用了新的TempTable
存储引擎，与以前版本中使用的MEMORY引擎相比，TempTable在内存中保存表时要优越得多，因为它支持可变宽度列（从
8.0.13版开始支持blob和文本列）。此外，TempTable 引擎还支持使用 mmap 将表溢出到磁盘，因此如果表无法放入内存，
就可以避免存储引擎转换。

在 MySQL 8 中，内部临时表主要有两个设置需要考虑：允许 TempTable 引擎使用多少内存，以及如果需要溢出到磁盘时该怎
么办。

使用 temptable_max_ram 选项可以配置内部临时表使用的最大内存量。这是一个全局设置，默认值为 1 GiB。所有需要使用
内部临时表的查询都会共享该内存，因此很容易对总内存使用量设置上限。temptable_max_ram 选项可以动态设置。

如果内存不足，就必须开始在磁盘上存储临时表。存储方式由 8.0.16 版中引入的 temptable_use_mmap 选项控制。默认值
为 ON，这意味着 TempTable 引擎会为磁盘上的数据分配内存映射临时文件的空间。这也是 8.0.16 之前使用的方法。如果将
该值设置为 OFF，则会使用 InnoDB 磁盘上的内部临时表。除非内存映射文件出现问题，否则建议使用默认设置。

您可以使用 memory/temptable/ physical_ram 和 memory/temptable/physical_disk 性能模式事件监控 TempTable
内存使用情况。物理 RAM 事件显示的是 TempTable 引擎内存部分的内存使用情况，而物理磁盘事件显示的是
temptable_use_mmap = ON 时的内存映射部分。清单 23-5 显示了查询两个内存事件内存使用情况的三个示例。

#+begin_src sql
  select *
  from sys.memory_global_by_current_bytes
  where event_name in ('memoey/temptable/physical_ram',
                       'memory/temptable/physical_disk')\G
#+end_src

#+begin_src sql
  select *
  from performance_schema.memory_summary_golbal_by_event_name
  where event_name in ('memory/temptable/physical_ram',
                       'memory/temptable/physical_disk')\G
#+end_src

#+begin_src sql
  select *
  from performance_schema.memory_summary_by_thread_by_event_name
  where event_name in ('memory/temptable/physical_ram',
                       'memory/temptable/physical_disk')
         and count_alloc >0\G
#+end_src

前两个查询请求的是全局使用情况，而第三个查询请求的是每个线程的使用情况。第一个查询使用
sys.memory_global_by_current_bytes 视图，该视图会返回 current_alloc 大于 0 的事件。第二个查询使用的是性
能模式，即使当前没有分配内存，也会始终返回两个事件的数据。 第三个查询会显示哪些线程分配了 TempTable 内存。由于
TempTable 溢出的实现方式，因此无法使用性能模式查看哪些线程在磁盘上有文件。

** 总结

本章介绍了配置 MySQL 实例的一般注意事项以及最常需要调整的选项。在考虑更改配置时，最重要的是想清楚为什么要进行更改、
更改应该解决什么问题、为什么要解决这些问题，并确认更改是否有效。最好的确认方法是每次对一个选项进行小规模的增量更改。

最有可能受益于非默认值的三个选项是：用于设置 InnoDB 缓冲池大小的 innodb_buffer_pool_size，以及用于设置重做日
志大小的 innodb_ log_file_size 和 innodb_log_files_in_group 选项。讨论的其他 InnoDB 选项还控制缓冲池实
例的数量、转储时缓冲池的容量、旧块子列表、如何刷新页面以及重做日志缓冲区的大小。

在MySQL8.0.14及更高版本中，支持并行执行某些查询。你可以使用 innodb_parallel_read_threads 选项限制并行性，
从 8.0.17 开始，该选项指定了 InnoDB 在所有连接中创建的并行线程总数上限。并行执行线程被视为后台线程，只有在并行执
行查询时才会存在。

您的查询也可能受益于更大的每次查询缓冲区，但您必须小心，因为较大的值并不一定比较小的值更好用。建议使用这些缓冲区的
默认值，只有在测试证明有明显好处的查询中才增加缓冲区。

最后，讨论了内部临时表。在 MySQL 8 中，这些表使用 TempTable 引擎，当达到全局最大内存使用量时，它支持溢出到磁盘。
在磁盘上存储内部临时表时，还可以将其转换为 InnoDB。

* 修改查询计划

性能不佳的查询无法按预期运行可能有多种原因。这些原因既包括在模式不佳的情况下查询明显错误，也包括非最佳查询计划或资源
争用等较低层次的原因。本章将讨论一些常见情况和解决方案。

本章首先介绍了用于本章大部分示例的测试数据，并讨论了过度全表扫描的症状。然后介绍查询中的错误如何导致严重的性能问题，以
及为什么即使存在索引也不能总是使用。本章的中间部分介绍了通过改进索引使用或重写复杂查询来改进查询的各种方法。最后一部分
讨论如何使用 SKIP LOCKED 子句实现队列系统，以及如何处理包含多个 OR 条件或包含多个值的 IN () 子句的查询。

** 测试数据

本章主要使用专门为本章示例创建的测试数据。本书 GitHub 仓库中的 chapter_24.sql 文件包含必要的表定义和数据，如果
你想自己尝试这些示例的话。脚本将删除 chapter_24 模式，并创建包含表的模式。你可以使用 MySQL Shell 中的\source
命令或 mysql 命令行客户端中的 SOURCE 命令来执行脚本。例如

你可以使用 MySQL Shell 中的\source 命令或 mysql 命令行客户端中的 SOURCE 命令来执行脚本。例如

#+begin_src sql
  \source chapter_24.sql
  show tables from chapter_24;
#+end_src

在获取 chapter_24.sql 脚本之前，需要先安装世界样本数据库。

#+begin_comment
  由于索引统计量是通过随机潜入索引来确定的，因此每次分析后其值都不会相同。因此，在尝试本章中的示例时，不应期望获得
  完全相同的输出结果。
#+end_comment

** 过度全表扫描的症状

造成最严重性能问题的原因之一是全表扫描，尤其是在涉及连接且全表扫描不在查询块中的第一个表上时。这会给 MySQL 带来大
量工作，甚至影响其他连接。当 MySQL 无法为查询使用索引时，就会发生全表扫描，原因要么是没有过滤条件，要么是现有条件
没有索引。全表扫描的一个副作用是大量数据被拉入缓冲池，可能不会返回应用程序。这会使磁盘 I/O 量急剧增加，从而导致更
多性能问题。

当查询执行了过多的表扫描时，需要注意的症状有：CPU 使用率增加、访问行数增加、索引使用率低、磁盘 I/O 可能增加以及
InnoDB 缓冲池效率降低。

检测过度全表扫描的最佳方法是进行监控。直接方法是在性能模式中查找已标记为使用全表扫描的查询，并比较检查行的比例与返
回或受影响行的数量，详见第 19 章。您还可以查看时间序列图，以发现访问行数过多或 CPU 使用率过高的模式。图 24-1 显
示了在 MySQL 实例上进行全表扫描期间的监控图示例。(如果要模拟类似情况，雇员数据库很有用，因为它有足够大的表，可以
进行一些相对较大的扫描）。

[[./images/pZwKGT.png]]

请注意，在图表的左侧，访问的行、通过全扫描读取行的行访问率和 CPU 使用率都有所增加。另一方面，与访问的行数相比，返
回的行数变化很小（百分比）。尤其是第二张图，它显示了通过索引读取行的比率与全扫描读取行的比率，以及读取行与返回行之
间的比率，这说明了一个问题。

#+begin_comment
  在 MySQl 8.0.18 及更高版本中，哈希连接可用于等连接，因此连接时的全表扫描问题不大。
#+end_comment

最重要的问题是，什么时候 CPU 占用率过高，什么时候访问的行数过多，不幸的是，答案是 "视情况而定"。 如果考虑 CPU 使
用率，那么它实际上只能说明正在进行工作，而对于正在访问的行数和访问速度，这些指标只能说明应用程序正在请求数据。问题
在于，对于应用程序需要回答的问题来说，如果工作量太大，访问的行数太多，就会出现问题。在某些情况下，优化查询可能会增加
而不是减少其中的一些指标，原因很简单，因为经过优化的 MySQL 查询可以做更多的工作。

这就是基线为何如此重要的一个例子。考虑指标的变化通常比查看指标的快照更有意义。同样，将这些指标结合起来查看（如比较
返回行和访问行）比单独查看更有帮助。

** 糟糕的查询

查询性能最差的常见原因之一是查询写错了。这似乎是一个不太可能的原因，但在实践中却比你想象的更容易发生。通常情况下，
问题出在缺少连接或过滤条件，或者引用了错误的表。如果使用框架，例如使用对象关系映射（ORM），框架中的错误也可能是罪
魁祸首。

在极端情况下，缺少过滤条件的查询会使应用程序超时（但不会杀死查询）并重试，因此 MySQL 会不断执行更多同样性能很差的
查询。这反过来又会导致 MySQL 连接耗尽。

另一种可能是，第一个提交的查询开始从磁盘向缓冲池调入数据。随后的每个查询都会越来越快，因为它们可以从缓冲池中读取部
分记录，而当它们读取到尚未从磁盘读取的记录时，速度就会减慢。最后，所有查询副本都将在短时间内完成，并开始向应用程序
返回大量数据，从而使网络达到饱和。网络饱和会导致连接尝试因握手错误而失败（performance_ schema.host_cache 中的
COUNT_HANDSHAKE_ERRORS 列），连接主机最终会被阻塞。

这种情况看似极端，而且在大多数情况下也不会变得那么糟糕。不过，本书作者确实遇到过这种情况，原因是生成查询的框架中存
在一个错误。鉴于现在的 MySQL 实例通常位于云中的虚拟机中，CPU 和网络等可用资源可能有限，一个糟糕的查询最终耗尽资
源的可能性也更大。

以缺少连接条件的查询和查询计划为例，请看清单 24-1，它连接了city表和country表。

#+begin_src sql
  explain
  select ci.CountryCode, ci.ID, ci.Name,
         ci.District, co.Name as Country,
         ci.Population
  from world.city ci
  inner join world.country co\G
#+end_src

#+begin_src sql
  explain analyze
  select ci.CountryCode, ci.ID, ci.Name,
         ci.district, co.Name as Country,
         ci.Population
  from world.city ci
  inner join world.cuontry co\G
#+end_src

请注意，两个表的访问类型都设置为 ALL，并且连接是在一个块嵌套循环中使用连接缓冲区。经常出现类似症状的原因是查询正
确，但查询无法使用索引。EXPLAIN ANALYZE 输出显示在 8.0.18 版中使用了散列连接。它还显示总共返回了近 100 万行！
查询的 Visual Explain 图如图 24-2 所示。

[[./images/JiUXHV.png]]

请注意这里的两个（红色）全表扫描是如何突出的，以及查询成本是如何估计超过 100,000 的。

多次全表扫描、估计返回行数非常多以及估计成本非常高，这些都是你需要寻找的线索。

导致查询性能不佳并出现类似症状的一个原因是，MySQL 无法为筛选器和连接条件使用索引。

** 未使用索引

当查询需要查找表中的记录时，基本上有两种方法：通过全表扫描直接访问记录或通过索引访问记录。如果过滤器的选择性很强，
通过索引访问记录通常比表扫描要快得多。

显然，如果过滤器适用的列上没有索引，MySQL 只能使用表扫描。你可能会发现，即使有索引，也无法使用。造成这种情况的三
个常见原因是：列不是多列索引中的首列；数据类型与比较不匹配；在有索引的列上使用了函数。本节将逐一讨论这些原因。

#+begin_comment
  还有一种情况是，优化器认为某个索引的选择性不够，因此与全表扫描相比，不值得使用该索引。 这种情况将在 "改进索引的
  使用 "部分以及 MySQl 使用错误索引的示例中进行处理。
#+end_comment

*** 不是索引的左前缀

要使用索引，必须使用索引的左前缀。例如，如果一个索引包括三列（a、b、c），那么只有在列 a 上也存在相等条件时，列
b 上的条件才能使用过滤器。

可使用该索引的条件举例如下

#+begin_src sql
  where a=10 and b=20 and c=30
  where a=10 and b=20 and c>10
  where a=10 and b=20
  where a=10 and b>20
  where a=10
#+end_src

无法有效使用索引的一个示例是 WHERE b = 20。在 MySQL 8.0.13 及更高版本中，如果 a 是一个 NOT NULL 列，
MySQL 可以使用跳过扫描范围优化来使用索引。如果 a 允许 NULL 值，则不能使用索引。WHERE c = 20 条件在任何情况
下都不能使用索引。

同样，对于 WHERE a > 10 AND b = 20 条件，索引将仅用于对 a 列进行筛选。当查询只使用索引中的一个列子集时，索
引中列的顺序必须与应用的筛选条件相对应。如果在其中一列上设置了范围条件，请确保该列是索引中最后使用的一列。例如，
请参考清单 24-2 中的表和查询。

#+begin_src sql
  show create table chapter_24.mytable\G
#+end_src

#+begin_src sql
  explain
  select *
  from chapter_24.mytable
  where a>10 and b=20\G
#+end_src

请注意，在 EXPLAIN 输出中，key_len 只有 4 字节，而如果索引同时用于 a 列和 b 列，则应为 9 字节。输出结果还
显示，估计只有 10% 的被检查记录会被包括在内。图 24-3 显示了 Visual Explain 中的同一示例。

[[./images/oTniEm.png]] 

但是，如果更改索引中列的顺序，使列 b 的索引顺序在列 a 之前，那么索引就可以用于这两列的条件。清单 24-3 显示了添
加新索引（b、a、c）后查询计划的变化情况。

#+begin_src sql
  alter table chapter_24.mytable
  add index bac(b,a,c);
#+end_src

#+begin_src sql
  explain
  select * from chapter_24.mytable
  where a>10 and b=20\G
#+end_src

请注意，key_len 列现在返回 9 个字节，过滤列显示表中将包含 100% 的已检查行。同样的情况也反映在 Visual Explain
中，如图 24-4 所示。

[[./images/Dshc5e.png]]

从图中可以看到，要检查的行数从 8000 多行减少到了 160 行，而且 "已用关键字部分 "现在包括了 b 列和 a 列。估计查
询成本也从 1683.84 降至 33.31。

*** 数据类型不匹配

另一件需要注意的事情是，条件的两边使用相同的数据类型，对于字符串，使用相同的校对方式。如果不是这种情况，MySQL
可能无法使用索引。

当查询因数据类型或校对不匹配而无法以最佳方式运行时，一开始可能很难意识到问题出在哪里。查询是正确的，但 MySQL 拒
绝使用您期望的索引。除了查询计划不符合预期外，查询结果也可能是错误的。例如，这种情况可能是铸造造成的：

#+begin_src sql
  select ('a130'=0), ('130a131'=130);
#+end_src

请注意，字符串 "a130 "被认为等于整数 0，这是因为字符串以非数字字符开头，因此被转换为数值 0。同样，字符串
"130a131 "被认为等于整数 130，因为字符串的前导数字部分被转换为整数 130。在 WHERE 子句或连接条件中使用转换时，
也会出现同样的意外匹配。在这种情况下，检查查询的警告有时也有助于发现问题。

如果考虑本章测试模式中的 country 表和 world 表（表定义将在示例讨论中显示），可以看到一个不使用索引的连接示例，
即使用 CountryId 列连接两个表。清单 24-4 显示了查询及其查询计划的示例。

#+begin_src sql
  explain
  select ci.ID, ci.Name, ci.District,
         co.Name as Country, ci.Population
  from chapter_24.city ci
  inner join chapter_24.country co using (CountryId)
  where co.CountryCode='AUS'\G
#+end_src

请注意，ci（城市）表的访问类型是 ALL。由于 co（国家）表是一个常量，因此该查询既不会使用嵌套循环块，也不会使用哈
希连接。这里包含了警告（如果不使用启用了警告的 MySQL Shell，则需要执行 SHOW WARNINGS 来获取警告），因为它们
提供了宝贵的提示，说明为什么不能使用索引，例如 由于字段 "CountryId "的类型或校对转换，无法在索引 "CountryId"
上使用 ref 访问。因此，有一个索引是候选索引，但由于数据类型或校对方式发生了变化而无法使用。图 24-5 显示了使用
Visual Explain 的相同查询计划。

[[./images/P3hHEN.png]]

在这种情况下，你需要使用文本输出来获取所有细节，因为 Visual Explain 并不包含警告。看到类似警告时，请返回检查
表定义。这些定义如清单 24-5 所示。

#+begin_src sql
  create table `chapter_24`.`city`  (
      `ID` int unsigned not null auto_increment,
      `Name` varchar(35) not null default ,
      `CountryCode` char(3) not null default,
      `CountryId` char(3) not null,
      `District` varchar(20) not null default,
      `Population` int unsigned not null default '0',
      primary key (`ID`),
      key `CountryCode` (`CountryCode`),
      key `CountryId` (`CountryId`),
  )engine=InnoDB default charset=utf8mb4 collate=utf8mb4_general_ci;
#+end_src

#+begin_src sql
  create table `chapter_24`.`country`(
     `CountryId` int unsigned not null auto_increment,
     `CountryCode` char(3) not null,
     `Name` varchar(52) not null,
     `Continent` enum('Asia', 'Europe', 'North America', 'Africa', 'Oceania',
     'Antarctica', 'South America') not null default 'Asia',
     `Region` varchar(26) default null,
     primary key(`CountryId`),
     unique index `CountryCode` (`CountryCode`)
  )engine=InnoDB default charset=utf8mb4 collate=utf8mb4_0900_ai_ci;
#+end_src

这里很明显，城市表的 CountryId 列是 char(3) 列，但国家表的 CountryId 是整数。这就是为什么当城市表是第二个
表时，不能使用城市表的 CountryId 列索引。CountryId 列上的索引。

#+begin_comment
  如果连接的方向相反，city表是第一个表，country表是第二个表，那么 city.CountryId 仍然会被转换为整数，而
  country.CountryId 不会改变，因此可以使用 country.CountryId 上的索引。因此可以使用 country.CountryId
  索引。
#+end_comment

还要注意的是，两个表的校对方式不同。city表使用 utf8mb4_general_ci 整理（MySQL 5.7 及更早版本中的默认
utf8mb4 整理），而country表使用 utf8mb4_0900_ai_ci（MySQL 8 中的默认 utf8mb4 整理）。不同的字符集或
校对方式甚至会完全阻止查询的执行：

#+begin_src sql
  select ci.ID, ci.Name, ci.District,
         co.Name as Country, ci.Population
  from chapter_24.city ci
  inner join chapter_24.country co using (CountryCode)
  where co.CountryCode='AUS';
#+end_src

如果在 MySQL 8 中创建表格，并在查询中将其与在早期 MySQL 版本中创建的表格一起使用，则需要注意这一点。在这种情
况下，需要确保所有表使用相同的校对方式。

数据类型不匹配的问题是在过滤器中使用函数的特殊情况，因为 MySQL 会进行隐式转换。一般来说，在过滤器中使用函数会妨
碍索引的使用。

*** 函数依赖

不使用索引的最后一个常见原因是对列应用了函数，例如：WHERE MONTH(birth_date = 7： 在这种情况下，需要重写条件
以避免使用函数，或者添加一个函数式索引。

在可能的情况下，处理因使用函数而无法使用索引的最佳方法是重写查询以避免使用函数。虽然也可以使用函数式索引，但除非
它有助于创建覆盖索引，否则索引会增加开销，而重写则可以避免开销。如清单 24-6 中的示例，使用 chapter_24.person
表查询 1970 年出生的人的详细信息。

#+begin_src sql
  show create table chapter_24.person\G
#+end_src

#+begin_src sql
  explain select * from chapter_24.person
  where year(BirthDate)=1970\G
#+end_src

该查询使用 YEAR() 函数来确定该人的出生年份。另一种方法是查找 1970 年 1 月 1 日至 1971 年 12 月 31 日（包括
这两天）之间出生的每个人，结果是一样的。清单 24-7 显示，在这种情况下使用的是出生日期列上的索引。

#+begin_src sql
  explain select * from chapter_24.person
  where BirthDate between '1970-01-01' and '1970-12-31'\G
#+end_src

这种重写将查询从使用表扫描检查 1000 行减少到仅使用索引范围扫描检查 6 行。在对日期使用函数时，通常可以进行类似的
重写，这样可以有效地提取一系列值。

#+begin_comment
  使用 LIKE 操作符重写日期或日期时间范围条件很有诱惑力，例如： WHERE birthdate LIKE '1970-%'： 这样，
  MySQL 将无法使用查询，因此不建议使用。
#+end_comment

使用函数的条件并不总是可以按刚才演示的方式重写。可能是条件没有映射到单一范围，也可能是查询是由框架或第三方应用程
序生成的，因此无法更改。在这种情况下，可以添加功能索引。

#+begin_comment
  MySQl 8.0.13 及更高版本支持功能索引。如果您使用的是早期版本，建议您升级。如果无法做到这一点，或者还需要函数
  返回的值，可以通过在函数表达式中添加虚拟列并在虚拟列上创建索引来模拟函数式索引。
#+end_comment

举例来说，如果要查询所有在某个月份过生日的人，例如，您想给他们发送生日贺卡。原则上，这可以使用范围来完成，但需要
每年使用一个范围，这既不实用也不高效。相反，您可以使用 MONTH() 函数提取月份的数值（1 月为 1，12 月为 12）。清
单 24-8 展示了如何添加功能索引，该索引可与查询一起使用，以查找 chapter_24.person 表中生日在当前月份的所有人
员。

#+begin_src sql
  alter table chapter_24.person
  add index ((month(BirthDate)));
#+end_src

#+begin_src sql
  explain select *
  from chapter_24.person
  where month(BirthDate)=month(now())\G
#+end_src

添加 MONTH(BirthDate) 功能索引后，查询计划显示使用的索引是 functional_index。

** 提高索引使用

上一节讨论了在连接子句或 WHERE 子句中不使用索引的查询。在某些情况下，虽然使用了索引，但可以改进索引，或者使用其他
索引可以获得更好的性能，或者由于过滤器的复杂性而无法有效使用索引。本节将举例说明如何改进已使用索引的查询。

*** 新增覆盖索引

在某些情况下，当你查询一个表时，过滤是由索引执行的，但你又请求了其他几列，所以 MySQL 需要检索整条记录。在这种情
况下，将这些额外列添加到索引中会更有效率，这样索引就包含了查询所需的所有列。

请看 chapter_24 示例数据库中的城市表：

#+begin_src sql
  create table `city`(
    `ID` int unsigned not null auto_increment,
    `Name` varchar(35) not null default,
    `CountryCode` char(3) not null,
    `District` varchar(20) not null default,
    `Population` int unsigned not null default 'O',
    primary key (`ID`),
    key `CountryCode` (`CountryCode`),
    key `CountryId` (`CountryId`)
  ) ENGINE=InnoDB default charset=utf8mb4 COLLATE=utf8mb4_general_ci;
#+end_src

如果要查找 CountryCode = 'USA' 的所有城市的名称和地区，则可以使用 CountryCode 索引来查找行。如清单 24-9
所示，这种方法非常有效。

#+begin_src sql
  explain
  select Name, District
  from chapter_24.city
  where CountryCode='USA'\G
#+end_src

请注意，索引使用了 12 个字节（每个字符最多 4 个字节宽），Extra 列不包括使用索引。如果创建一个新索引，将
CountryCode 作为第一列，将 District 和 Name 作为其余列，那么索引中就包含了查询所需的所有列。选择 "地区 "
和 "姓名 "的顺序是因为您很可能在筛选器、ORDER BY 和 GROUP BY 子句中将它们与 "国家代码 "一起使用。如果同样
有可能在筛选器中使用这两列，则在索引中选择 Name 优先于 District，因为城市名称的选择性比 District 更强。清
单 24-10 展示了一个示例和新的查询计划。

#+begin_src sql
  alter table chapter_24.city
  alter index CountryCode invisible,
  add index Country_District_Name
           (CountryCode, District, Name);
#+end_src

#+begin_src sql
  explain
  select Name, District
  from chapter_24.city
  where CountryCode='USA'\G
#+end_src

添加新索引时，只覆盖 CountryCode 列的旧索引将不可见。这样做是因为新索引也可以用于旧索引的所有用途，所以通常没
有理由同时保留两个索引。(考虑到仅用于 CountryCode 列的索引小于新索引，某些查询可能会从旧索引中受益。通过使其
不可见，可以在放弃之前确认不需要它）。

键长度仍然返回为 12 字节，因为这是用于过滤的长度。不过，"额外 "列现在包含了 "Using index"，以显示正在使用覆
盖索引。

*** 糟糕的索引

当 MySQL 可以在多个索引之间进行选择时，优化器必须根据两个查询计划的估计成本来决定使用哪个索引。由于索引统计和成
本估计并不精确，MySQL 可能会选择错误的索引。特殊情况是，即使有可能使用索引，优化器也会选择不使用该索引，或者优
化器选择使用表扫描速度更快的索引。无论哪种情况，都需要使用索引提示。

#+begin_comment
  索引提示也可用于影响索引是否用于排序或分组，第 17 章对此进行了讨论。有必要使用索引提示的一个例子是，当查询选
  择使用索引进行排序而不是过滤时，会导致性能低下，反之亦然。
#+end_comment

当怀疑使用了错误的索引时，需要查看 EXPLAIN 输出中的 possible_keys 列，以确定哪些索引是候选索引。清单 24-11
显示了一个查找 2020 年日本年满 20 岁且会说英语的人的信息的示例。(树形格式的 EXPLAIN 输出的一部分已被省略号取
代，以便将大部分行保持在书页宽度范围内，从而提高可读性。

#+begin_src sql
  show create table chapter_24.person\G
#+end_src

#+begin_src sql
  show create table chapter_24.language\G
#+end_src

#+begin_src sql
  explain select PersonId, FirstName,
                 Surname, BrithDate,
          from chapter_24.person
          inner join chapter_24.address using (AddressId)
          inner join chapter_24.language using (LanguageId)
          where BirthDate between '2000-01-01' and '2000-12-31'
                and CountryCode='JPN' and Language='Enginsh'\G
#+end_src

#+begin_src sql
  EXPLAIN FORMAT=TREE
  SELECT PersonId, FirstName,
         Surname, BirthDate
  FROM chapter_24.person
  INNER JOIN chapter_24.address USING (AddressId)
  INNER JOIN chapter_24.language USING (LanguageId)
  WHERE BirthDate BETWEEN '2000-01-01' AND '2000-12-31'
        AND CountryCode = 'JPN' AND Language = 'English'\G
#+end_src

本例中的关键表是person表，该表同时与language表和address表连接。使用 UPDATE 和 FLUSH 语句更新
mysql.innodb_index_stats 表，并刷新该表，使新的索引统计信息生效，从而模拟索引统计信息已过期的情况。

查询可以使用 BirthDate、AddressId 或 LanguageId 索引。三个 WHERE 子句（每个表一个）的有效性可以非常准确
地确定，因为优化器会要求存储引擎计算每个条件的记录数。优化程序的难点在于根据连接条件的有效性确定最佳连接顺序，以
及为每个连接使用哪个索引。根据 EXPLAIN 输出，优化器选择从语言表开始，使用 LanguageId 索引连接人表，最后连接
address表。

如果怀疑查询中使用了错误的索引（在本例中，使用 LanguageId 连接person表并不是最佳选择，只是因为索引统计信
息 "错误 "才选择该索引），首先要做的就是更新索引统计信息。其结果如清单 24-12 所示。

#+begin_src sql
  analyze table chapter_24.person, chapter_24.address, chapter_24.language;
#+end_src

#+begin_src sql
  explain
  select PersonId, FirstName,
          Surname, BirthDate
  from chapter_24.person
  inner join chapter_24.address using (AddressId)
  inner join chapter_24.language using(LanguageId)
  where BirthDate between '2000-01-10' and '2000-12-31'
        and CountryCode='JPN' and Language='English'\G
#+end_src

#+begin_src sql
  explain format=TREE
  select PersonId, FirstName,
         Surname, BirthDate
  from chapter_24.person
  inner join chapter_24.address using (AddressId)
  inner join chapter_24.language using (LanguageId)
  where BirthDate between '2000-01-01' and '2000-12-31'
    and CountryCode='JPN' and Language='English'\G
#+end_src

这极大地改变了查询计划（为便于阅读，仅包含树形格式查询计划的一部分），比较树形格式查询计划最容
易看出这一点。表仍按相同顺序连接，但现在使用哈希连接来连接language表和person表。这样做很有
效，因为language表中只有一条记录，所以对person表进行表扫描并根据出生日期进行过滤是个不错的
选择。在大多数使用了错误索引的情况下，更新索引统计信息就能解决问题，可能的话，在改变 InnoDB 对
表进行索引挖掘的次数之后。

#+begin_comment
  ANALYZE TABLE 会触发被分析表的隐式 FLUSH TABLES。如果有长期运行的查询要使用分析过的表，则在
  长期运行的查询完成之前，不能启动其他需要访问这些表的查询。
#+end_comment

在某些情况下，无法通过更新索引统计来解决性能问题。在这种情况下，可以使用索引提示（IGNORE INDEX、
USE INDEX 和 FORCE INDEX）来影响 MySQL 将使用哪个索引。清单 24-13 显示了一个示例，说明在将索引
统计信息改回过时后，如何对与之前相同的查询执行此操作。

#+begin_src sql
  update mysql.innodb_index_stats
  set stat_value=1000
  where database_name = 'chapter_24'
    and table_name = 'person'
    and index_name = 'LanguageId'
    and stat_name = 'n_diff_pfx01';
#+end_src

#+begin_src sql
  flush table chapter_24.person;
#+end_src

#+begin_src sql
  explain
  select PersonId, FirstName,
         Surname, BirthDate
  from chapter_24.person use index(BirthDate)
  inner join chapter_24.address using (AddressId)
  inner join chapter_24.language using (LanguageId)
  where BirthDate between '2000-01-01' and '2000-12-31'
    and CountryCode='JPN'
    and Language='English'\G
#+end_src

这次为person表添加了 USE INDEX (BirthDate) 索引提示，它提供的查询计划与更新索引统计信息时
相同。请注意，person表的可能键只包括 BirthDate。这种方法的缺点是，如果数据发生变化，优化器无
法灵活更改查询计划，因此 BirthDate 索引不再是最优的。

这个示例在person表上有三个不同的条件（出生日期的日期范围和两个连接条件）。在某些情况下，当
一个表上有多个条件时，对查询进行更广泛的重写是有益的。

*** 重写复杂索引条件

在某些情况下，查询会变得非常复杂，以至于优化程序无法提出一个好的查询计划，因此有必要重写查询。
重写查询可以提供帮助的一个例子是，同一个表上有多个过滤器，索引合并算法无法有效使用。

考虑如下代码

#+begin_src sql
  explain format=tree
  select *
  from chapter_24.person
  where BirthDate < '1930-01-01'
  or AddressId = 3471\G
#+end_src

BirthDate 和 AddressId 列都有索引，但没有跨越两列的索引。一种可能的方法是使用索引合并，如果优化
程序认为收益足够大，就会默认选择这种方法。通常，这是执行查询的首选方式，但对于某些查询（尤其是
比本例更复杂的查询），将两个条件拆分成两个查询并使用联合来合并结果可能会有帮助：

#+begin_src sql
  explain format=tree
  (select *
   from chapter_24.person
   where BirthDate < '1930-01-01'
  ) union distinct (
   select *
   from chapter_24.person
   where AddressId=3417
  )\G
#+end_src

使用 UNION DISTINCT（也是默认联合）可确保满足两个条件的记录不会被包含两次。图 24-6 并排显示了
这两个查询计划。

[[./images/JZlioT.png]]

左边是使用索引合并（sort_union 算法）的原始查询，右边是手动编写的合并。


*** 重写复杂查询

在 MySQL 8 中，优化器增加了几种转换规则，因此可以将查询重写为性能更好的形式。这意味着，随着优
化器知道的变换越来越多，重写复杂查询的需求也在不断减少。例如，在 8.0.l7 版本中，支持将 NOT IN
(子查询)、NOT EXISTS (子查询)、IN (子查询) IS NOT TRUE 和 EXISTS (子查询) IS NOT TRUE 重写为反
连接，这意味着将删除子查询。

尽管如此，考虑如何重写查询仍然是有好处的，这样可以在优化器没有找到最优解或不知道如何自行重写的
情况下帮助优化器。在某些情况下，还可以利用对常用表表达式（CTE，也称为 with 语法）和窗口函数的
支持，使查询更有效、更易读。本节将首先讨论常用表表达式和窗口函数，然后将使用 IN（子查询）重写
查询，最后将其连接并使用两个查询。

公共表表达式和窗口函数

使用常用表表达式和窗口函数的细节超出了本书的范围。本章将包括几个示例，让你了解如何使用这些功能。
要了解总体概况，《MariaDB和MySQL常用表表达式和窗口函数揭秘》是一个很好的起点，作者是Daniel
Bartholomew，由apress出版（www.apress.com/gp/book/9781484231197）。

guilhem Bichot（在 MySQl 中实现常用表格表达式的 MySQl 开发人员）在开发常用表格表达式功能之初，
也写过一篇分四部分的系列博客：https://mysqlserverteam.com/?s=common+table+expressions 。 其他
MySQl 开发人员也写过两篇关于窗口函数的博客：https://mysqlserverteam.com/?s=window+functions。

最新信息的最佳来源是 MySQl 参考手册。https://dev.mysql.com/doc/refman/en/with.html 中介绍了常
用的表格表达式。根据函数是常规函数还是聚合函数，窗口函数分为两部分：
https://dev.mysql.com/doc/refman/en/window-functions. html，其中也包括对窗口函数的一般性讨论
https://dev.mysql.com/doc/refman/en/group-by-functions.html 用于聚合窗口函数。

*** 公共表达式

通过通用表表达式功能，可以在查询开始时定义子查询，并在查询的主要部分将其作为普通表使用。使用通
用表表达式而不是内联子查询有几个优点，包括更高的性能和可读性。性能更好的部分原因是查询中支持多
次引用普通表表达式，而内联子查询只能引用一次。

举例来说，如果对 sakila 数据库进行查询，可以计算出每个负责租赁的员工每月的销售额：

#+begin_src sql
    SELECT
          DATE_FORMAT(r.rental_date, '%Y-%m-01' ) AS FirstOfMonth,
          r.staff_id,
          SUM(p.amount) as SalesAmount
  FROM
          sakila.payment p
  INNER JOIN sakila.rental r
                  USING (rental_id)
  GROUP BY
          FirstOfMonth,
          r.staff_id;
#+end_src

如果想知道各月销售额的变化情况，就需要将一个月的销售额与上个月的销售额进行比较。要做到这一点而
不使用常用的表表达式，您需要将查询结果存储在临时表中，或者将其复制为两个子查询。清单 24-14 显
示了后者的一个示例。

#+begin_src sql
  select current.staff_id,
    year(current.FirstOfMonth) as Year,
    month(current.FirstOfMonth) as Month,
    current.SalesAmount,
    (current.SalesAmount-ifnull(prev.SalesAmount, 0)
    ) as DeltaAmount 
  from (select date_format(r.rental_date, '%Y-%m-01') as FirstOfMonth,
       r.staff_id, 
       sum(p.amount) as SalesAmount 
       from sakila.payment p 
       inner join sakila.rental r using (rental_id)
       group by FirstOfMonth, r.staff_id )current 
  left outer join (
     select date_format(r.rental_date, '%Y-%m-01') as FirstOfMonth,
     r.staff_id,
     sum(p.amount) as SalesAmount
     from sakila.payment p 
     inner join sakila.rental r using (rental_id) 
     group by FirstOfMonth, r.staff_id

  )prev on prev.FirstOfMonth = current.FirstOfMonth-INTERVAL 1 MONTH 
  and prev.staff_id=current.staff_id 
  order by current.staff_id, current.FirstOfMonth;
#+end_src

这很难称得上是最容易阅读和理解的查询。这两个子查询完全相同，与用于查找每位员工每月销售额的子查
询相同。通过比较同一员工当前月份和前几个月的情况，将两个导出表连接起来。最后，按员工和当前月份
对结果进行排序。结果如清单 24-15 所示。

[[./images/bkUQVc.png]]

从结果中可以看出，2005 年 9 月至 2006 年 1 月期间没有销售数据。查询假定这段时间的销售额为 0。在
重写该查询以使用窗口函数时，显示了如何添加缺失的月份。

图 24-7 显示了该版本查询的查询计划。

[[./images/fIuqcw.png]]

查询计划显示，子查询评估了两次；然后在名为 current 的子查询上使用全表扫描执行连接，并在嵌套循
环中使用索引（和自动生成的索引）进行连接，形成按文件排序的结果。

如果使用常用的表表达式，只需定义一次子查询并引用两次即可。这样可以简化查询并使其执行得更好。使
用通用表表达式的查询版本如清单 24-16 所示。

#+begin_src sql
  with monthly_sales as (
   select date_format(r.rental_date, '%Y-%m-01') as FirstOfMonth,
   r.staff_id, 
   sum(p.amount) as SalesAmount 
   from sakila.payment p 
   inner join sakila.rental r using (rental_id)
   group by FirstOfMonth, r.staff_id
  )
  select current.staff_id,
    year(current.FirstOfMonth) as Year,
    month(current.FirstOfMonth) as Month,
    current.SalesAmount,
    (current.SalesAmount - ifnull(prev.SalesAmount, 0) )as DeltaAmount 
  from monthly_sales current 
  left outer join monthly_sales prev on prev.FirstOfMonth=current.FirstOfMonth - INTERVAL 1 MONTH 
  and prev.staff_id=current.staff_id 
  order by current.staff_id, current.FirstOfMonth;
#+end_src

首先使用 WITH 关键字定义通用表表达式，并将其命名为 monthly_sales。这样，查询主要部分中的表列表
就可以直接引用 monthly_sales。查询的执行时间约为原始查询的一半。这样做的另一个好处是，如果业务
逻辑发生变化，只需在一个地方进行更新，从而减少了查询中出现错误的可能性。图 24-8 显示了使用通用
表表达式的查询版本的查询计划。

[[./images/ikXMlc.png]]

查询计划显示，子查询只执行一次，然后作为常规表重复使用。否则，查询计划将保持不变。您也可以使用
窗口函数来解决这个问题。

*** 窗口函数

窗口函数允许您定义一个框架，其中窗口函数返回的值取决于框架中的其他行。您可以用它来生成行数和总
数的百分比，将某一行与上一行或下一行进行比较，等等。这里将探讨之前查找每月销售数字并与上个月进
行比较的示例。

您可以使用 LAG() 窗口函数获取前一行中某列的值。清单 24-17 展示了如何使用 LAG() 窗口函数重写每
月销售额查询，以及如何添加没有销售额的月份。

#+begin_src sql

  with recursive 
  month as 
  (select min(date_format(rental_date, '%Y-%m-01')) as FirstOfMonth,
    max(date_format(rental_date, '%Y-%m-01')) as LastMonth 
    from sakila.rental 
    union 
    select FirstOfMonth+INTERVAL 1 MONTH, LastMonth from month 
    where FirstOfMonth < LastMonth 
    ),
    staff_member as (select staff_id from sakila.staff),
    monthly_sales as (
    select month.FirstOfMonth,
       s.staff_id,
       ifnull(sum(p.amount), 0) as SalesAmount 
       from month 
       cross join staff_member s 
       left outer join sakila.rental r on r.rental_date>=month.FirstOfMonth 
       and r.rental_date<month.FirstOfMonth+INTERVAL 1 MONTH 
       and r.staff_id=s.staff_id 
       left outer join sakila.payment p using (rental_id)
       group by FirstOfMonth, s.staff_id
    )
    select staff_id, year(FirstOfMonth) as Year,
          month(FirstOfMonth) as Month,
          SalesAmount,
          (SalesAmount - LAG(SalesAmount, 1, 0) over w_month) as DeltaAmount 
         from monthly_sales 
         window w_month as (order by staff_id, FirstOfMonth)
         order by staff_id, FirstOfMonth;
#+end_src

这个查询起初看起来相当复杂，原因是前两个普通表表达式用于在有租金数据的第一个月和最后一个 月之
间添加每个月的销售数据。月份表和 staff_member 表之间的交叉乘积（请注意，为了明确交叉连接的目的，
使用了明确的 CROSS JOIN）被用作 monthly_sales 表的基础，并在外连接了租金表和付款表。

现在主查询变得简单了，因为所需的所有信息都可以在 monthly_sales 表中找到。通过按 staff_ id 和
FirstOfMonth 排序销售数据，定义了一个窗口，并在该窗口上使用了 LAG() 窗口函数。清单 24-18 显示
了结果。

[[./images/OexnQC.png]]


#+begin_comment
  窗口并不要求排序数据的值必须按顺序排列。如果省略月份和 staff_member 表达式，2006 年 2 月的滞
  后值就会变成 2005 年 8 月的滞后值。这很可能是您想要的结果，但与清单 24-14 中原始查询的结果不
  同。读者可以尝试修改查询，看看有什么不同。
#+end_comment

*** 用连接重写子查询

如果有子查询，可以选择将子查询改为连接查询。在可能的情况下，优化器通常会自行执行这种重写，但偶
尔也需要优化器的帮助。

例如，请看下面的查询：

#+begin_src sql
  SELECT * FROM chapter_24.person
  WHERE AddressId IN ( SELECT AddressId
                       FROM chapter_24.address
                       WHERE CountryCode = 'AUS' AND District = 'Queensland');
#+end_src

此查询可查找居住在澳大利亚昆士兰州的所有人员。也可以写成人表和地址表之间的连接：

#+begin_src sql
  SELECT person.*
  FROM chapter_24.person
  INNER JOIN chapter_24.address USING (AddressId)
  WHERE CountryCode = 'AUS' AND District = 'Queensland';
#+end_src

事实上，MySQL 也会进行这样的重写（只不过优化器会将地址表放在第一位，因为过滤器就在那里）。这是
一个半连接优化的例子。如果遇到优化器无法重写查询的情况，可以考虑这样的重写。通常情况下，查询越
接近于只由连接组成，查询的性能就越好。不过，查询调整的过程比这更复杂，有时反其道而行之会提高查
询性能。经验就是不断测试。

另一种方法是将查询分成几个部分，然后分步执行。

*** 将查询分成几个部分

最后一种方法是将查询分成两个或多个部分。由于 MySQL 8 支持常用的表表达式和窗口函数，这种类型的
重写几乎不像旧版本的 MySQL 那样频繁。不过，记住这一点还是很有用的。

#+begin_comment
  不要低估将复杂查询拆分成两个或更多简单查询并逐步生成查询结果的能力。
#+end_comment

举例来说，与前面讨论的查询相同，您要查找所有居住在澳大利亚昆士兰州的人。您可以将子查询作为自己
的查询来执行，然后将结果放回 IN() 操作符中。这种重写在应用程序中效果最好，因为应用程序可以通过
编程生成下一个查询。为简单起见，本讨论将只显示所需的 SQL。清单 24-19 显示了两个查询。

#+begin_src sql
  set session transaction_isolation='REPEATABLE-READ';

  start transaction;

  select AddressId from chapter_24.address
  where CountryCode='AUS' and District='Queuesland';

  select * from chapter_24.person
  where AddressId in (132, 136, 142, 143)\G

  commit;
#+end_src

查询是使用具有 REPEATABLE-READ 事务隔离级别的事务执行的，这意味着两个 SELECT 查询将使用相同的
读视图，因此对应的时间点与作为一个查询执行的时间点相同。对于像这样简单的查询，使用多个查询不
会有什么好处；但对于真正复杂的查询，将部分查询（可能包括一些连接）拆分出来可能会有好处。将查
询拆分成若干部分还有一个好处，那就是在某些情况下可以提高缓存效率。在本例中，如果有其他查询使用
相同的子查询来查找昆士兰州的地址，那么缓存可以让您将结果重复用于多种用途。

*** 队列系统: SKIP LOCK

与数据库有关的一项常见任务是处理存储在队列中的任务列表。处理商店订单就是一个例子。重要的是，所
有任务都要处理，而且只处理一次，但由哪个应用程序线程处理每个任务并不重要。SKIP LOCKED 子句非常
适合这种情况。

请看清单 24-20 中定义的表 jobqueue。

#+begin_src sql
  show create table chapter_24.jobqueue\G
#+end_src

当 HandledDate 为 NULL 时，该任务尚未被处理，可以被抓取。如果你的应用程序被设置为获取最早的未
处理任务，并且你想依靠 InnoDB 行锁来防止两个线程获取相同的任务，那么你可以使用 SELECT ... FOR
UPDATE 等语句（在现实世界中，该语句是更大事务的一部分）：

#+begin_src sql
  select JobId
  from chapter_24.jobqueue
  where HandleDate is null
  order by SubmitDate
  limit 1 for update;
#+end_src

这对第一个请求很有效，但下一个请求会阻塞，直到锁等待超时或第一个任务已处理完毕，因此任务处理被
序列化。诀窍在于确保在过滤和排序的列上有索引，然后使用 SKIP LOCKED 子句。这样，第二个连接就会
简单地跳过锁定的行，找到满足搜索条件的第一条非锁定行。清单 24-21 显示了两个连接分别从队列中获
取作业的示例。

#+begin_src sql
  start transaction;
  select JobId
  from chapter_24.jobqueue
  where HandleDate is null
  order by SubmitDate
  limit 1 for update skip locked;
#+end_src

#+begin_src sql
  start transaction;
  select JobId
  from chapter_24.jobqueue
  where HandleDate is null
  order by SubmitDate
  limit 1 for update skip locked;
#+end_src

现在，两个连接可以同时获取任务并处理它们。一旦任务完成，就可以设置处理日期，并将任务标记为已完
成。与连接设置锁列相比，这种方法的优势在于，如果连接因故失败，锁会自动释放。

您可以使用性能模式中的 data_locks 表查看哪个连接拥有每个锁（锁的顺序取决于线程 ID，您的线程 ID
可能不同）：

#+begin_src sql
  select THREAD_ID, INDEX_NAME, LOCK_DATA
  from performance_schema.data_locks
  where OBJECT_SCHEMA='chapter_24' and OBJECT_NAME='jobqueue'
     and LOCK_TYPE='RECORD'
  order by THREAD_ID, EVENT_ID;
#+end_src

十六进制值是提交日期列的编码日期时间值。从输出中可以看出，正如 SELECT 查询返回的 JobId 值所预
期的那样，每个连接都在二级索引和主键中各持有一个记录锁。

*** 存在多个in和or条件

在性能方面可能引起混乱的一种查询类型是包含许多范围条件的查询。当有很多 OR 条件或 IN () 操作符
有很多值时，这通常会成为一个问题。在某些情况下，对条件稍作改动就可能完全改变查询计划。

当优化器在索引列上遇到范围条件时，有两种选择：一是假设索引中的所有值出现的频率相同，二是要求
存储引擎进行索引挖掘，以确定每个范围的频率。前者最便宜，但后者要精确得多。要决定使用哪种方法，
可以使用 eq_range_index_dive_limit 选项（默认值为 200）。如果存在 eq_range_ index_dive_limit
或更多范围，优化器将只查看索引的卡入度，并假设所有值都以相同频率出现。如果范围较少，则会询问存
储引擎每个范围的情况。

当每个值出现的频率相同的假设不成立时，就会出现性能问题。在这种情况下，当通过
eq_ range_index_dive_limit 设置的阈值时，符合条件的估计记录数可能会突然发生重大变化，导致查询
计划完全不同。(当 IN () 操作符中包含很多值时，真正重要的是与所包含值相匹配的记录的平均数量接近
于从索引统计中获得的估计值。因此，列表中的值越多，就越有可能包含具有代表性的样本）。

清单 24-22 显示了付款表的一个示例，其中有一列 ContactId 和一个索引。大部分记录的 ContactId 设
置为 NULL，索引的卡片性为 21。

#+begin_src sql
  show create table chapter_24.payment\G

  select count(ContractId), count(*)
  from chapter_24.payment;

  select CARDINALITY
  from information_schema.STATISTICS
  where TABLE_SCHEMA='chapter_24'
     and TABLE_NAME='payment'
     and INDEX_NAME='ContractId';

  set session eq_range_index_dive_limit=5;

  explain select *
          from chapter_24.payment
          where ContractId in (1,2,3,4)\G
#+end_src

在示例中，eq_range_index_dive_limit 被设置为 5，以避免指定一个很长的值列表。有了 4 个值，优化
器就会请求这 4 个值中每个值的统计信息，估计行数为 4。 但是，如果将值列表变长，情况就会开始发生
变化：

#+begin_src sql
  explain select *
  from chapter_24.payment
  where ContractId in (1,2,3,4,5)\G
#+end_src

突然间，估计有 4785 条记录匹配，而不是真正匹配的 5 条记录。索引仍在使用，但如果带有此条件的支付
表涉及到连接，那么优化器很可能会选择一个非最佳连接顺序。如果让值列表变得更长，优化程序就会完全
停止使用索引，并进行全表扫描，因为它认为索引的作用非常糟糕：

#+begin_src sql
  explain
  select *
  from chapter_24.payment
  where ContractId in (1,2,3,4,5,6,7)\G
#+end_src

该查询只返回 7 条记录，因此索引的选择性很高。那么，如何才能提高优化器的理解能力呢？根据估算不
准确的具体原因，可以采取多种措施。对于这个特殊问题，你有以下选择：

+ 增大eq_range_index_dive_limit的值
+ 修改innodb_stats_method选项
+ 强制MySQL使用索引


最简单的解决办法是增加 eq_range_index_dive_limit 的值。默认值是 200，这是一个很好的起点。如果
有一个候选查询，可以使用不同的 eq_range_index_dive_limit 值进行测试，以确定索引潜入所增加的成
本是否值得为获得更好的行估计值而节省的费用。测试查询的 eq_range_index_dive_limit 新值的好方法
是在 SET_VAR() 优化器提示中设置该值：

#+begin_src sql
  select /*+ SET_VAR(eq_range_index_dive_limit=8) */ *
  from chapter_24.payment
  where ContractId in (1,2,3,4,5,6,7);
#+end_src

在这种情况下，依赖cardinality会导致糟糕的行估计，原因是几乎所有行的ContactId都被设置为
NULL。在默认情况下，InnoDB 会认为所有索引值为 NULL 的记录都具有相同的值。这就是为什么在这个例
子中，cardinality只有21。如果把 innodb_stats_method 改为 nulls_ ignored，那么就会如清单
24-23 所示，只根据非 NULL 值来计算中心性。

  #+begin_src sql
    set global innodb_stats_method=nulls_ignored;

    analyze table chapter_24.payment;

    select CARDINALITY
    from information_schema.STATISTICS
    where TABLE_SCHEMA='chapter_24'
         and TABLE_NAME='payment'
         and INDEX_NAME='ContractId';

    explain
    select *
    from chapter_24.payment
    where ContractId in (1,2,3,4,5,6,7)\G
  #+end_src

这种方法的最大问题是 innodb_stats_method 只能全局设置，因此会影响所有表，而且可能对其他查询产
生负面影响。在这个例子中，把 innodb_stats_method 设回默认值，然后重新计算索引统计：

#+begin_src sql
  set global innodb_stats_method=DEFAULT;

  select @@global.innodb_stats_method\G

  analyze table chapter_24.payment;

#+end_src

最后一个选项是使用索引提示强制 MySQL 使用索引。您需要使用 FORCE INDEX 变体，如清单 24-24 所示。

#+begin_src sql
  explain
  select *
  from chapter_24.payment force index (ContractId)
  where ContractId in (1,2,3,4,5,6,7)\G
#+end_src

这将使查询的执行速度与更准确的统计数据一样快。但是，如果付款表是具有相同 WHERE 子句的连接的一
部分，那么估计的行数仍有偏差（估计行数为 6699 行，而实际行数为 7 行），因此查询计划仍可能出错

** 总结

本章举例说明了提高查询性能的技巧。第一个主题是了解过度全表扫描的症状，然后了解全表扫描的两个主
要原因：查询错误和无法使用索引。无法使用索引的典型原因是所使用的列没有形成索引的左前缀、数据类
型不匹配或在列上使用了函数。

还有一种情况是，虽然使用了索引，但可以改进索引的使用。这可能是为了转换索引以覆盖查询所需的所有
列，也可能是使用了错误的索引，或者是重写带有复杂条件的查询可以改进查询计划。

它对重写复杂查询也很有用。MySQL 8支持常用的表表达式和窗口函数，可用于简化查询，并可能使其执行
得更好。在其他情况下，进行一些优化器通常会做的重写，或将查询拆分成多个部分，也会有所帮助。

最后，讨论了两种常见情况。第一种情况是在队列中使用 SKIP LOCKED 子句来高效访问第一条非锁定记录。
第二种情况是有一个长长的 OR 条件列表或一个有很多值的 IN () 操作符，当范围的数量达到
eq_range_index_dive_limit 选项设置的数量时，查询计划会发生惊人的变化。

* DDL和批量数据导入

有时，有必要执行模式更改或向表中导入大量数据。这可能是为了适应新功能、恢复备份、导入第三方进程生成
的数据或类似情况。原始磁盘写入性能自然非常重要，但在 MySQL 方面也有几种方法可以提高这些操作的性能。

#+begin_comment
  如果您遇到恢复备份耗时过长的问题，可考虑改用直接复制数据文件（物理备份）的备份方法，如使用 MySQL
  企业备份。物理备份的一大好处是恢复速度比逻辑备份（包含 INSERT 语句或 CSV 文件中的数据）快得多。
#+end_comment

本章首先讨论模式更改，然后讨论有关加载数据的一些一般注意事项。这些注意事项同样适用于一次插入单行的
情况。本章其余部分包括如何通过按主键顺序插入来提高数据加载性能、缓冲池和次索引对性能的影响、配置以
及语句本身的调整。最后，演示了 MySQL Shell 的并行导入功能。

** 模式修改

当你需要对模式进行更改时，存储引擎可能需要做大量的工作，可能需要为表创建一个全新的副本。本节将从
模式更改所支持的算法入手，介绍如何加快这一过程，并介绍配置等其他注意事项。

#+begin_comment
  虽然 OPTIMIZE TABLE 不会对表的模式进行任何修改，但 InnoDB 会将其作为 ALTER TABLE 之后的 ANALYZE
  TABLE 来实现。因此，本节的讨论也适用于 OPTIMIZE TABLE。
#+end_comment

** 算法

MySQL 支持多种 ALTER TABLE 算法，算法决定了如何执行模式更改。有些模式更改可以通过更改表定义 “立
即 ”完成，而有些更改则需要将整个表复制到一个新表中。

按所需工作量排序，这些算法是

+ *INSTANT* 只对表格定义进行更改。虽然更改不是即时的，但速度非常快。INSTANT 算法在 MySQL 8.0.12
  及更高版本中可用。
+ *INPLACE* 一般情况下，更改是在现有表空间文件中进行的（表空间 id 不会改变），但也有一些例外，如
  ALTER TABLE <表名> FORCE（由 OPTIMIZE TABLE 使用），它更像 COPY 算法，但允许并发数据更改。这可
  能是一种相对便宜的操作，但也可能涉及复制所有数据。
+ *COPY* 现有数据被复制到新的表空间文件中。这是影响最大的算法，因为它通常需要更多的锁，导致更多
  的 I/O，耗时更长。

通常，INSTANT 和 INPLACE 算法允许并发数据更改，从而减少对其他连接的影响，而 COPY 算法至少需要一
个读锁。MySQL 会根据请求的更改选择影响最小的算法，但也可以明确请求特定算法。例如，如果你想确保
MySQL 在不支持你所选算法的情况下不进行更改，这就很有用。例如，可以使用 ALGORITHM 关键字指定算法：

#+begin_src sql
  alter table world.city
  add column Council varchar(50), algorithm=INSTANT;
#+end_src

如果无法使用所请求的算法执行更改，语句就会失败，例如出现 ER_ALTER_OPERATION_NOT_SUPPORTED 错误
（错误编号 1845）：

#+begin_src sql
  alter table world.city
  drop column Council, algorithm=INSTANT;
#+end_src

如果可以使用 INSTANT 算法，显然可以获得最佳的 ALTER 表性能。在撰写本文时，允许使用 INSTANT 算法
进行以下操作：

+ 添加新列作为表格的最后一列。
+ 添加生成的虚拟列。
+ 删除已生成的虚拟列。
+ 为现有列设置默认值。
+ 删除现有列的默认值。
+ 使用枚举或集合数据类型更改列允许的值列表。要求是不改变列的存储空间大小。
+ 更改是否为现有索引显式设置索引类型（如 BTREE）。

此外，还有一些需要注意的局限性：

+ 行格式不能是COMPRESSED。
+ 表格不能有全文索引。
+ 不支持临时表。
+ 数据字典中的表不能使用 INSTANT 算法。

#+begin_comment
  例如，如果您需要在现有表格中添加一列，请确保将其添加为最后一列，以便 “即时 ”添加。
#+end_comment

就性能而言，就地更改通常比复制更改更快，但并非总是如此。此外，当模式变更是在线进行（LOCK=NONE）
时，InnoDB必须跟踪在执行模式变更期间所做的变更。这会增加开销，而且在操作结束时应用模式变更期间
所做的变更也需要时间。如果能对表使用共享锁（LOCK=SHARED）或独占锁（LOCK=EXCLUSIVE），一般来说就
能获得比允许并发更改更好的性能。


** 其他考虑

由于就地或复制 ALTER TABLE 所做的工作非常耗费磁盘，因此对性能影响最大的就是磁盘的速度以及在模式
变更期间有多少其他写入活动。这意味着，从性能角度看，最好选择在实例和主机上几乎没有其他写入活动时
执行需要复制或移动大量数据的模式变更。这包括备份，因为备份本身的 I/O 密度非常高。

#+begin_comment
  最简单的方法是使用 sys.session 视图，然后查看进度列，该列显示了以总工作量百分比表示的大致进度。
#+end_comment

如果 ALTER TABLE 包括创建或重建二级索引（这包括 OPTIMIZE TABLE 和其他重建表的语句），那么可以使用
innodb_sort_buffer_size 选项来指定每个排序缓冲区可以使用多少内存。需要注意的是，一个 ALTER TABLE
会创建多个缓冲区，因此要小心不要把值设得太大。默认值为 1MB，最大允许值为 64MB。在某些情况下，较
大的缓冲区可能会提高性能。

如果 ALTER TABLE 包括创建或重建二级索引（这包括 OPTIMIZE TABLE 和其他重建表的语句），那么可以使用
innodb_sort_buffer_size 选项来指定每个排序缓冲区可以使用多少内存。需要注意的是，一个 ALTER TABLE
会创建多个缓冲区，因此要小心不要把值设得太大。默认值为 1MB，最大允许值为 64MB。在某些情况下，较
大的缓冲区可能会提高性能。

创建全文索引时，可以使用 innodb_ft_sort_pll_degree 选项来指定 InnoDB 将使用多少线程来构建搜索索
引。默认值是2，支持的值在1到32之间。如果要在大型表上创建全文索引，增加
innodb_ft_sort_pll_degree的值可能会有好处。

** 删除或清理表

似乎没有必要考虑删除表的性能优化问题。似乎只需删除表空间文件并移除对表的引用即可。实际上，事情
并非如此简单。

丢弃或截断表时的主要问题是缓冲池中对表数据的所有引用。特别是自适应散列索引会造成问题。因此，在
丢弃或截断大型表时，可以通过在操作期间禁用自适应散列索引等方法大大提高性能：

#+begin_src sql
  set global innodb_adaptive_hash_index=OFF;

  drop table <name of large table>;

  set global innodb_adaptive_hash_index=ON;
#+end_src

禁用自适应散列索引会使受益于散列索引的查询运行速度变慢，但对于大小为几百千兆字节或更大的表来说，
禁用自适应散列索引带来的相对较小的运行速度降低，通常要比由于删除对正在被丢弃或截断的表的引用所产
生的开销而导致的潜在停滞要好。

** 通用数据加载考虑

在讨论如何提高批量插入的性能之前，我们不妨先进行一个小测试并讨论其结果。在测试中，有 200,000 行
被插入到两个表中。其中一个表的主键是自动递增计数器，另一个表的主键是随机整数。两个表的行大小相同。

数据加载完成后，可以使用清单 25-1 中的脚本来确定表空间文件中每个页面的年龄（以日志序列号 (LSN)
度量）。日志序列号越高，说明页面最近被修改过。这个脚本受到 Jeremy Cole1 的 innodb_ruby 的启发，
生成的映射图与 innodb_ruby space-lsn-age-illustrate-svg 命令类似。不过，innodb_ruby 还不支持
MySQL 8，因此我们开发了一个单独的 Python 程序。该程序已在 Python 2.7 (Linux) 和 3.6 (Linux 和
Microsoft Windows) 下进行了测试。本书 GitHub 代码库中的 listing_25_1.py 文件也提供了该程序。

#+begin_src python
  
#+end_src

在 FIL_PAGE_OFFSET、FIL_PAGE_LSN 和 FIL_PAGE_TYPE 常量为每个页面定义的位置（以字节为单位）提取
页码、日志序列号和页面类型。如果页面类型具有 FIL_PAGE_TYPE_ALLOCATED 常量的值，则表示尚未使用，
因此可以跳过 - 这些页面在日志序列号映射表中被涂成黑色。

#+begin_comment
  如果你想了解页眉中的信息，源代码中的 storage/innobase/include/fil0types.h 文件
  （https://github.com/mysql/mysql-server/blob/8.0/storage/innobase/include/ fil0types.h）和
  MySQL 内部手册（https://dev.mysql.com/doc/internals/en/ innodb-fil-header.html）中对 fil 页眉
  的描述都是很好的起点。
#+end_comment

通过使用 --help 参数调用程序，可以获得使用帮助。唯一需要的参数是要分析的表空间文件的路径。除非你
将 innodb_page_size 选项设置为 16384 字节以外的其他值，否则你只需要使用可选参数的默认值，除非你
想更改生成映射的尺寸和大小。

#+begin_src sql
  create schema chapter_25;

  create table chapter_25.table_autoinc(
    id bigint unsigned not null auto_increment,
    val varchar(36),
    primary key(id)
  );
#+end_src

#+begin_src python
  for i in range(40):
      session.start_transaction()
      for j in range(5000):
          session.run_sql("INSERT INTO chapter_25.table_autoinc (val) VALUES (UUID())")
          session.commit()
#+end_src

该表有一个 bigint 主键和一个用 UUID 填充的 varchar(36)，用于创建一些随机数据。MySQL Shell 的
Python 语言模式用于插入数据。session.run_sql() 方法在 8.0.17 及更高版本中可用。最后，可以执行
listing_25_1.py 脚本，生成可缩放矢量图形 (SVG) 格式的表空间年龄图：

#+begin_src shell
  python listing_25_1.py <path to datadir>\chapter_25\table_autoinc. ibd > table_autoinc.svg
#+end_src

程序输出显示表空间中有 880 个页面，文件末尾可能还有一些未使用的页面。

[[./images/4GRxh6.png]]

在图中，左上方代表表空间的第一页。从图中从左到右、从上到下，页面在表空间文件中的位置越来越靠前，
右下方代表最后一页。从图中可以看出，除第一页外，其他页龄的变化规律与图底部的 LSN 页龄刻度相同。
这意味着页面的年龄会随着表空间的增大而变小。前几页是个例外，因为它们包括表空间的页眉。

这种模式表明，数据是按顺序插入表空间的，因此表空间尽可能紧凑。这也使得如果查询从多个页面读取的数
据是按逻辑顺序排列的，那么它们在表空间文件中也是按物理顺序排列的。

那么，如果以随机顺序插入，情况会怎样呢？随机顺序插入的一个常见例子是使用 UUID 作为主键，但为了确
保两个表的行大小相同，我们使用随机整数来代替。清单 25-3 展示了如何填充 table_random 表。

#+begin_src sql
  create table chapter_25.table_random(
    id bigint unsigned not null,
    val varchar(36),
    primary key(id)
  );
#+end_src

#+begin_src python
  import random
  import math
  maxint=math.pow(2,64)-1
  random.seed(42)

  for i in range(40):
      session.start_transaction()
      for j in range(5000):
          session.run_sql("INSERT INTO chapter_25.table_random \
                         VALUE ({0}, UUID())".format(random.randint(0, maxint)))
          session.commit()
#+end_src

Python 随机模块用于生成 64 位随机无符号整数。种子是明确设置的，因为（通过实验）得知，42 的种子
可以连续生成 200,000 个不同的数字，因此不会出现重复键错误。表格填充完成后，执行 listing_25_1.py
脚本：

#+begin_src shell
  python listing_25_1.py <path to datadir>\chapter_25\table_random.ibd > table_random.svg
#+end_src

listing_25_1.py 脚本的输出结果显示，该表空间中有 1345 个页面。生成的页龄图如图 25-2 所示。

[[./images/tyt1BY.png]]

这次的日志序列号年龄模式完全不同。除了未使用的页面外，所有页面的年龄颜色都与最新日志序列号的颜色
一致。这意味着所有有数据的页面最近一次更新的时间大致相同，换句话说，它们都是在批量加载结束前被写
入的。与自动递增主键表中使用的 880 个页面相比，包含数据的页面数量为 1345 个。页数增加了 50%以上。

以随机顺序插入数据会导致相同数量的数据需要更多页面，这是因为 InnoDB 会在插入数据时填充页面。当数
据按照主键顺序依次插入时，这意味着下一条记录总是与前一条记录相继，因此当记录按照主键顺序排列时，
这种方法效果很好。如图 25-3 所示。

[[./images/VApF9P.png]]

图中显示了两行新插入的数据。id = 1005 的行刚好可以插入第 N 页，因此当插入 id = 1006 的行时，它
将被插入下一页。在这种情况下，一切都非常紧凑。

在这种情况下，id = 3500 的记录被插入，但在逻辑上属于第 N 页的地方已经没有空间了。因此，N 页被分
成 N 页和 N+1 页，每页大约有一半的数据。

页面分割有两个直接后果。首先，以前占用一页的数据现在要占用两页，这就是为什么按随机顺序插入的数据
最终要多占用 50%的页面，这也意味着同样的数据在缓冲池中需要更多的空间。增加页面的一个重要副作用
是，B 树索引最终会有更多的叶页，树中可能会有更多的层级，鉴于树中的每一级都意味着访问页面时需要额
外的寻道，这会导致额外的 I/O。

其次，以前一起读入内存的记录现在被分到了位于磁盘不同位置的两个页面中。当 InnoDB 增加表空间文件
大小时，会在页面大小为 16 KiB 或更小的情况下分配 1 MiB 的新范围。这有助于提高磁盘 I/O 的顺序性
（新范围在磁盘上获得连续扇区的程度）。页面拆分越多，页面不仅会在一个范围内分散，还会在多个范围内
分散，导致磁盘 I/O 更加随机。由于页面分割而创建的新页面很可能位于磁盘的完全不同部分，因此在读取
页面时，随机 I/O 量也会增加。如图 25-5 所示。

[[./images/xKb7zE.png]]

图中显示了三个扩展。为简单起见，每个扩展只显示 5 个页面（默认页面大小为 16 KiB，每个扩展有 64
个页面）。页面拆分后的页面会突出显示。第 11 页被拆分时，后面的页面只有第 13 页，因此第 11 页和
第 12 页的位置仍然相对较近。然而，第 15 页在创建了几个额外页面后被拆分，这意味着第 16 页最终位
于下一个 extent 中。

更深的 B 树、更多占用缓冲池空间的页面以及更多随机 I/O 的组合意味着，以随机主键顺序插入行的表的
性能不如以主键顺序插入数据的同类表。这种性能差异不仅适用于插入数据，也适用于数据的后续使用。因
此，以主键顺序插入数据对于获得最佳性能非常重要。接下来将讨论如何做到这一点。

** 按主键顺序插入

正如前面的讨论所示，按照主键顺序插入数据有很大的好处。最简单的方法是使用无符号整数自动生成主键
值，并声明该列为自动递增列。或者，你需要确保按照主键顺序插入数据。本节将研究这两种情况。

*** 自动增长主键

确保按主键顺序插入数据的最简单方法是使用自动递增主键，让 MySQL 自行赋值。具体方法是在创建表格
时为主键列指定自动递增属性。也可以在多列主键中使用自动递增列；在这种情况下，自动递增列必须是
索引中的第一列。

清单 25-4 显示了创建两个使用自动递增列按主键顺序插入数据的表的示例。

#+begin_src sql
  drop schema if exists chapter_25;

  create schema chapter_25;

  create table chapter_25.t1(
    id int unsigned not null auto_increment,
    val varchar(10),
    primary key (id)
  );

  create table chapter_25 (
    id int unsigned not null auto_increment,
    CreateDate datetime not null default current_timestamp(),
    val varchar(10),
    primary key(id, CreateDate)
  );
#+end_src

t1 表的主键只有一列，而且值是自动递增的。使用无符号整数而不是有符号整数的原因是，自动递增值总
是大于 0，因此使用无符号整数可以在用完可用值之前允许多出两倍的值。示例中使用的是 4 字节整数，
如果使用所有值，可容纳的行数略少于 43 亿行。如果还不够，可以将列声明为 bigint unsigned，使用
8 字节，可容纳 1.8E19 行。

t2 表在主键中添加了日期时间列，例如，如果要按行的创建时间进行分区，这将非常有用。自动递增的 id
列仍能确保创建的记录具有唯一主键，而且由于 id 列是主键中的第一列，即使主键中的后续列是随机的，
记录仍会按主键顺序插入。

使用自动递增主键时，可以使用 sys 模式中的 schema_auto_ increment_columns 视图来检查自动递增值的
使用情况，并监控是否有任何表接近耗尽其值。清单 25-5 显示了 sakila.payment 表的输出结果。

#+begin_src sql
  select *
  from sys.schema_auto_increment_columns
  where table_schema='sakila'
    and table_name='payment'\G
#+end_src

使用自动递增主键时，可以使用 sys 模式中的 schema_auto_ increment_columns 视图来检查自动递增值
的使用情况，并监控是否有任何表接近耗尽其值。清单 25-5 显示了 sakila.payment 表的输出结果。

如果从外部源插入数据，可能已经为主键列分配了值（即使使用自动递增主键）。让我们看看在这种情况下
可以做些什么。

*** 插入存在的数据

无论您是需要插入某个流程生成的数据、恢复备份，还是使用不同的存储引擎转换表，在插入数据之前，
最好都要确保数据是按主键顺序排列的。如果数据是由你生成的，或者已经存在，那么你可以考虑在插入
前对数据进行排序。或者，在导入完成后使用 OPTIMIZE TABLE 语句重建表。

#+begin_src sql
  optimize table chapter_25.t1\G
#+end_src

对于大型表来说，重建可能需要大量时间，但除了在开始和结束时需要加锁以确保一致性的短暂时间外，
整个过程都是在线进行的。

如果使用 mysqldump 程序创建备份，可以添加 --orderby-primary 选项，这样 mysqldump 就会添加包含
主键列的 ORDER BY 子句（mysqlpump 没有类似选项）。如果备份对象是使用所谓堆组织数据的存储引擎
（如 MyISAM）创建的表，目的是将其恢复到 InnoDB 表（使用索引组织数据），那么这个选项就特别有用。

#+begin_comment
  一般来说，在使用不带ORDER BY子句的查询时，不应该依赖返回行的顺序，但InnoDB的索引组织行意味
  着，即使省略了ORDER BY子句，全表扫描通常（但不保证）也会按主键顺序返回行。一个明显的例外情
  况是，表中包含一个覆盖所有列的二级索引，并且优化器选择在查询中使用该索引。
#+end_comment

如果将数据从一个表复制到另一个表，也可以使用相同的原理。清单 25-6 显示了将 world.city 表中的行
复制到 world.city_new 表的示例。

#+begin_src sql
  create table world.city_new like world.city;

  insert into world.city_new select * from world.city order by id;
#+end_src

*** UUID主键

如果主键只能使用 UUID，例如，因为无法更改应用程序以支持自动递增主键，那么可以通过交换 UUID 组
件并将 UUID 存储在二进制列中来提高性能。

一个 UUID（MySQL 使用版本 1 的 UUID）由一个时间戳、一个序列号（以保证时间戳向后移动时的唯一性，
例如在夏令时）和一个 MAC 地址组成。

#+begin_comment
  在某些情况下，透露 MAC 地址可能会被认为是一个安全问题，因为它可以用来识别计算机和潜在用户。
#+end_comment

时间戳是一个 60 位值，包含自 1582 年 10 月 15 日午夜（公历开始使用）起 100 毫微秒的间隔数，使
用 UTC。(时间戳的高字段还包括 UUID 版本的四个比特）。UUID 的组成也如图 25-6 所示）。

[[./images/WJ4Gin.png]]

时间戳的低位部分最多代表 4,294,967,295 (0xffffffffffff) 个 100 纳秒或略低于 430 秒的间隔。这意
味着，每隔 7 分钟零 10 秒，时间戳的低部分就会滚动一次，从而使 UUID 从排序的角度重新开始。这就
是为什么普通 UUID 不能很好地用于索引组织的数据，因为这意味着插入的数据在很大程度上将是主键树中
的随机位置。

MySQL 8包含两个新函数来处理UUID，使其更适合作为InnoDB的主键：UUID_TO_BIN()和BIN_TO_UUID()。这些
函数分别将 UUID 从十六进制转换为二进制，然后再转换回来。它们接受相同的两个参数：要转换的 UUID
值，以及是否交换时间戳的高低部分。清单 25-7 展示了一个使用这些函数插入和检索数据的示例。

#+begin_src sql
  create table chapter_25.t3(
    id binary(16) not null,
    val varchar(10),
    primary key(id)
  );

  insert into chapter_25.t3
  values (uuid_to_bin(
            '14614d6e-b5a8-11e9-ae6e-080027b7c106',
  	  TRUE 
  ), 'abc');

  select bin_to_uuid(id, true) as id, val from chapter_25.t3\G
#+end_src

这种方法有两方面的优势。由于 UUID 的低时间和高时间部分互换，因此它变得单调递增，更适合索引组织的行。二
进制存储意味着 UUID 只需要 16 个字节的存储空间，而不是用破折号分隔 UUID 各部分的十六进制版本的 36 个字
节。请记住，由于数据是通过主键组织的，因此主键会被添加到二级索引中，这样就可以从索引转到行，所以存储主
键所需的字节数越少，二级索引就越小。

** InnoDB缓冲池和二级索引

影响批量数据加载性能的最重要因素是 InnoDB 缓冲池的大小。本节将讨论为什么缓冲池对批量数据加载很重要。


向表中插入数据时，InnoDB 需要在缓冲池中存储数据，直到数据被写入表空间文件。缓冲池中能存储的数据越多，
InnoDB 向表空间文件刷新脏页面的效率就越高。不过，还有第二个原因，那就是维护二级索引。

次要索引需要在插入数据时进行维护，但次要索引的排序顺序与主键不同，因此在插入数据时会不断重新排列。只要
索引能在内存中得到维护，插入率就能保持较高水平，但当缓冲池中无法再容纳索引时，维护索引的成本就会突然大
大增加，插入率也会显著下降。图 25-7 说明了性能如何取决于缓冲池处理二级索引的可用性。

[[./images/uk86g9.png]]

从图中可以看出，插入率在一段时间内基本保持稳定，在此期间，越来越多的缓冲池被用于二级索引。当缓冲池中无
法存储更多索引时，插入率会突然下降。在极端情况下，将数据加载到带有单一二级索引的表中，该表包含整条记录，
没有其他任何内容，当二级索引使用了近一半的缓冲池（剩余部分用于主键）时，插入率就会下降。

您可以使用 information_schema.INNODB_BUFFER_PAGE 表来确定索引在缓冲池中使用了多少空间。例如，要查找
world.city 表中 CountryCode 索引在缓冲池中使用的内存量

#+begin_src sql
  select count(*) as NumPages,
  ifnull(sum(DATA_SIZE), o) as DataSize,
  ifnull(sum(if(COMPRESSED_SIZE = 0,
  @@glbal.innodb_page_size,
  COMPRESSED_SIZE)),
  0) as CompressesSize
  from information_schema.INNODB_BUFFER_PAGE
  where TABLE_NAME='`world`.`city`'
  and INDEX_NAME='CountryCode';
#+end_src

结果将取决于使用索引的程度，因此一般来说结果会有所不同。该查询最好在测试系统上使用，因为查询
INNODB_BUFFER_PAGE 表可能会产生很大的开销。

#+begin_comment
  在生产系统上查询INNODB_BUFFER_PAGE表时一定要小心，因为开销可能会很大，尤其是在缓冲池较大、表
  和索引较多的情况下。
#+end_comment

当二级索引无法放入缓冲池时，有以下三种策略可以避免性能受影响：

+ 增大缓冲池
+ 插入数据的时候删除二级索引
+ 分区表

在批量加载过程中增加缓冲池大小是最明显的策略，但也是最不可能有用的策略。它主要适用于向已经有大量数据的
表中插入数据的情况，因为在数据加载期间，可以将其他进程需要的部分内存用于缓冲池。动态调整缓冲池大小的支
持在这种情况下非常有用。例如，将缓冲池大小设置为 256 MiB

#+begin_src sql
  set global innodb_buffer_pool_size=256*1024*1024;
#+end_src

数据加载完成后，可以将缓冲池大小设置回常规值（如果使用默认值，则为 134217728）。

如果要向空表中插入数据，一个非常有用的策略是在加载数据前删除所有二级索引（可能为数据验证保留唯一索引），
然后再将索引添加回去。在大多数情况下，这比在加载数据时维护索引更有效率，如果使用 mysqlpump 工具创建备
份，这也是该工具的作用。

最后一种策略是对表进行分区。由于索引是分区的本地索引（这也是分区键必须是所有唯一索引的一部分的原因），
因此如果按分区顺序插入数据，InnoDB 只需维护当前分区中数据的索引。这样一来，每个索引的体积都会变小，从
而更容易放入缓冲池。

** 配置

您可以通过对执行加载的会话进行配置来影响加载性能。这包括考虑关闭约束检查、自动增加 ID 的生成方式等。

表 25-1 总结了除缓冲池大小外与批量数据性能有关的最重要配置选项。范围是指该选项是可以在会话级别更改，还
是只能全局更改。

| 选项名                         | 作用域  | 描述                                                                                                                                                                     |
|--------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| foreign_key_checks             | Session | 指定是否检查新记录是否违反外键。禁用该选项可以提高带外键表的性能。                                                                                                       |
| unique_checks                  | Session | 指定是否检查新记录是否违反唯一性约束。禁用该选项可以提高具有唯一索引的表的性能。                                                                                         |
| innodb_autoinc_lock_mode       | Global  | 指定 InnoDB 如何确定下一个自动递增值。将此选项设置为 2（MySQL 8 中的默认值要求 binlog_format = ROW）可获得最佳性能，但可能会出现不连续的自动递增值，需要重新启动 MySQL。 |
| innodb_flush_log_at_trx_commit | Global  | 决定 InnoDB 刷新数据文件更改的频率。如果使用许多小事务导入数据，将此选项设置为 0 或 2 可以提高性能。                                                                     |
| sql_log_bin                    | Session | 设置为 0 或 OFF 时禁用二进制日志，这将大大减少写入的数据量。                                                                                                             |
| transaction_isolation          | Session | 设置事务隔离级别。如果不读取 MySQL 中的现有数据，请考虑将隔离级别设置为 READ UNCOMMITTED。                                                                               |

所有选项都有副作用，因此请仔细考虑更改设置是否适合你。例如，如果要从现有实例向新实例导入数据，而且知道
外键和唯一键约束没有问题，那么可以在导入数据的会话中禁用外键检查（foreign_key_checks）和唯一性检查
（unique_checks）选项。另一方面，如果你是从数据源导入数据，而且不确定数据的完整性，那么最好还是启用约
束检查，以确保数据的质量，即使这意味着会降低加载性能。

对于 innodb_flush_log_at_trx_commit 选项，你需要考虑是否可以接受丢失最后一秒左右已提交事务的风险。如果
你的数据加载进程是实例上唯一的事务，而且很容易重做导入，你可以把 innodb_flush_log_at_trx_commit 设置为
0 或 2，以减少刷新次数。这一改动主要适用于小型事务。如果导入的提交次数少于每秒一次，那么这一修改的作用
就很小。如果你修改了 innodb_flush_log_at_trx_commit，那么记得在导入后把值设回 1。

对于二进制日志，禁用写入导入的数据非常有用，因为这样可以大大减少必须写入磁盘的数据变化量。如果二进制日
志与重做日志和数据文件位于同一磁盘上，这一点尤其有用。如果无法修改导入过程以禁用 sql_log_bin，可以考虑
使用 skip-log-bin 选项重新启动 MySQL，以完全禁用二进制日志，但要注意这也会影响系统上的所有其他事务。如
果在导入过程中确实禁用了二进制日志，那么在导入后立即创建一个完整备份可能会很有用，这样就可以再次使用二
进制日志进行时间点恢复。

#+begin_comment
  如果使用复制，请考虑在拓扑中禁用 sql_log_bin 的每个实例上分别进行数据导入。请注意，只有在 MySQL 不生
  成自动递增主键时才有效，只有在需要导入大量数据时才值得增加复杂性。对于 MySQL 8.0.17 中的初始加载，您
  只需填充复制的源，然后使用克隆插件3 创建副本即可。
#+end_comment

** 事务和加载方法

一个事务表示一组更改，InnoDB 在事务提交之前不会完全应用这些更改。每次提交都会将数据写入重做日志，还包
括其他开销。如果事务很小，比如一次只插入一条记录，那么这种开销就会严重影响加载性能。

最佳事务大小没有金科玉律。对于较小的行大小，通常几千行就足够了，而对于较大的行大小，可以选择较少的行数。
最终，你需要在你的系统和数据上进行测试，以确定最佳事务大小。

对于加载方法，主要有两种选择： INSERT 语句或 LOAD DATA [LOCAL] INFILE 语句。一般来说，LOAD DATA 比
INSERT 语句的性能更好，因为它的解析工作更少。对于 INSERT 语句，使用扩展插入语法的优势在于使用一条语句
插入多行，而不是多条单行语句。

#+begin_comment
  使用 mysqlpump 备份时，可以将 --extendedinsert 选项设置为每个 INSERT 语句包含的行数，默认值为 250。
  每个 INSERT 语句要包含的记录数，默认值为 250。对于 mysqldump，--extended-insert 选项是一个开关。启用
  该选项后（默认值），mysqldump 会自动决定每条语句的行数。
#+end_comment

** MySQL并行导入数据

将数据加载到 MySQL 时可能会遇到的一个问题是，单个线程无法将 InnoDB 推到它所能承受的极限。如果将
数据分成批次，使用多个线程加载数据，就能提高整体加载速度。自动做到这一点的一个方法是使用 MySQL
Shell 8.0.17 及更高版本的并行数据加载功能。

并行加载功能可通过 Python 模式下的 util.import_table() 工具和 JavaScript 模式下的 util.importTable()
方法实现。本讨论假定您使用的是 Python 模式。第一个参数是文件名，第二个参数（可选）是一个包含可选参数的
字典。您可以使用 util.help() 方法获取 import_table() 工具的帮助文本，例如

#+begin_src python
  util.help('import_table')
#+end_src

帮助文本包括对所有设置的详细说明，这些设置可以通过第二个参数中指定的字典进行设置。

MySQL Shell 会禁用重复键和外键检查，并将导入连接的事务隔离级别设置为 READ UNCOMMITTED，以尽可能减少导
入过程中的开销。

默认情况下，数据会插入当前模式中与文件同名且不带扩展名的表中。例如，如果文件名为 t_load.csv，默认表名
为 t_load。将文件 D:\MySQL\Files\t_ load.csv 加载到表 chapter_25.t_load 的简单示例如清单 25-8 所示。本
书的 GitHub 代码库中提供了 t_load.csv 文件 t_load.csv.zip。

#+begin_src sql
  create schema if not exists chapter_25;

  drop table if exists chapter_25.t_load;

  create table chapter_25.t_load(
     id int unsigned not null auto_increment,
     val varchar(40) not null,
     primary key(id),
     index(val)
  );

  set global local_infile=on;
#+end_src

#+begin_src python
  util.import_table('./t_load.csv')
#+end_src

创建 chapter_25 模式时出现的警告取决于您是否已提前创建了该模式。请注意，必须启用 local_infile 选项才能
使用该工具。

示例中最有趣的部分是导入的执行。如果不指定任何内容，MySQL Shell 会将文件分割成 50 MB 的块，并最多使用
8 个线程。在本例中，文件大小为 85.37 MB（MySQL Shell 使用公制文件大小，85.37 MB 等于 81.42 MiB），因此
有两块，第一块为 50 MB，第二块为 35.37 MB。这样的分配并不糟糕。


#+begin_comment
  在调用 util.import_table() 工具之前，必须在服务器端启用 local_infile。
#+end_comment

您可以选择告诉 MySQL Shell 分割的大小。最佳做法是每个线程最终处理的数据量相同。例如，如果要分割 85.37
MB 的数据，可将分块大小设置为稍大于一半的大小，如 43 MB。如果为大小指定了十进制值，则会向下舍入。还可
以设置其他一些选项，清单 25-9 展示了一个设置这些选项的示例。

#+begin_src sql
  truncate table chapter_25.t_load
#+end_src

#+begin_src python
  settings={
      'schema':'chapter_25',
      'table':'t_load',
      'columns':['id', 'val'],
      'threads':4,
      'bytesPerChunk':'21500k',
      'fieldsTerminatedBy':'\t',
      'fieldsOptionallyEnclosed':False,
      'linesTerminatedBy':'\n'
  }
  util.import_table('./t_load.csv')
#+end_src

在这种情况下，会明确指定目标模式、表和列，并将文件分成大致相等的四块，同时将线程数设为四个。CSV 文件的
格式也包含在设置中（指定值为默认值）。最佳线程数因硬件、数据和其他正在运行的查询而有很大不同。您需要通
过实验来找到适合您系统的最佳设置。

** 总结

本章讨论了决定 DDL 语句和批量数据加载性能的因素。第一个主题是 ALTER TABLE 和 OPTIMIZE TABLE 方面的模式
更改。在进行模式更改时，支持三种不同的算法。性能最好的算法是 INSTANT 算法，它可用于在行尾添加列和几种
元数据更改。第二好的算法是 INPLACE 算法，在大多数情况下，它会修改现有表空间文件中的数据。最后一种算法
是 COPY，通常也是最昂贵的算法。

在无法使用 INSTANT 算法的情况下，会有大量的 I/O，因此磁盘性能很重要，需要磁盘 I/O 的其他工作越少越好。
锁定表可能也会有帮助，这样 MySQL 就不需要跟踪数据更改并在模式更改结束时应用这些更改。

在插入数据时，与会者讨论了按主键顺序插入数据的重要性。如果插入顺序是随机的，就会导致表变大、聚类索引的
B 树索引变深、磁盘寻道次数增多以及 I/O 更加随机。按主键顺序插入数据的最简单方法是使用自动递增主键，让
MySQL 决定下一个值。对于 UUID，MySQL 8 增加了 UUID_TO_BIN() 和 BIN_TO_UUID()函数，可以将 UUID 所需的存
储空间减少到 16 字节，并交换时间戳的低阶和高阶部分，使 UUID 单调递增。

插入数据时，插入速度突然变慢的一个典型原因是二级索引不再适合缓冲池。如果插入到一个空表中，在导入过程中
删除索引会有好处。分区也可能有帮助，因为它将索引分成每个分区的一个部分，所以每次只需要索引的一部分。

在某些情况下，可以禁用约束检查、减少重做日志的刷新、禁用二进制日志记录，以及将事务隔离减少到 READ
UNCOMMITTED。这些配置更改都有助于减少开销，但也都有副作用，因此必须仔细考虑系统是否可以接受这些更改。
您还可以通过调整事务大小来影响性能，从而在减少提交开销和处理大型事务的开销之间取得平衡。

对于批量插入，有两种加载数据的方法。可以使用常规的 INSERT 语句，也可以使用 LOAD DATA 语句。一般来说，
后者是首选方法。它还允许你使用 MySQL Shell 8.0.17 及更高版本的并行表导入功能。

* 复制

多年来，MySQL 广受欢迎的功能之一就是对复制的支持，它允许你建立一个 MySQL 实例，自动接收来自源的更新并
应用它们。有了快速事务和低延迟网络，复制可以接近实时，但要注意的是，除了 NDB Cluster 之外，MySQL 中没
有同步复制，因此仍然存在潜在的巨大延迟。数据库管理员的一项经常性任务就是努力提高复制的性能。多年来，
MySQL 复制有了许多改进，其中一些改进可以帮助您提高复制性能。

#+begin_comment
  本章重点介绍传统的异步复制。MySQL 8还支持组复制及其衍生的InnoDB簇。讨论组复制的细节超出了本书的范围；
  不过，讨论仍然适用于一般情况。关于组复制的详细信息，推荐阅读 Charles Bell（Apress）所著的《InnoDB
  Cluster 简介》（www.apress.com/gp/book/9781484238844）一书，以及 MySQL 参考手册
  （https://dev.mysql.com/doc/refman/en/group-replication.html）的最新更新。
#+end_comment

** 复制总览
本章首先将提供复制的高级概述，目的是介绍术语和测试设置，该测试设置将用于复制监控部分。本章的后半部分将
讨论如何提高连接和应用程序线程的性能，以及如何利用复制将工作卸载到副本。

#+begin_comment
  传统上，MySQL 复制的源和目标使用主站和从站这两个术语来描述。近来，术语已转向使用源和副本这两个词。同
  样，在副本上，处理复制事件的两种线程类型传统上称为 I/O 线程和 SQL 线程，而现在的术语是连接线程和应用
  线程。本书将尽可能使用新术语，但在某些情况下仍会使用旧术语。
#+end_comment

复制的工作原理是记录对复制源所做的更改，然后将这些更改发送到副本，在副本中，连接线程存储数据，一个或多
个应用线程应用这些数据。图 26-1 显示了复制的简化概览，省略了所有与存储引擎和实施细节相关的内容。

[[./images/DdYyJ7.png]]

复制的工作原理是记录对复制源所做的更改，然后将这些更改发送到副本，在副本中，连接线程存储数
据，一个或多个应用线程应用这些数据。图 26-1 显示了复制的简化概览，省略了所有与存储引擎和实
施细节相关的内容。

在副本上，连接线程接收事件并将其写入中继日志。中继日志的工作方式与二进制日志相同，只是在应
用者线程应用这些事件之前，将其用作临时存储。可能有一个或多个应用线程。也可能是副本从多个源
复制（称为多源复制），在这种情况下，每个复制通道都有一组连接线程和一个或多个应用者线程。
(在这种情况下，每个复制通道都有一个连接线程和一个或多个应用线程（也就是说，最常见的情况是
每个副本只有一个源）。可选的做法是，副本将更改写入自己的二进制日志，使其成为复制链下游副本
的源。在这种情况下，通常称其为中继实例。图 26-2 显示了一个复制从两个源接收更新的设置示例，
其中一个是中继实例。

[[./images/i6YTy7.png]]

在这里，源 1 复制到中继实例，中继实例再复制到复制实例。源 2 也复制到复制实例。每个通道都有
一个名称，以便区分它们，在多源复制中，每个通道都必须有一个唯一的名称。默认的通道名称是空字
符串。在讨论监控时，将使用如图所示的复制设置。

** 监控

当你遇到复制性能问题时，第一步是确定在上一节描述的步骤链中哪里出现了延迟。如果您在 MySQL
早期版本中使用复制，您可能会跳转到 SHOW SLAVE STATUS 命令来检查复制的健康状况；但在 MySQL
8 中，这是最后一个要检查的监控信息源。

在 MySQL 8 中，复制的主要监控信息来源是性能模式（Performance Schema），该模式包含多个表，
描述复制器上每个复制步骤的复制配置和状态。性能模式表的一些优点如下：


+ 状态表包括复制延迟的更详细信息，其形式为复制过程中每一步的微秒分辨率时间戳，以及原始源和
  直接源的时间戳。
+ 您可以使用 SELECT 语句查询表。这样就可以查询你最感兴趣的信息，还可以对数据进行操作。在有
  多个复制通道的情况下，SHOW SLAVE STATUS 的输出很快就会滚出屏幕，因此在控制台中检查时很难
  使用。
+ 数据按逻辑分组，每组一个表。配置和应用程序流程有单独的表格，配置和状态也有单独的表格。

#+begin_comment
 SHOW SLAVE STATUS 中的 Seconds_Behind_Master 列传统上用于测量复制延迟。它主要显示自原始源
 上的事务开始以来已经过去了多长时间。这意味着，它只有在所有事务都非常快且没有中继实例的情
 况下才能真正起作用。如果您仍在使用 Seconds_Behind_Master 来监控复制延迟，建议您改用性能模
 式表。
#+end_comment

刚开始使用性能模式复制表时，可能很难理解表与表之间的关系以及它们与复制流程的关 系。图 26-3
显示了单个复制通道的复制流程，并添加了与其包含的信息相对应的复制表。图 26-3 中的表也可用于
组复制设置，在这种情况下，当节点在线时，group_replication_applier 通道用于事务，而
group_replication_recovery 通道用于恢复期间。

[[./images/u4rpdn.png]]

事件从直接源到达图的顶部，由连接线程处理，该线程有两个表 replication_connection_
configuration 和 replication_connection_status。连接线程将事件写入中继日志，应用程序运行复
制过滤器的同时从中继日志中读取事件。复制过滤器可在 replication_applier_ filters 表和
replication_applier_global_filters 表中找到。整个应用程序界面的配置和状态可在
replication_applier_configuration 表和 replication_applier_status 表中找到。

如果是并行复制（也称为多线程从属），协调器会处理事务，并将其提供给工人使用。协调器可通过
replication_applier_status_by_ coordinator 表进行监控。如果复制使用单线程复制，则跳过协调
器步骤。

最后一步是应用程序员。在并行复制的情况下，每个复制通道都有 slave_ parallel_workers 线程，
每个线程在 replication_applier_status_by_worker 表中都有一行记录其状态。

** 连接表

复制事件到达副本的第一步是将其写入中继日志。连接线程负责处理

有两个性能模式表提供与连接相关的信息：

+ replication_connection_configuration： 每个复制通道的配置。
+ replication_connection_status： 复制通道的状态。其中包括时间戳，显示最后和当前队列事务最
  初提交的时间、在直接源实例上提交的时间以及写入中继日志的时间。每个通道有一行。

复制连接表包括与直接上游源连接相关的信息，以及原始源提交最新接收事件的时间戳。在简单复制设
置中，直接源和原始源是相同的，但在链式复制中，两者是不同的。清单 26-1 显示了上一节讨论的复
制设置中中继通道的两个连接表的内容示例。为提高本书的可读性，输出已重新格式化。原始格式化输
出（包括 source2 复制通道的行）包含在文件 listing_26_1.txt 中。

#+begin_src sql
  select *
  from performance_schema.replication_connection_configuration
  where CHANNEL_NAME='relay'\G
#+end_src

#+begin_src sql
  select *
  from performance_schema.replication_connection_status
  where CHANNEL_NAME='relay'\G
#+end_src

配置表在很大程度上与使用 CHANGE MASTER TO 语句设置复制时给出的选项相对应，除非明确更改配置，
否则数据是静态的。状态表主要包含易变数据，这些数据会随着事件的处理而快速变化

状态表中的时间戳尤其值得关注。它分为两组，第一组显示上次排队事件的时间戳，第二组显示当前正
在排队的事件的时间戳。正在排队的事件意味着该事件正在写入中继日志。例如，请看最后一个排队事
件的时间戳：

+ last_queued_transaction_original_commit_timestamp： 事件在原始源（源 1）上提交的时间。
+ last_queued_transaction_immediate_commit_timestamp： 事件在即时源（中继）上提交的时间。
+ last_queued_transaction_start_queue_timestamp： 该实例开始排队处理事件的时间，即收到事件
  后连接线程开始将事件写入中继日志的时间。
+ last_queued_transaction_end_queue_timestamp： 连接线程完成将事件写入中继日志的时间。

时间戳的分辨率为微秒级，因此可以让您详细了解事件从原始源发送到中继日志的时间。零时间戳
（“0000-00-00 00:00:00”）表示没有数据要返回；例如，当连接线程完全更新时，当前排队的时间戳
就会出现这种情况。应用程序表提供了事件在副本中运行的更多细节。

** 应用表

应用者线程更为复杂，因为它们既要处理过滤事件，又要应用事件，而且还支持并行应用者。

在撰写本报告时，存在以下性能模式表，其中包含有关应用程序线程的信息：

+ replication_applier_configuration： 该表显示每个复制通道的应用程序线程配置。目前唯一的设
  置是配置的复制延迟。每个通道有一行。
+ replication_applier_filters： 每个复制通道的复制过滤器。信息包括过滤器的配置位置和启用时
  间。
+ replication_applier_global_filters： 适用于所有复制通道的复制过滤器。信息包括过滤器的配
  置位置和启用时间。
+ replication_applier_status： 应用程序综合状态，包括服务状态、剩余延迟（当配置了所需延迟
  时）和事务重试次数。每个通道有一行。
+ replication_applier_status_by_coordinator： 使用并行复制时协调线程看到的应用程序运行状态。
  最后处理的事务和当前处理的事务都有时间戳。每个通道有一行。对于单线程复制，此表为空。
+ replication_applier_status_by_worker： 每个工作者的应用者状态。其中有上次应用事务和当前
  应用事务的时间戳。配置并行复制时，每个工作者（工作者数量由 slave_parallel_workers 配置）
  在每个通道上都有一条记录。对于单线程复制，每个通道只有一行。


在高层，应用程序表与连接表的模式相同，只是增加了过滤器配置表和对并行应用程序的支持。清单
26-2 显示了中继复制通道的 replication_applier_ status_by_worker 表的内容示例。为提高可读性，
输出已重新格式化。输出结果也可在本书 GitHub 代码库中的 listing_26_2.txt 文件中找到。

#+begin_src sql
  select *
  from performance_schema.replication_applier_status_by_worker
  where CHANNEL_NAME='relay'\G
#+end_src

时间戳与前面看到的模式相同，都包含最后处理的事务和当前事务的信息。请注意，第一行的所有时间
戳都为零，这表明应用程序无法利用并行复制。

对于第二行中最后一个应用的全局事务标识符为 4d22b3e5-a54f11e9-8bdb-ace2d35785be:213 的事务，
可以看到该事务在 11:29:36.1076 提交到原始源，在 11:29:44.822024 提交到直接源，在
11:29:51.910259 开始在本实例上执行，并在 11:29:52.403051 执行完毕。这表明每个实例增加了约
8 秒的延迟，但事务本身的执行时间仅为半秒。由此可以得出结论：复制延迟不是由应用单个大事务造
成的，而是中继和复制实例处理事务的速度不及原始源的累积效应；延迟是由先前的长期运行事件造成
的，而复制尚未跟上；或者延迟是由复制链的其他部分造成的。

** 日志状态

日志状态表（log_status）是一个与复制相关的表，它提供二进制日志、中继日志和 InnoDB 重做日志
的信息，使用日志锁返回同一时间点的相应数据。该表是为备份而引入的，因此需要 BACKUP_ADMIN 权
限才能查询该表。清单 26-3 显示了一个使用 JSON_PRETTY() 函数的输出示例，以方便阅读以 JSON
文档形式返回的信息。

#+begin_src sql
  select SERVER_UUID,
         JSON_PRETTY(LOCAL) AS LOCAL,
         JSON_PRETTY(REPLICATION) AS REPLICATION,
         JSON_PRETTY(STORAGE_ENGINES) AS STORAGE_ENGINES
  from performance_schema.log_status\G
#+end_src

LOCAL 列包含有关已执行的全局事务标识符、二进制日志文件和该实例位置的信息。REPLICATION 列显
示与复制过程有关的中继日志数据，每个通道一个对象。STORAGE_ENGINES 列包含有关 InnoDB 日志序
列号的信息。

** 组复制表

如果使用 “组复制”，还可以使用另外两个表来监控复制。其中一个表包含组内成员的高级信息，另一
个表包含成员的各种统计信息。

两张表

+ replication_group_members： 成员的高级概览。每个成员都有一行，数据包括当前状态以及是主成
  员还是辅助成员。
+ replication_group_member_stats： 较低级别的统计信息，如队列中的事务数量、所有成员中已提
  交的事务数量、本地或远程事务数量等。

复制组成员表对验证成员的状态最有用。复制组成员统计表可用于查看每个节点如何看待已完成的工作，
以及冲突和回滚率是否很高。这两个表都包含群集中所有节点的信息。既然知道了如何监控复制，就可
以开始优化连接和应用程序线程了。

** 连接

连接线程负责处理与直接复制源的出站连接、复制事件的接收以及将事件保存到中继日志中。这意味着，
连接过程的优化主要围绕复制事件、网络、维护已接收事件的信息以及编写中继日志。

** 复制事件

当使用基于行的复制（默认和推荐）时，事件包括有关被更改的行和新值（之前和之后的映像）的信息。
默认情况下，更新和删除事件会包含完整的之前映像。这样，即使源和副本的列顺序不同或主键定义不
同，副本也能应用事件。不过，这也会使二进制日志（以及中继日志）变大，这意味着更多的网络流量、
内存使用量和磁盘 I/O。

如果不需要完整的行前图像，可以将 binlog_row_image 选项配置为 minimal 或 noblob。值 minimal
意味着前图像只包含识别行所需的列，后图像只包含事件所改变的列。使用 noblob 时，除了 blob 列
和文本列外，所有列都会包含在前图像中，而 blob 列和文本列只有在其值发生变化时才会包含在后图
像中。使用最小化是性能的最佳选择，但在生产系统上进行更改之前，请确保进行了全面测试。

#+begin_comment
  在生产系统中更改配置前，请确保已验证应用程序能在 binlog_ row_image = minimal 的情况下运
  行。如果应用程序无法使用该设置，将导致复制失败。
#+end_comment

** 网络

MySQL 内部用于复制的网络的主要调整选项是使用的接口和是否启用压缩。如果网络负荷过重，复制很
快就会落后。避免这种情况的办法是为复制流量使用专用的网络接口和路由。另一种方法是启用压缩，
这可以减少传输的数据量，但代价是增加 CPU 负载。这两种解决方案都可以通过 CHANGE MASTER TO
命令来实现。

在定义如何连接复制源时，可以使用 MASTER_BIND 选项指定连接使用的接口。例如，如果要使用复制
机上 IP 地址为 192.0.2.102 的接口从 192.0.2.101 的源进行复制，则可以使用
MASTER_BIND='192.0.2.102'：

#+begin_src sql
  CHANGE MASTER TO MASTER_BIND='192.0.2.102',
         	      	 MASTER_HOST='192.0.2.101',
  		 MASTER_PORT=3306,
  		 MASTER_AUTO_POSITION=1,
  		 MASTER_SSL=1;
#+end_src

#+begin_comment
  不启用 SSL 可能会提高网络性能。如果这样做，包括身份验证信息和数据在内的通信将在未加密的
  情况下传输，任何接入网络的人都可以读取数据。因此，对于任何处理生产数据的设置来说，确保所
  有通信的安全性都是非常重要的，对于复制来说，这就意味着要启用 SSL。
#+end_comment

在 MySQL 8.0.18 及更高版本中，可使用 MASTER_COMPRESSION_ ALGORITHMS 选项启用压缩功能，该选
项可使用一组允许的算法。支持的算法有

+ uncompressed： 禁用压缩。这是默认设置。
+ zlib： 使用 zlib 压缩算法。
+ zstd：使用 ztd 1.3 版压缩算法。

如果包含 zstd 算法，则可以使用 MASTER_ZSTD_COMPRESSION_ LEVEL 选项指定压缩级别。支持的压缩
级别为 1-22（都包括在内），默认为 3。配置复制连接使用压缩级别为 5 的 zlib 或 zstd 算法的示
例如下

#+begin_src sql
  CHANGE MASTER TO MASTER_COMPRESSION_ALGORITHMS='zlib,zstd',
                   MASTER_ZSTD_COMPRESSION_LEVEL=5;
#+end_src

在 MySQL 8.0.18 之前，使用 slave_ compressed_protocol 选项指定是否使用压缩。如果源和副本都
支持 zlib 压缩算法，将该选项设置为 1 或 ON 会使复制连接使用 zlib 压缩。

#+begin_comment
  如果在 MySQL 8.0.18 或更高版本中启用了 slave_compressed_protocol 选项，它将优先于
  MASTER_COMPRESSION_ ALGORITHMS。建议禁用 slave_compressed_protocol，并使用 CHANGE MASTER
  TO 命令来配置压缩，因为这样可以使用 zstd 算法，而且可以在
  replication_connection_configuration 性能模式表中提供压缩配置。
#+end_comment
  
** 保存源信息

副本需要跟踪它从源接收到的信息。这可以通过 mysql.slave_master_info 表来完成。也可以将信息
存储在文件中，但从 8.0.18 版本开始，这种做法已被弃用，不建议使用。使用文件还会降低副本从崩
溃中恢复的能力。

关于维护这些信息的性能，重要的选项是 sync_master_info。该选项指定了信息的更新频率，默认为
每 10000 个事件更新一次。你可能会认为，与复制源端 sync_binlog 类似，在每次事件后同步数据很
重要，但事实并非如此。

#+begin_comment
  没有必要设置 sync_master_info = 1，这样做是复制滞后的常见原因。
#+end_comment

无需频繁更新信息的原因是，如果信息丢失，可以通过丢弃中继日志并从应用程序运行到的点开始获取
所有信息来恢复。因此，默认值 10000 是个不错的值，很少有必要更改。

#+begin_comment
  复制可以从崩溃中恢复的确切规则非常复杂，而且会随着新改进的增加而不断变化。您可以在
  https://dev.mysql.com/doc/refman/en/replication-solutions-unexpected-slave-halt.html
  中查看最新信息。
#+end_comment

** 写入中继日志

中继日志是接收复制事件的连接与应用程序处理复制事件之间的中间存储。影响中继日志写入速度的因
素主要有两个：磁盘性能和中继日志同步到磁盘的频率。

您需要确保写入中继日志的磁盘有足够的 I/O 容量来支持写入和读取活动。一种方法是将中继日志存
储在单独的存储设备上，这样其他活动就不会干扰中继日志的写入和读取。

中继日志同步到磁盘的频率由 sync_ relay_log 选项控制，该选项相当于 sync_binlog。默认值是每
10000 个事件同步一次。除非使用基于位置的复制（禁用 GTID 或 MASTER_AUTO_POSITION=0）和并行
应用程序线程，否则没有理由更改 sync_relay_log 的值，因为中继日志是可以恢复的。如果是基于位
置的并行复制，则需要 sync_relay_log = 1，除非在操作系统崩溃时重建副本是可以接受的。

这意味着，从性能角度看，建议启用全局事务标识符，并在执行 CHANGE MASTER TO 时设置
MASTER_AUTO_POSITION=1。否则，与主信息和中继日志相关的其他设置应保持默认值。

** 应用者

应用程序员是复制滞后的最常见原因。主要问题在于，源上所做的更改往往是高度并行工作负载的结果。
相比之下，默认情况下应用程序运行的是单线程，因此单线程必须跟上源上潜在的数十或数百个并发查
询。这就意味着，消除应用程序员造成的复制滞后的主要工具是启用并行复制。此外，还将讨论主键的
重要性、放宽数据安全设置的可能性以及复制过滤器的使用。


#+begin_comment
  如果中继日志存储库使用一个表，而 mysql.slave_relay_log_info 表使用 InnoDB（这两种表都是
  默认和推荐使用的），则更改 sync_relay_log_info 设置不会有任何影响。在这种情况下，设置实
  际上会被忽略，信息会在每次事务后更新。
#+end_comment

** 并行应用

将应用程序配置为使用多个线程并行应用事件，是提高复制性能的最有效方法。不过，这并不像将
slave_parallel_workers 选项设置为大于 1 的值那么简单。

| 选项名                                     | 描述                                                                                                                     |
|--------------------------------------------+--------------------------------------------------------------------------------------------------------------------------|
| binlog_transaction_dependency_tracking     | 在二进制日志中包含哪些有关事务间依赖关系的信息。                                                                         |
| binlog_transaction_dependency_history_size | 某一行最后一次更新的信息会保留多长时间。                                                                                 |
| transaction_write_set_extraction           | 如何提取写入集信息。                                                                                                     |
| binlog_group_commit_sync_delay             | 在分组提交功能中，等待更多事务分组的延迟时间。                                                                           |
| slave_parallel_workers                     | 为每个通道创建多少个应用程序线程                                                                                         |
| slave_parallel_type                        | 是在数据库上并行，还是在逻辑时钟上并行。                                                                                 |
| slave_pending_jobs_size_max                | 有多少内存可用于保存尚未应用的事件。                                                                                     |
| slave_preserve_commit_order                | 是否确保副本按照与源日志相同的顺序将事务写入其二进制日志。启用此功能需要将 slave_parallel_workers 设置为 LOGICAL_CLOCK。 |
| slave_checkpoint_group                     | 两次检查点操作之间要处理的事务的最大数量。                                                                               |
| slave_checkpoint_period                    | 检查点操作之间的最长间隔时间（毫秒）。                                                                                                                         |

最常用的选项是源上的 binlog_transaction_dependency_ tracking 和
transaction_write_set_extraction，以及副本上的 slave_parallel_ workers 和
slave_parallel_type。

源上的二进制日志事务依赖性跟踪和写集提取选项是相关的。transaction_write_set_extraction 选
项指定如何提取写集信息（有关哪些行受事务影响的信息）。写集也是群复制用于冲突检测的信息。将
其设置为 XXHASH64，这也是 Group Replication 所需的值。

binlog_transaction_dependency_tracking 选项用于指定二进制日志中的事务依赖性信息。这对并行
复制很重要，因为它能让我们知道哪些事务可以安全地并行应用。默认情况下使用提交顺序并依赖提交
时间戳。为提高根据逻辑时钟并行复制时的并行复制性能，请将
binlog_transaction_dependency_tracking 设置为 WRITESET。

binlog_transaction_dependency_history_size 选项指定了记录哈希值的数量，这些哈希值提供了最
后一次修改给定记录的事务的信息。默认值 25000 通常已经足够大；不过，如果对不同行的修改率非
常高，则值得增加依赖历史记录的大小。

在复制器上，可以使用 slave_parallel_workers 选项启用并行复制。这是为每个复制通道创建的应用
程序员线程数。设置足够高的数量，使复制跟得上，但也不要太高，以免出现空闲的工作线程，或因并
行工作负载过多而造成争用。

另一个通常需要在副本上更新的选项是 slave_parallel_type 选项。该选项指定了事件在应用程序员
之间的分割方式。默认选项是 DATABASE，顾名思义，就是根据更新所属的模式来分割更新。另一种选
择是 LOGICAL_CLOCK，它会使用二进制日志中的组提交信息或写入集信息来确定哪些事务可以安全地一
起应用。除非在二进制日志中不包含写集信息的情况下有多层副本，否则 LOGICAL_CLOCK 通常是最佳
选择。

如果在未启用写集的情况下使用 LOGICAL_CLOCK 并行化类型，可以增加源上的
binlog_group_commit_sync_delay，以便在分组提交功能中将更多事务分组，但提交延迟会更长。这将
使并行复制有更多事务可在工作者之间分配，从而提高效率。

** 主键

使用基于行的复制时，处理事件的应用程序员必须找到必须更改的行。如果有主键，这就非常简单高效，
只需进行主键查找即可。但是，如果没有主键，就必须检查所有行，直到找到一条所有列的值都与复制
事件前映像中的值相同的行。

如果表很大，这种搜索的成本就会很高。如果事务在一个相对较大的表中修改了很多行，那么在最坏的
情况下，它可能会使复制看起来像停止了一样。MySQL 8 采用了一种优化方法，即使用哈希值来匹配表
中的一组行；不过，这种方法的有效性取决于一次事件中修改的行数，而且永远不会像主键查找那样高
效。

强烈建议为所有表添加一个显式主键（或一个非空的唯一键）。不添加主键不会节省磁盘空间或内存，
因为如果你不自己添加主键，InnoDB 就会添加一个隐藏主键（不能用于复制）。隐藏主键是一个 6 字
节的整数，使用全局计数器，因此如果有很多表都有隐藏主键，计数器就会成为瓶颈。此外，如果要使
用组复制，则必须严格要求所有表都有一个显式主键或一个非空的唯一索引。

#+begin_comment
  启用 sql_require_primary_key 选项，要求所有表都有主键。该选项在 MySQL 8.0.13 及更高版本
  中可用。
#+end_comment

如果无法在某些表中添加主键，那么散列搜索算法的效果会更好，因为每个复制事件中包含的行数越多。
对于修改同一表中大量行的事务，可以通过增加复制源实例上 binlog_row_event_max_size 的大小来
增加分组的行数。

** 放松数据安全

事务提交后，必须在磁盘上持久化。在 InnoDB 中，通过重做日志来保证持久性，而对于复制，则通过
二进制日志来保证持久性。在某些情况下，在副本上放松对更改已被持久化的保证是可以接受的。这种
优化的代价是，如果操作系统崩溃，就需要重建副本。

InnoDB使用选项innodb_flush_log_at_trx_commit来决定是否在事务提交时刷新重做日志。默认（也是
最安全的设置）是在每次提交后刷新（innodb_flush_log_at_trx_commit = 1）。刷新是一项昂贵的操
作，即使是某些固态硬盘驱动器也很难满足繁忙系统的刷新需求。如果你能承受一秒钟已提交事务的损
失，可以将 innodb_flush_log_at_trx_commit 设置为 0 或 2。如果愿意进一步推迟刷新时间，可以
增加 innodb_flush_log_at_timeout。默认值和最小值都是 1 秒。这意味着如果发生灾难性故障，很
可能需要重建副本，但好处是应用线程提交更改的成本比源线程低，因此更容易跟上进度。

二进制日志同样使用 sync_binlog 选项，该选项的默认值也是 1，这意味着每次提交后都会刷新二进
制日志。如果不需要副本上的二进制日志（注意，对于组复制，必须在所有笔记上启用二进制日志），
可以考虑完全禁用它，或者降低日志同步的频率。通常，在这种情况下，最好将 sync_binlog 设置为
100 或 1000，而不是 0，因为 0 通常会导致二进制日志在旋转时被一次性刷新。刷新一个千兆字节的
日志可能需要几秒钟；在此期间，会有一个静态代理阻止提交事务。


#+begin_comment
  如果放宽了副本上的数据安全设置，在将副本升级为复制源时，例如需要执行维护时，请确保将其设
  置回更严格的值。
#+end_comment

** 复制过滤

如果不需要副本上的所有数据，可以使用复制过滤器来减少应用线程所需的工作，并降低磁盘和内存需
求。这还能帮助副本与其源保持同步。有六个选项可用于设置复制过滤器。如表 26-2 所示，这些选项
可分为三组，其中有一个执行选项和一个忽略选项。


| 选项名                      | 描述                                                                                              |
|-----------------------------+---------------------------------------------------------------------------------------------------|
| replicate-do-db             | 是否包含作为值给出的模式（数据库）的更改。                                                        |
| replicate-ignore-db         |                                                                                                   |
| replicate-do-table          | 是否包含作为值给定的表格的更改。                                                                  |
| replicate-ignore-table      |                                                                                                   |
| replicate-wild-do-table     | 与 replicate-do-table 和 replicate-ignoretable 选项类似，但在编写 LIKE 子句时支持 _ 和 % 通配符。 |
| replicate-wild-ignore-table |                                                                                                   |

指定其中一个选项时，可以选择在模式/表前加上规则应适用的通道名称和冒号。例如，要忽略
source2 频道的world数据库更新

#+begin_comment
  [mysqld]
  replicate-do-db = source2:world
#+end_comment

这些选项只能在 MySQL 配置文件中设置，并且需要重新启动 MySQL 才能生效。可以多次指定每个选项，
以添加多条规则。如果需要动态更改配置，可以使用 CHANGE REPLICATION FILTER 语句配置过滤器：

#+begin_src sql
  CHANGE REPLICATION FILTER
         REPILCATE_IGNORE_DB=(world)
         FOR CHANNEL 'source2';
#+end_src

world 两边的括号是必需的，因为如果需要包含多个数据库，可以指定一个列表。如果您多次指定同一
规则，则后者适用，前者将被忽略。

#+begin_comment
  要查看 CHANGE REPLICATION FILTER 的完整规则，请参阅 https://dev.mysql.com/doc/refman/en/change-replication-filter.html 。
#+end_comment

复制筛选器最适合基于行的复制，因为事件影响到哪个表是一目了然的。如果使用语句，该语句可能会
影响多个表，因此对于基于语句的复制，过滤器是否应允许该语句并不总是很明确。使用
replicate-do-db 和 replicate-ignore-db 时应特别小心，因为对于基于语句的复制，它们会使用默
认模式来决定是否允许语句。更糟糕的是，复制过滤器会混合使用行和语句事件（binlog_format =
MIXED），因为过滤器的效果可能取决于复制更改的格式。

#+begin_comment
  使用复制筛选器时，最好使用 binlog_format = row（默认值）。有关复制过滤器的完整评估规则，
  请参阅 https://dev.mysql.com/doc/refman/en/replication-rules.html。
#+end_comment

关于如何提高复制性能的讨论到此结束。还有一个话题与前面讨论的内容正好相反，那就是如何通过使
用复制来提高源的性能。

** 将工作卸载至副本

如果遇到实例因读取查询而超载的问题，提高性能的常用策略是将部分工作卸载到一个或多个副本。一
些常见的情况是使用副本进行读取扩展，以及使用副本进行报告或备份。本节将对此进行探讨。

#+begin_comment
  使用复制（例如，使用组复制的多主模式）并不能扩展写入，因为所有更改仍必须应用于所有节点。
  要实现写扩展，就需要对数据进行分片，例如 MySQL nDB Cluster 中的做法。分片解决方案超出了
  本书的范围。
#+end_comment

** 读扩展

复制最常见的用途之一是允许读取查询使用副本，从而减少复制源的负载。之所以能做到这一点，是因
为复制的数据与其源数据相同。需要注意的主要问题是，即使在最好的情况下，从源上提交事务到副本
发生更改也会有少量延迟。

如果应用程序对读取陈旧数据很敏感，那么可以选择组复制或 InnoDB 群集，它们在 8.0.14 及更高版
本中支持一致性级别，因此可以确保应用程序使用所需的一致性级别。

#+begin_comment
  要很好地解释如何使用组复制一致性级别，强烈推荐阅读 Lefred 在
  https://lefred.be/content/mysql-innodbcluster-consistency-levels/ 上
  发表的博客，以及博客顶部与组复制开发人员博客的链接。
#+end_comment

使用副本进行读取还能帮助你拉近应用程序和 MySQL 与最终用户之间的距离，从而减少往返延迟，让
用户获得更好的体验。

** 分离任务

副本的另一个常见用途是在副本上执行一些影响较大的任务，以减少复制源的负载。两个典型的任务是
报告和备份。

当您使用副本进行报告查询时，您可能会受益于以不同于源的方式配置副本，以便针对其用于的特定工
作负载进行优化。还可以使用复制过滤器，避免包含源的所有数据和更新。更少的数据意味着副本需要
应用更少的事务和写入更少的数据，你可以将更大比例的数据读入缓冲池。

使用副本进行备份也很常见。如果副本专门用于备份，那么只要副本能在下一次备份前赶上，就不必担
心磁盘 I/O 或缓冲池污染导致的锁和性能下降。您甚至可以考虑在备份期间关闭副本并执行冷备份。

** 总结

本章介绍了复制的工作原理、如何监控和提高复制过程的性能，以及如何使用复制在多个实例之间分配
工作。

本章开头概述了复制，包括介绍术语，并说明了在哪里可以找到复制的监控信息。在 MySQL 8 中，监
控复制的最佳方法是使用一系列性能模式（Performance Schema）表，这些表根据线程类型、配置或状
态将信息分割开来。还有专门用于日志状态和组复制的表。

连接线程可以通过减少复制事件的大小来优化，方法是在复制事件中只包含有关更新行的前值的最小信
息。但这并不适用于所有应用程序。还可以更改网络和中继日志的写法。建议使用基于 GTID 的复制，
并启用自动定位功能，这样可以放宽中继日志的同步。

对应用程序性能而言，最重要的两件事是启用并行复制和确保所有表都有主键。并行复制可以在更新影
响的模式上进行，也可以按逻辑时钟进行。后者通常性能最好，但也有例外，因此需要根据工作负载进
行验证。

最后，讨论了如何使用复制来卸载原本需要在复制源上执行的工作。您可以使用复制来进行读取扩展，
因为您可以使用复制来读取数据，而将源专用于需要写入数据的任务。您还可以将复制用于高强度工作，
如报告和备份。


* 缓存

最便宜的查询是那些根本不执行的查询。本章将探讨如何使用缓存来避免执行查询或降低查询的复杂性。
首先，将讨论缓存如何无处不在，以及缓存有哪些不同类型。然后介绍如何在 MySQL 内部使用缓存表
和近似值来使用缓存。接下来的两节将讨论两种流行的缓存产品：Memcached 和 ProxySQL。最后，将
讨论一些缓存技巧。

** 缓存无处不在

即使你不认为自己已经实施了缓存，你也已经在多个地方使用了缓存。这些缓存是透明的，由硬件、操
作系统或 MySQL 层面维护。其中最明显的缓存是 InnoDB 缓冲池。

图 27-1 举例说明了缓存是如何存在于整个系统中的，以及如何添加自定义缓存。这张图（包括交互）
并不完整，但可以说明缓存是多么常见，以及缓存可以出现在多少个地方。

[[./images/OGV9tW.png]]


左下角是 CPU，它有几级缓存，用于缓存 CPU 指令和数据。操作系统有 I/O 缓存，InnoDB 有缓冲池。
所有这些缓存都是返回最新数据的缓存。

还有一些缓存可以提供略微陈旧的数据。这包括在 MySQL 中实施缓存表、在 ProxySQL 中缓存查询结
果或直接在应用程序中缓存数据。在这些情况下，您通常会定义一个时间段，将其视为足够新鲜的数据，
当数据达到给定的年龄--生存时间（TTL）--缓存条目就会失效。Memcached 解决方案很特别，因为它
有两个版本。常规的 Memcached 守护进程会使用 “生存时间”（time to live）或一些取决于应用程序
的逻辑，在数据太旧时将其驱逐；不过，还有一个特殊的 MySQL 版本，它作为插件工作，可以从
InnoDB 缓冲池中获取数据，并将数据写回缓冲池，因此数据永远不会过时。

在应用程序中使用可能过时的数据似乎是错误的。但在许多情况下，这完全没有问题，因为并不需要精
确的数据。如果您有一个显示销售数字仪表盘的应用程序，那么这些数据是在执行查询时的最新数据还
是几分钟前的数据有多大区别呢？当用户读完这些数据时，它们很可能已经稍微过时了。重要的是，销
售数字要保持一致并定期更新。

#+begin_comment
  请仔细考虑您的应用程序有哪些要求，记住，一开始放宽对数据必须是最新的要求，并在必要时提高
  要求的严格程度，要比让用户相信他们不能再获得最新结果要容易得多。如果您使用的缓存数据不会
  自动更新到最新值，您可以考虑存储数据的最新时间，并将其显示给用户，这样用户就能知道数据的
  最后刷新时间。
#+end_comment

** MySQL中的缓存

在 MySQL 内部实施缓存是一个合理的地方。如果缓存数据与其他表一起使用，这一点尤其有用。这样
做的缺点是仍然需要从应用程序往返数据库查询数据，而且需要执行查询。本节将介绍在 MySQL 中缓
存数据的两种方法：缓存表和直方图统计。

*** 缓存表

缓存表可用于预先计算数据，例如用于报告或仪表盘。它主要适用于经常需要的复杂聚合。

使用缓存表有几种方法。你可以选择创建一个表来存储与之配合使用的功能的结果。这样做成本低，但
灵活性相对较差，因为它只能用于该特征。或者，您可以创建需要连接在一起的构件，这样它们就可以
用于多个特征。这样查询的成本会稍高一些，但可以重复使用缓存数据，避免重复数据。哪种方法最好
取决于您的应用程序，您最终可能会选择一种混合方法，即一些表单独使用，另一些表连接在一起。

填充缓存表有两种主要策略。要么定期完全重建表，要么使用触发器持续更新数据。完全重建表的最佳
方法是创建缓存表的新副本，并在重建结束时使用 RENAME TABLE 交换表，因为这样可以避免在事务中
删除潜在的大量行，并避免随着时间的推移而产生碎片。另外，当缓存数据所依赖的数据发生变化时，
也可以使用触发器来更新缓存数据。在大多数情况下，如果可以接受使用不完全最新的数据，那么重建
缓存表是首选，因为这样做不容易出错，而且刷新是在后台完成的。

#+begin_comment
  如果通过在事务中删除现有数据来重建缓存表，那么要么禁用自动重新计算索引统计，并在重建结束
  时使用 ANALYZE TABLE，要么启用 innodb_stats_ include_delete_marked 选项。
#+end_comment

缓存列是一种特殊情况，它包含在一个不缓存数据的表中。缓存列有用的一个例子是存储属于某个组的
最新事件的时间、状态或 ID。设想一下，您的应用程序支持发送文本信息，您会为每条信息存储历史
记录，如在应用程序中创建信息的时间、发送信息的时间以及收件人确认信息的时间。在大多数情况下，
只需要最新状态和何时达到该状态，因此您可能希望将其存储在消息记录本身中，而不必明确查询。在
这种情况下，您可以使用两个表来存储状态：

#+begin_src sql
  create table message(
    message_id bigint unsigned not null auto_increment,
    message_text varchar(1024) not null,
    cached_status_time datetime(3) not null,
    cached_status_id tinyint unsigned not null,
    primary key(message_id)
  );
  create table message_status_history(
    message_status_id bigint unsigned not null auto_incrment,
    message_id bigint unsigned not null,
    status_time datetime(3) not null,
    status_id tinyint unsigned not null,
    primary key(message_status_id)
  );
#+end_src

在现实世界中，可能会有更多的列和外键，但对于本示例来说，这些信息就足够了。当一条消息的状态
发生变化时，就会在 message_status_history 表中插入一条记录。您可以查找消息的最新行来查找最
新状态，但这里创建了一个业务规则，用最新状态和更改时间来更新消息表中的缓存状态时间和缓存状
态 ID。这样，要返回消息的应用程序详细信息（需要历史记录时除外），只需查询消息表即可。您可
以通过应用程序或触发器更新缓存列，如果不需要缓存状态完全是最新的，也可以使用后台作业。

#+begin_comment
  使用一种命名方案，明确哪些数据是缓存数据，哪些不是。例如，可以用 cached_ 作为缓存表和列
  的前缀。
#+end_comment

** 直方图统计

回顾第 16 章，直方图统计是统计一列中每个值出现的频率。你可以利用这一点，将直方图统计数据用
作缓存。如果列最多有 1024 个唯一值，它就会发挥主要作用，因为这是支持的最大桶数，所以 1024
是可以使用单列直方图的最大值数。

清单 27-1 显示了一个使用直方图返回世界数据库中印度（CountryCode = IND）城市数量的示例。

#+begin_src sql
  analyze table world.city
    update histogram on CountryCode
      with 1024 buckets\G
#+end_src

#+begin_src sql
  select Bucket_Value, Frequency
  from (
    select (Row_ID-1) as Bucket_Number,
      SUBSTRING_INDEX(Bucket_Value, ':', -1)
        as Bucket_Value,
      (Cumulative_Frequency
       - LAG(Cumulative_Frequency, 1, 0)
          OVER(order by Row_ID))
       as Frequency
     from information_schema.COLUMN_STATISTICS
       inner join JSON_TABLE(
         histogram->'$.buckets',
         '$[*]' COLUMNS(
           Row_ID for orderinality,
  	 Bucket_Value varchar(42) PATH '$[0]',
  	 Cumulative_Frequency double PATH '$[1]'
        )
      ) buckets
  where SCHEMA_NAME='world'
        and TABLE_NAME='city'
        and COLUMN_NAME='CountryCode'
  ) stats
  where Bucket_Value='IND'
#+end_src

#+begin_src sql
  select TABLE_ROWS
  from information_schema.TABLES
  where TABLE_SCHEMA='world'
    and TABLE_NAME='city';
#+end_src

#+begin_src sql
  SELECT 0.08359892130424124*4188;
#+end_src

#+begin_src sql
  select COUNT(*)
  from world.city
  where CountryCode='IND';
#+end_src

如果你觉得针对 COLUMN_STATITICS 的查询看起来很熟悉，那么它就来自于第 16 章中列出单个直方图
的水桶信息时使用的查询。必须在子查询中收集直方图信息，否则无法计算频率。

您还需要知道总行数。您可以使用 information_schema.TABLES 视图中的近似值，或者缓存该表的
SELECT COUNT(*) 结果。在示例中，估计城市表有 4188 行（您的估计值可能不同），加上印度的频率，
表明表中大约有 350 个印度城市。精确计数显示有 341 个。偏差来自总行数估计值（城市表中有
4079 行）。

将直方图用作缓存主要适用于最多具有 1024 个唯一值的列的大型表，尤其是在列上没有索引的情况下。
这意味着它并不适合所有的使用案例。不过，它确实是一个跳出框框思考问题的例子--这在尝试寻找缓
存解决方案时非常有用。

** Memcached

Memcached 是一种简单但具有高度可扩展性的内存键值存储，作为一种缓存工具广受欢迎。传统上，它
主要用于网络服务器，但也可用于任何类型的应用程序。Memcached 的优势在于它可以分布在多个主机
上，这样就可以创建一个大型缓存。

#+begin_comment
  Memcached 仅在 Linux 和 Unix 上获得官方支持。
#+end_comment

在 MySQL 中使用 Memcached 有两种方法。可以使用常规的独立 Memcached，也可以使用 MySQL
InnoDB Memcached 插件。本节将展示使用这两种方式的一个简单示例。有关 Memcached 的完整文档，
请访问官方主页 https://memcached.org/ 和官方维基
https://github.com/memcached/memcached/wiki。

** 独立 Memcached

独立的 Memcached 是 https://memcached.org/ 的官方守护进程。它允许您将其用作分布式缓存，或
将缓存设置在离应用程序非常近的地方（可能在同一台主机上），从而降低查询缓存的成本。

安装 Memcached 有几种选择，包括使用操作系统的软件包管理器和从源代码编译。最简单的方法是使
用软件包管理器，例如在 Oracle Linux、Red Hat Enterprise Linux 和 CentOS 7 上：

#+begin_src shell
  sudo yum install memcached libevent
#+end_src

由于 memcached 需要 libevent 软件包，因此将其包含在内。在 Ubuntu Linux 上，该软件包名为
libevent-dev。您可能已经安装了 libevent 和/或 memcached，在这种情况下，软件包管理器会告诉
您无需进行任何操作。

您可以使用 memcached 命令启动守护进程。例如，使用所有默认选项启动它

#+begin_src shell
  memcached
#+end_src

如果在生产中使用，则应配置 systemd 或使用的任何服务管理器，以便在操作系统启动和关闭时启动
和停止守护进程。测试时，只需从命令行启动即可。

#+begin_comment
  Memcached 没有安全支持。将缓存数据限制为非敏感数据，确保 Memcached 实例只能在内部网络中
  使用，并使用防火墙限制访问。一种方法是将 Memcached 部署在与应用程序相同的主机上，并防止
  远程连接。
#+end_comment

现在，您可以使用 Memcached，将从 MySQL 获取的数据存储到缓存中。多种编程语言都支持
Memcached。在本讨论中，将使用 Python 的 pymemcache 模块1 和 MySQL Connector/ Python。清单
27-2 展示了如何使用 pip 安装模块。输出结果可能会有些不同，这取决于您使用的 Python 版本和已
经安装的版本，Python 命令的名称也取决于您的系统。在撰写本文时，pymemcache 支持 Python 2.7、
3.5、3.6 和 3.7。示例使用的 Python 3.6 是作为 Oracle Linux 7 的一个额外软件包安装的。

#+begin_src shell
  python3 -m pip install --user pymemcache
#+end_src

#+begin_src shell
  python3 -m pip install --user mysql-connector-python
#+end_src

在您的应用程序中，您可以通过一个键来查询 Memcached。如果键被找到，Memcached 将返回与该键一
起存储的值，如果没有找到，则需要查询 MySQL 并将结果存储到缓存中。清单 27-3 展示了一个查询
world.city 表的简单示例。该程序也可以在本书 GitHub 代码库中的 listing_27_3.py 文件中找到。
如果要执行该程序，需要更新 connect_args 中的连接参数，以反映连接到 MySQL 实例时使用的设置。

#+begin_src python
  from pymemcache.client.base import Client
  import mysql.connector

  connect_args={
      "user":"root",
      "password":"password",
      "host":"localhost",
      "port":3306,
  }

  db=mysql.connector.connect(**connect_args)
  cursor=db.cursor()
  memcache=Client(("localhost", 11211))

  sql="select CountryCode,Name from world.city where id=%s"
  city_id=130
  city=memcache.get(str(city_id))
  if city is not None:
      country_code, name=city.decode("utf-8").split("|")
      print("memcached: country: {0} - city: {1}".format(country_code, name))
  else:
      cursor.execute(sql, (city_id))
      country_code, name=cursor.fetchone()
      memcache.set(str(city_id), "|".join([country_code, name]), expire=60)
      print("MySQL: country: {0} - city: {1}".format(country_code, name))

  memcache.close()
  cursor.close()
  db.close()
#+end_src

程序一开始会同时创建 MySQL 和 memcached 守护进程的连接。在这种情况下，连接参数和要查询的
id 都是硬编码。在实际程序中，应从配置文件或类似文件中读取连接参数。

#+begin_comment
  切勿在应用程序中存储连接详情，尤其是切勿硬编码密码。在应用程序中存储连接详情既不灵活也不
  安全。
#+end_comment

然后，程序尝试从 Memcached 抓取数据；注意整数是如何转换为字符串的，因为 Memcached 使用字符
串作为键。如果键被找到，则通过在 | 字符处分割字符串，从缓存值中提取国家代码和名称。如果在
缓存中没有找到键，则从 MySQL 获取城市数据并存储在缓存中，缓存中保留该值的时间设置为 60 秒。
每种情况都会添加打印语句，以显示从哪里获取数据。

每次重启 memcached 后第一次执行程序时，它都会查询 MySQL：

#+begin_src sh
  python3 listing_27_3.py
#+end_src

在随后长达一分钟的执行中，数据将在缓存中找到：

#+begin_src sh
  python3 listing_27_3.py
#+end_src

完成对 Memcached 的测试后，可以在运行 memcached 的会话中使用 Ctrl+C 或向其发送 SIGTEM (15)
信号等方式停止 Memcached：

#+begin_src sh
  kill -s SIGTERM $(pidof memcached)
#+end_src

在本例中直接使用 Memcached 的优点是可以拥有一个守护进程池，而且可以在靠近应用程序的地方运
行守护进程，甚至可以与应用程序在同一台主机上运行。缺点是必须自己维护缓存。另一种方法是使用
MySQL 的 memcached 插件，它可以为您管理缓存，甚至可以自动持续写入缓存。

** MySQL的InnoDb Memcached插件

InnoDB Memcached 插件是在 MySQL 5.6 中引入的，用于访问 InnoDB 数据，而无需解析 SQL 语句。
该插件的主要用途是让 InnoDB 通过其缓冲池处理缓存，而仅使用 Memcached 作为查询数据的机制。
这样使用插件的一些好处是，写入插件的内容会被写入 InnoDB 的底层表，数据始终是最新的，而且可
以同时使用 SQL 和 Memcached 访问数据。

#+begin_comment
  在安装 MySQL innoDB Memcached 插件之前，请确保已停止独立 Memcached 进程，因为它们默认使
  用相同的端口。
#+end_comment

在安装 MySQL memcached 守护进程之前，必须确保已安装 libevent 软件包，就像独立安装
Memcached 一样。安装 libevent 后，需要安装 innodb_memcache 模式，其中包括用于配置的表。安
装的方法是获取 MySQL 发行版中的 share/innodb_memcached_config.sql 文件。该文件是相对于
MySQL 基本目录的，例如，您可以通过 basedir 系统变量找到该目录：

#+begin_src sql
  select @@global.basedir as basedir;
#+end_src

如果使用 https://dev.mysql.com/downloads/ 中的 RPM 安装了 MySQL，命令是

#+begin_src sql
  source /usr/share/mysql-8.0/innodb_memcached_config.sql
#+end_src

#+begin_comment
  请注意，该命令在 MySQL Shell 中不起作用，因为脚本中的 USE 命令没有分号，而 MySQL Shell
  在脚本中不支持分号。
#+end_comment

脚本还创建了 test.demo_test 表，该表将在接下来的讨论中使用。innodb_memcache 模式由三个表组
成：

+ cache_policies: 缓存策略的配置，用于定义缓存的工作方式。默认情况下使用 InnoDB。这通常是
  推荐的做法，可确保不会读取过期数据。
+ config_options: 插件的配置选项。其中包括返回多列值时使用的分隔符和表格映射分隔符。
+ containers: InnoDB 表映射的定义。您必须为所有要与 InnoDB memcached 插件一起使用的表添加
  映射。

containers表是最常用的表。默认情况下，该表包含 test.demo_test 表的映射：

#+begin_src sql
  select * from innodb_memcached.containers\G
#+end_src

在查询表时，可以使用该名称来引用由 db_schema 和 db_table 定义的表。key_columns 列定义了
InnoDB 表中用于键查找的列。你可以在 value_columns 列中指定要包含在查询结果中的列。如果包含
多列，则使用 config_options 表中配置的分隔符（默认为 |）来分隔列名。

cas_column 列和 expire_time_column 列很少用到，在此不再讨论。最后一列
unique_idx_name_on_key 是表中唯一索引的名称，最好是主键。

#+begin_comment
  有关表格及其使用的详细说明，请参见
  https://dev.mysql.com/doc/refman/en/innodb-memcachedinternals.html。
#+end_comment

现在您可以安装插件了。您可以使用 INSTALL PLUGIN 命令进行安装（请记住，该命令在 Windows 系
统上无效）：

#+begin_src sql
  install plugin daemon_memcached soname 'libmemcached.so';
#+end_src

该语句必须使用传统的 MySQL 协议（默认端口为 3306）执行，因为 X 协议（默认端口为 33060）不
允许安装插件。就是这样--InnoDB memcached 插件现在可以进行测试了。最简单的测试方法是使用
telnet 客户端。清单 27-4 展示了一个明确指定容器和使用默认容器的简单示例。

#+begin_src sh
  telnet localhost 11211
  get @@aaa.AA
  value @@aaa.AA 8 12
  HELLO, HELLO
  end
  get AA
  value AA 8 12
  HELLO, HELLO
  end
#+end_src

为了便于查看这两条命令，在每条命令前都插入了一行空行。第一条命令使用 @@ 在键值之前指定容器
名称。第二条命令依赖于 Memcached 使用默认容器（按容器名称字母升序排序时的第一个条目）。按
Ctrl+] 退出 telnet，然后按 quit 命令：

#+begin_src sh
  ^]
  quit 
#+end_src

与独立 Memcached 实例一样，守护进程默认使用 11211 端口。如果要更改端口或任何其他 Memcached
选项，可以使用 daemon_memcached_option 选项，该选项包含一个包含 memcached 选项的字符串。例
如，要将端口设置为 22222

#+begin_export ascii
  [mysqld]
  daemon_memcached_option="-p22222"
#+end_export

该选项只能在 MySQL 配置文件或命令行中设置，因此需要重启 MySQL 才能使更改生效。

如果在容器表中添加新条目或更改现有条目，则需要重启 memcached 插件，使其重新读取定义。为此，
您可以重启 MySQL 或卸载并安装插件：

#+begin_src sh
  uninstall plugin daemon_memcached;

  install plugin daemon_memcached soname 'libmemcached.so';
#+end_src

在实践中，您大多会在应用程序中使用该插件。如果您习惯使用 Memcached，那么使用方法就很简单。
例如，清单 27-5 显示了使用 pymemcache 模块的几条 Python 命令。请注意，该示例假定您已将端口
设置为 11211。

#+begin_src python
  from pymemcache.client.base import Client
  client=Client(('localhost', 11211))
  client.get('@@aaa.AA')
  client.set('@@aaa.BB', 'Hello World')
  client.get('@@aaa.BB')
#+end_src

交互式 Python 环境用于通过 memcached 插件查询 test.demo_test 表。创建连接后，使用 get() 方
法查询现有记录，并使用 set() 方法插入新记录。在这种情况下不需要设置超时，因为 set() 方法最
终会直接写入 InnoDB。最后，再次检索新记录。请注意，与需要自己维护缓存的常规 Memcached 相比，
这个示例是多么简单。

您可以通过在 MySQL 中查询来验证新记录是否真的插入到了表中：

#+begin_src sql
  select * from test.demo_test;
#+end_src

使用 MySQL InnoDB Memcached 插件还有更多内容。如果您计划使用它，建议您阅读
https://dev.mysql.com/doc/refman/en/innodb-memcached.html 参考手册中的 “InnoDB memcached
插件 ”部分。

** ProxySQL

ProxySQL 项目2 由 René Cannaò 创立，是一个高级代理，支持负载平衡、基于查询规则的路由、缓存
等功能。缓存功能根据查询规则进行缓存，例如，你可以设置要缓存具有给定摘要的查询。缓存会根据
你为查询规则设置的有效时间值自动过期。

您可以从 https://github.com/sysown/proxysql/releases/ 下载 ProxySQL。在撰写本文时，最新版
本是 2.0.8，也就是示例中使用的版本。

#+begin_comment
  proxySQL 仅受 Linux 正式支持。有关支持的发行版的完整文档（包括安装说明），请参见
  https:// github.com/sysown/proxysql/wiki。
#+end_comment

清单 27-6 显示了在 Oracle Linux 上使用 ProxySQL GitHub 代码库中的 RPM 安装 ProxySQL 2.0.8
的示例。在其他 Linux 发行版上，使用该发行版的软件包命令，安装过程也类似（当然，根据使用的
软件包命令，输出结果会有所不同）。安装完成后，启动 ProxySQL。

#+begin_src sh
  wget https://github.com/sysown/proxysql/releases/download/v2.0.8/ proxysql-2.0.8-1-centos7.x86_64.rpm
  sudo yum install proxysql-2.0.8-1-centos7.x86_64.rpm
  sudo systemctl start proxysql
#+end_src

只能通过 ProxySQL 的管理界面进行配置。该界面使用 mysql 命令行客户端，对 MySQL 管理员来说有
一种熟悉的感觉。默认情况下，ProxySQL 的管理界面使用 6032 端口，管理员用户名为 admin，密码
设为 admin。清单 27-7 显示了一个连接到管理界面并列出可用模式和表的示例。

#+begin_src sh
  mysql --host=127.0.0.1 --port=6032 \
        --user=admin --password \
        --default-character-set=utf8mb4 \
        --prompt='ProxySQL> '
#+end_src

#+begin_src sql
  show schemas;
  show tables;
#+end_src

虽然表是按模式分组的，但可以直接访问表，而无需引用模式。SHOW TABLES 的输出会显示主模式中与
ProxySQL 配置相关的表。

配置过程分为两个阶段，首先是准备新配置，然后是应用新配置。应用更改意味着将更改保存到磁盘
（如果要持久保存），并将其加载到运行线程中。

名称中带有 runtime_ 前缀的表用于向运行时线程推送配置。配置 ProxySQL 的一种方法是使用 SET
语句，类似于在 MySQL 中设置系统变量，但也可以使用 UPDATE 语句。第一步应该是更改管理员密码
（也可选择更改管理员用户名），可以通过设置 admin-admin_credentials 变量来完成，如清单 27-8
所示。

#+begin_src sql
  set admin-admin_crendentials='admin:password'
  save admin variables to disk;
  load admin variables to runtime;
  select @@admin-admin_crendentials;
#+end_src

admin-admin_credentials 选项的值是以冒号分隔的用户名和密码。SAVE ADMIN VARIABLES TO DISK
语句将持续更改，LOAD ADMIN VARIABLES TO RUNTIME 命令将更改应用到运行时线程。有必要将变量加
载到运行时线程中，因为出于性能考虑，ProxySQL 会在每个线程中保留一份变量副本。你可以像在
MySQL 中查询系统变量一样，查询当前值（无论是已应用还是待应用）。

在 mysql_servers 表中配置 ProxySQL 可用于引导查询的 MySQL 后端实例。在本讨论中，将使用与
ProxySQL 位于同一主机上的单个实例。清单 27-9 显示了如何将其添加到 ProxySQL 可以路由到的服
务器列表中。

#+begin_src sql
  show create table mysql_servers\G
#+end_src

#+begin_src sql
  insert into mysql_servers (hostname, port, use_ssl)
  values('127.0.0.1', 3306, 1);
  save mysql servers to disk;
  load mysql servers to runtime;
#+end_src

示例显示了如何使用 SHOW CREATE TABLE 获取 mysql_servers 表的信息。表的定义包括可以包含的设
置和允许值的信息。除主机名外，所有设置都有默认值。列表的剩余部分将为本地主机端口 3306 上的
MySQL 实例插入一行，并要求使用 SSL。然后将更改持久化到磁盘并加载到运行时线程中。

#+begin_comment
  SSL 只能在 proxySQL 与 MySQL 实例之间使用，而不能在客户端与 proxySQL 之间使用。
#+end_comment

您还需要指定哪些用户可以使用该连接。首先，在 MySQL 中创建一个用户：

#+begin_src sql
  create user myuser@'127.0.01' identified with mysql_native_password;
  grant all on world.* to myuser@'127.0.0.1';
#+end_src

ProxySQL 目前不支持缓存_sha2_password 身份验证插件（在 MySQL 8 中，使用 MySQL Shell 连接时
默认使用该插件）（但使用 mysql 命令行客户端时支持该插件），因此需要使用
mysql_native_password 插件创建用户。然后在 ProxySQL 中添加用户：

#+begin_src sql
  insert into mysql_users(username, password)
  values('myuser', 'password');
  save mysql users to disk;
  load mysql users to runtime;
#+end_src

现在可以通过 ProxySQL 连接到 MySQL。SQL 接口默认使用端口 6033。除了端口号和可能的主机名外，
通过 ProxySQL 的连接方式与通常相同：

#+begin_src sh
  mysqlsh --user=myuser --password \
  	--host=127.0.0.1 --port=6033 \
  	--sql --table \
  	-e "SELECT * FROM world.city WHERE ID = 130;"
#+end_src

ProxySQL 收集统计数据的方式与性能模式类似。你可以在 stats_mysql_query_digest 和
stats_mysql_query_digest_ reset 表中查询统计数据。这两个表的区别在于，后者只包含上次查询后
的摘要。例如，要获得按总执行时间排序的查询结果

#+begin_src sql
  select count_star, sum_time,
         digest, digest_text
  from stats_mysql_query_digest_reset
  order by sum_time desc \G
#+end_src

如果您想缓存查询结果，可以根据查询摘要添加查询规则。假设您想缓存按 ID（摘要
0x94656E0AA2C6D499）查询 world.city 表的结果，您可以添加如下规则：

#+begin_src sql
  insert into mysql_query_rules
         (active, digest, cache_ttl, apply)
  values (1, '0x94656E0AA2C6D499', 60000, 1);
  save mysql query rules to disk;
  load mysql query rules to runtime;
#+end_src

active 列指定 ProxySQL 在评估可使用的规则时是否应考虑该规则。digest 是要缓存的查询的摘要，
cache_ttl 则指定使用多长时间（以毫秒为单位）后结果才会被视为过期，并刷新结果。存活时间设置
为 60000 毫秒（1 分钟），以便在缓存失效前有时间执行几次查询。将 “应用 ”设为 1 意味着当查询
与此规则匹配时，将不会评估后面的规则。

如果在一分钟内执行几次查询，就可以查询 stats_mysql_global 表中的缓存统计数据，了解缓存的使
用情况。输出示例如下

#+begin_src sql
  select *
  from stats_mysql_glbal
  where Variable_Name like 'Query_Cache%';
#+end_src

您的数据很可能会有所不同。它显示缓存使用了 3659 字节，对缓存进行了六次查询，其中五次查询的
结果都是从缓存返回的。六次查询中的最后一次需要在 MySQL 后端执行查询。

您可以设置两个选项来配置缓存。它们是

+ mysql-query_cache_size_MB: 缓存的最大容量（兆字节）。这是一个软限制，由清除线程决定从缓
  存中清除多少查询。因此，内存使用量可能会暂时大于配置的大小。默认值为 256。

+ mysql-query_cache_stores_empty_result: 是否缓存没有记录的结果集。默认为 “true”。也可以在
  查询规则表中按查询进行配置。

更改配置的方式与之前更改管理员密码的方式类似。例如，要将查询缓存限制为 128 兆字节

#+begin_src sql
  set mysql-query_cache_size_MB=128;
  save mysql variables to disk;
  load mysql variables to runtime;
#+end_src

首先准备配置更改，然后将其保存到磁盘，最后将 MySQL 变量加载到运行线程中。

** 缓存提示

如果决定为 MySQL 实例实施缓存，有几件事需要考虑。本节将介绍一些一般的缓存技巧。

最重要的考虑因素是缓存什么。本章前面提到的缓存单行主键查找结果的示例并不能很好地说明哪类查
询最适合缓存。一般来说，查询越复杂、越昂贵、执行次数越多，就越适合使用缓存。有一种方法可以
使缓存更有效，那就是将复杂查询拆分成较小的部分。这样你就可以分别缓存复杂查询每个部分的结果，
从而使其更有可能被重复使用。

您还应该考虑查询返回的数据量。如果查询返回的结果集很大，您可能会为一次查询耗尽所有缓存内存。

另一个考虑因素是缓存的位置。缓存离应用程序越近，效率就越高，因为这样可以减少网络通信时间。
缺点是，如果有多个应用程序实例，就必须在复制缓存和远程共享缓存之间做出选择。如果需要在其他
MySQL 表中使用缓存数据，则属于例外情况。在这种情况下，最好以缓存表或类似形式将缓存保留在
MySQL 内部。

** 总结

本章概述了 MySQL 的缓存。它首先描述了缓存是如何从 CPU 内部到专用缓存进程随处可见的。然后讨
论了如何在 MySQL 内部使用缓存表和直方图进行缓存。

这两个主要部分讨论了使用 Memcached 和 ProxySQL 进行缓存。Memcached 是一种内存键值存储，你
可以在应用程序中使用它，也可以使用 MySQL 附带的特殊版本，它允许你直接与 InnoDB 交互。
ProxySQL 结合了路由器和缓存机制，可根据你定义的查询规则透明地存储结果集。

最后，还介绍了有关缓存的一些注意事项。执行查询的频率越高，执行查询的成本越高，就越能从缓存
中获益。第二个考虑因素是，缓存离应用程序越近越好。

MySQL 8 查询性能调优之旅的最后一章到此结束。希望这是一次收获颇丰的旅程，你已经准备好在工作
中使用这些工具和技术了。请记住，查询调优练习得越多，你就会做得越好。祝你查询调优愉快。












