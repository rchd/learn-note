
* 查询执行阶段

** 示例数据库
本书前几部分的示例基于只有少量行的简单表。本部分和后续部分涉及的是查询执行，在这方面要求更高：我们需要行数更多的相关
表。我没有为每个示例创建一个新的数据集，而是使用了一个现有的演示数据库，该数据库展示了俄罗斯的航空客运交通情况。它有
多个版本；我们将使用 2017 年 8 月 15 日创建的较大版本。要安装该版本，必须从压缩包中提取包含数据库副本的文件，并在
psql 中运行该文件。

在开发这个演示数据库时，我们努力使其模式足够简单，无需额外解释即可理解；同时，我们又希望它足够复杂，以便编写有意义
的查询。数据库中充满了真实的数据，这使得示例更加全面，使用起来也会很有趣。

在此，我将简要介绍主要的数据库对象；如果您想查看整个模式，可以参考脚注中的完整描述。

主要实体是预订（映射到预订表）。一个预订可以包括多个乘客，每个乘客都有一张单独的电子票（车票）。乘客并不是一个独立
的实体；在我们的实验中，我们将假设所有乘客都是唯一的。

每张机票包括一个或多个航段（映射到 ticket_flights 表）。在两种情况下，一张机票可以有多个航段：要么是往返机票，
要么是联程机票。虽然模式中没有相应的约束，但预订中的所有机票都被假定为具有相同的航班航段。

每个航班从一个机场飞往另一个机场。航班号相同的航班出发点和目的地相同，但出发日期不同。

航线视图以航班表为基础，显示不依赖于特定航班日期的航线信息。

办理值机手续时，每位旅客都会收到一张带有座位号的登机牌 (boarding_passes)。只有在机票中包含某个航班时，乘客才能
办理该航班的登机手续。航班座位组合必须是唯一的，因此不可能为同一个座位签发两张登机牌。

飞机上的座位（席位）数量及其在不同舱位之间的分布取决于执行飞行任务的飞机（机型）的具体型号。假设每种机型只能有一种客
舱配置。

有些表使用代理主键，有些则使用自然主键（其中有些是复合主键）。这只是为了演示，绝不是可以效仿的范例。

演示数据库可以看作是真实系统的转储：它包含过去某一特定时间的数据快照。要显示这个时间，可以调用 bookings.now() 函
数。在演示查询中使用该函数时，在现实生活中需要使用 now() 函数。

机场、城市和飞机型号的名称存储在 airports_data 表和 aircrafts_data 表中；它们以两种语言（英语和俄语）提供。在
构建本章示例时，我通常会查询实体关系图中显示的机场和飞机视图；这些视图会根据 bookings.lang 参数值选择输出语言。不
过，一些基础表的名称仍会出现在查询计划中。

[[./images/Gap5iC.png]]

** 简单查询协议
客户端-服务器协议的一个简单版本可以执行 SQL 查询：它向服务器发送查询文本，并获得完整的执行结果（无论包含多少行）。

解析

首先，PostgreSQL 必须解析查询文本，以了解需要执行的内容。

词法和句法分析。词法分析器将查询文本分割成一组词素（如关键字、字符串字面量和数字字面量），而解析器则根据 SQL 语言语
法验证这组词素。 PostgreSQL 依赖标准的解析工具，即 Flex 和 Bison 工具。

解析后的查询以抽象语法树的形式反映在后端内存中。

例如，我们来看看下面的查询：

#+begin_src sql
  select schemaname, tablename
  from pg_tables
  where tableowner='postgres'
  order by tablename;
#+end_src

词法选择器会选出五个关键字、五个标识符、一个字符串字面意义和三个单字母词素（逗号、等号和分号）。解析器使用这些词素
构建解析树，下图是一棵非常简化的解析树。树节点旁边的标题说明了查询的相应部分：

[[./images/TLkU80.png]]

RTE 是 Range Table Entry（范围表输入）的缩写，这个缩写相当晦涩难懂。PostgreSQL 源代码使用范围表一词来指表、
子查询、连接结果，换句话说，指任何可由 SQL 操作符处理的记录集。

语义分析。语义分析的目的是确定数据库中是否包含该查询名称所指的任何表或其他对象，以及用户是否有权限访问这些对象。语义
分析所需的所有信息都存储在系统目录中

收到解析树后，语义分析器会进行进一步重组，包括添加对特定数据库对象、数据类型和其他信息的引用。

如果启用 debug_print_parse 参数，就可以在服务器日志中查看完整的解析树，但这并没有什么实际意义。

变换

在下一阶段，可以对查询进行转换（重写）。

PostgreSQL 核心使用转换来实现几个目的。其中之一就是将解析树中的视图名称替换为与该视图的基本查询相对应的子树。

使用转换的另一种情况是行级安全实施。

递归查询的 SEARCH 和 CYCLE 子句也会在此阶段进行转换。

在上面的示例中，pg_tables 是一个视图；如果我们把它的定义放到查询文本中，就会如下所示：

#+begin_src sql
  select schemaname, tablename
  from (
     -- pg_tables
     select n.nspname as schemaname,
     c.relname as tablename,
     pg_get_userbyid(c.relowner) as tableowner,
     ...
     from pg_class c
       left join pg_namespace n on n.oid=c.relnamespace
       left join pg_tablespace t on t.oid=c.reltablespace
     where c.relkind=any(array['r'::char, 'p'::char])
   )
   where tableowner='postgres'
   order by tablename;
  
#+end_src

不过，服务器不会处理查询的文本表示；所有操作都是在解析树上执行的。图中显示的是已转换树的缩小版本（如果启用
debug_print_rewitten 参数，可以在服务器日志中查看完整版本）。

解析树反映了查询的语法结构，但并不说明执行操作的顺序。

PostgreSQL 还支持自定义转换，用户可通过重写规则系统实现自定义转换。

[[./images/7tfzd1.png]]

规则系统支持被宣布为 Postgres 开发的主要目标之一；在规则首次实施时，它还是一个学术项目，但从那时起，它们已经被重
新设计了多次。规则系统是一个非常强大的机制，但却很难理解和调试。甚至有人提议从 PostgreSQL 中完全删除规则，但这一
想法并未得到一致支持。在大多数情况下，使用触发器而不是规则更安全、更简单。

规划器

SQL 是一种声明式语言：查询指定要获取哪些数据，但不指定如何获取。

任何查询都有多种执行路径。解析树中显示的每个操作都可以通过多种方式完成：例如，可以通过读取整个表（并过滤掉冗余）或
通过索引扫描查找所需记录来检索结果。数据集总是成对连接的，因此在连接顺序上有大量不同的选择。此外，还有各种连接算法：
例如，执行器可以扫描第一个数据集的行，然后在另一个数据集中搜索匹配的行，或者先对两个数据集进行排序，然后合并在一起。
对于每种算法，我们都能找到其性能优于其他算法的使用案例。


最优计划和非最优计划的执行时间可能相差几个数量级，因此优化解析查询的规划器是系统中最复杂的组件之一。

计划树。执行计划也用树形表示，但其节点处理的是数据上的物理操作，而不是逻辑操作。

如果想查看完整的计划树，可以启用 debug_print_plan 参数将其转储到服务器日志中。但实际上，查看 EXPLAIN 命令显示
的计划文本就足够了。

现在，让我们注意以下两点：

+ 计划树中的三个查询表只包含两个：规划器发现其中一个表不需要用于检索结果，因此将其从计划树中删除。
+ 在树的每个节点上，规划器都会提供估计成本和预计要处理的行数。

#+begin_src sql
  explain select schemaname, tablename
  from pg_tables
  where tableowner='postgres'
  order by tablename;
#+end_src

查询计划中显示的 Seq Scan 节点对应于读表，而 Nested Loop 节点表示连接操作。

[[./images/qlf7ma.png]]


计划搜索。PostgreSQL 使用基于成本的优化器；它会查看潜在的计划，并估算执行这些计划所需的资源（如 I/O 操作或 CPU
周期）。这一估算值被归一化为一个数值，称为计划成本。在所有考虑过的计划中，选择成本最低的一个。

问题在于，潜在可用计划的数量会随着连接表的数量呈指数增长，因此不可能考虑所有计划，即使是相对简单的查询也是如此。通
常使用动态规划算法结合一些启发式方法来缩小搜索范围。这样，规划器就能在可接受的时间内为表数较多的查询找到数学上精确
的解决方案。

#+begin_comment
  精确的解决方案并不能保证所选计划确实是最优的，因为规划者使用的是简化的数学模型，而且可能缺乏可靠的输入数据。
#+end_comment


管理连接顺序。查询的结构可以在一定程度上限制搜索范围（有可能错过最佳计划）。

+ 普通表表达式和主查询可以分开优化；为保证这种行为，可以指定 MATERIALIZED 子句。
+ 在非 SQL 函数中运行的查询总是单独进行优化。(SQL 函数有时可以内联到主查询中。）
+ 如果设置了 join_collapse_limit 参数并在查询中使用了显式 JOIN 子句，那么某些连接的顺序将由查询语法结构来定义
  from_collapse_limit 参数对子查询也有同样的作用。


后一点可能需要解释一下。让我们看看没有为 FROM 子句中列出的表指定任何显式连接的查询：

#+begin_src sql
  select ...
  from a, b, c, d, e
  where ...
#+end_src

在这里，规划器必须考虑所有可能的连接对。查询由解析树的以下部分表示（如图所示）：

[[./images/FyoxVR.png]]

在下一个示例中，连接具有由 JOIN 子句定义的特定结构：

#+begin_src sql
  select ...
  from a, b join c on ..., d, e
  where ...
#+end_src

解析树反映了这种结构：

[[./images/EluhJc.png]]

规划器通常会将连接树扁平化，使其看起来像第一个示例中的树。该算法会递归遍历连接树，并将每个 JOINEXPR 节点替换为其
元素的扁平列表。

但是，只有当生成的扁平列表中的元素数量不超过 join_collapse_limit 时，才会执行这种折叠操作。在这种特殊情况下，
如果 join_collapse_limit 值小于 5，JOINEXPR 节点就不会折叠。

对于规划器来说，这意味着以下几点：

+ 表 B 必须与表 C 连接（反之亦然，C 必须与 B 连接；表对中的连接顺序不受限制）。
+ A、D、E 以及连接 B 和 C 的结果可以任意顺序连接。

如果 join_collapse_limit 参数设置为 1，显式 JOIN 子句定义的顺序将得到保留。

至于全外连接操作数，无论 join_collapse_limit 参数的值如何，它们都不会折叠。

from_collapse_limit 参数以类似的方式控制子查询的扁平化。虽然子查询看起来并不像 JOIN 子句，但其相似性在解析树层
次上却很明显。

下面是一个查询示例：

#+begin_src sql
  select ...
  from a,
  ( select ... from b, c where ...) bc, d, e
  where ...
#+end_src

相应的连接树如下所示。唯一不同的是，这棵树包含的是 FROMEXPR 节点，而不是 JOINEXPR（参数名称由此而来）。

[[./images/xJvniE.png]]

遗传查询优化。一旦扁平化，树的一个层次可能包含过多元素--无论是表还是连接结果，都必须分别进行优化。规划时间与需要连
接的数据集数量成指数关系，因此其增长可能超出所有合理的限制。

如果启用了 geqo 参数，且某一级元素数量超过了 geqo_threshold 值，规划器将使用遗传算法优化查询。这种算法比动态编
程算法快得多，但不能保证找到的计划是最优的。因此，经验法则是通过减少需要优化的元素数量来避免使用遗传算法。

遗传算法有几个可配置的参数，但我不打算在这里介绍。

选择最佳计划。计划是否最优取决于特定客户将如何使用查询结果。如果客户需要一次性获得全部结果（例如，创建一份报告），
则计划应优化所有行的检索。但如果优先级是尽快返回第一行（例如，在屏幕上显示），最佳计划可能完全不同。

要做出这种选择，PostgreSQL 会计算成本的两个组成部分：

#+begin_src sql
  explain
  select schemaname, tablename
  from pg_tables
  where tableowner='postgres'
  order by tablename;
#+end_src

第一部分（启动成本）是您为准备节点执行所支付的费用，而第二部分（总成本）则包括获取结果所产生的所有费用。

#+begin_comment
  有时人们会说，启动成本就是检索结果集第一行的成本，但这并不十分准确。
#+end_comment

为了选出首选计划，优化器会检查查询是否使用了游标（通过 SQL 中提供的 DECLARE 命令或在 PL/pgSQL 中明确声明）。如
果没有，则假定客户需要一次性得到全部结果，优化器会选择总成本最低的计划。

如果查询是使用游标执行的，所选计划必须只优化检索所有记录中的 cursor_tuple_fraction。更准确地说，PostgreSQL 会
选择以下表达式中值最小的计划：

#+begin_center
 startup cost+cursor_tuple_fraction(total cost - startup cost)
#+end_center

成本估算概要。要估算一个计划的总成本，我们必须对其所有节点进行成本估算。节点的成本取决于其类型（显然，读取堆数据的
成本与排序成本不同）和节点处理的数据量（数据量越大，成本通常越高）。虽然节点类型是已知的，但数据量只能根据输入集的
估计卡入度（节点作为输入的行数）和节点的选择性（输出中剩余行的比例）来预测。这些计算依赖于收集到的统计数据，如表格
大小和表格列中的数据分布。

因此，所进行的优化取决于自动真空收集和更新的统计数据的正确性。

如果对每个节点的Cardinality估计准确，计算出的成本就有可能充分反映实际成本。主要的规划缺陷通常源于对中心性和选
择性的不正确估计，其原因可能是统计数据不准确或过时、无法使用统计数据，或在较小程度上源于规划模型不完善。

Cardinality估算 要计算节点的卡性，规划器必须递归完成以下步骤：

1. 估算每个子节点的Cardinality，并评估节点将从它们那里接收的输入行数。
2. 估算节点的Selectivity，即输入行中能保留到输出的部分。

节点的Cardinality是这两个值的乘积。

#+begin_comment
  数字越小，选择性越高；反之亦然，数字越接近 1，选择性越低。这看似不合逻辑，但我们的想法是，选择性高的条件几乎会
  剔除所有行，而只剔除少数行的条件选择性低。
#+end_comment

首先，规划器会估算定义数据访问方法的叶节点的Cardinality。这些计算依赖于收集到的统计数据，例如表的总大小。

过滤条件的Selectivity取决于其类型。在最普通的情况下，它可以被假定为一个恒定值，不过规划器会尽量利用所有可用信
息来完善估算。一般来说，只要知道如何估算简单的过滤条件就足够了；如果条件包含逻辑运算，其选择性可通过以下公式计算：

#+begin_export latex
sel_{x and y} =sel_xsel_y
sel_{x or y}=1-(1-sel_x)(1-sel_y)=sel_x+sel_y-sel_xsel_y
#+end_export

遗憾的是，这些公式假设谓词 x 和 y 互不相关。对于相关的谓词，这样的估算将是不准确的。

为了估算连接的Cardinality ，规划器必须获取笛卡尔积（即两个数据集的Cardinality之积）的Cardinality，并估算
连接条件的选择性，这同样取决于条件类型。

其他节点（如排序或聚合）的 Cardinality 也是以类似方式估算的。

值得注意的是，对较低计划节点的Cardinality估计不正确会影响所有后续计算，导致总成本估计不准确和计划选择错误。更糟
糕的是，规划器没有关于连接结果的统计数据，只有关于表的统计数据。

成本估算。估算成本的过程也是递归的。要计算子树的成本，需要计算并汇总其所有子节点的成本，然后再加上父节点本身的成本。

要估算一个节点的成本，PostgreSQL 会将已估算出的节点Cardinality作为输入，应用该节点所执行操作的数学模型。对于
每个节点，都会计算启动成本和总成本。

有些操作没有先决条件，因此会立即开始执行；这些节点的启动成本为零。

相反，其他操作则需要等待一些初步操作完成。例如，排序节点通常需要等待来自其子节点的所有数据，然后才能继续执行自己的
任务。这类节点的启动成本通常高于零：即使上述节点（或客户端）只需要全部输出中的一行，也必须支付这一代价。

规划器进行的所有计算都只是估算，可能与实际执行时间无关。它们的唯一目的是在相同条件下对同一查询的不同计划进行比较。
在其他情况下，比较查询（尤其是不同查询）的成本毫无意义。例如，由于统计数据过时，成本可能被低估了；一旦统计数据被刷
新，计算出的数字可能会上升，但由于估算变得更加准确，服务器会选择更好的计划。

执行

现在必须执行查询优化过程中建立的计划。

执行器会在后端内存中打开一个门户，这是一个保存当前正在执行的查询状态的对象。该状态用一棵树来表示，它重复了计划树的
结构。该树的节点像流水线一样运行，相互请求和发送记录。

[[./images/YFSvIs.png]]


查询从根节点开始执行。根节点（在本例中代表 SORT 操作）从其子节点获取数据。收到所有行后，它会对其进行排序，并将其
传递给客户端。

有些节点（如图中所示的 NESTLOOP 节点）会连接从不同来源接收的数据集。这样的节点从两个子节点中提取数据，并在接收到
一对满足连接条件的记录后，立即将得到的记录向上传递（与排序不同，排序必须先得到所有记录）。此时，节点的执行将被中断，
直到其父节点请求下一条记录。如果只需要部分结果（例如，查询中有 LIMIT 子句），则不会执行全部操作。

树中的两个 SEQSCAN 叶节点负责表格扫描。当父节点向这些节点请求数据时，它们会从相应的表中获取后续行。

因此，有些节点不存储任何行，而是立即向上传递，但其他节点（如 SORT）则必须保存潜在的大量数据。为此，会在后端内存中
分配一个 work_mem 块；如果不够用，剩余的数据就会溢出到磁盘上的临时文件中。

一个计划可能有多个节点需要数据存储，因此 PostgreSQL 可能会分配多个内存块，每个内存块的大小为 work_mem。查询可使
用的总内存大小不受任何限制。

** 扩展查询协议

在使用简单查询协议时，每条命令（即使重复多次）都必须经过上述所有阶段：

1. 解析
2. 转换
3. 规划器
4. 执行器

然而，重复解析同一个查询毫无意义。重复解析仅常量不同的查询也没有多大意义--解析树结构仍然保持不变。

简单查询协议的另一个缺点是，无论查询结果包含多少行，客户端都会一次性收到全部结果。

一般来说，使用 SQL 命令可以克服这些限制。要解决第一个问题，可以在运行 EXECUTE 命令前准备查询；第二个问题可以通过
使用 DECLARE 创建游标并通过 FETCH 返回记录来解决。但在这种情况下，这些新创建对象的命名必须由客户端来处理，而服务
器则要承担解析额外命令的额外开销。

扩展的客户端-服务器协议提供了另一种解决方案，可以在协议本身的命令层对独立的操作员执行阶段进行精确控制。

准备工作

在准备阶段，查询会像往常一样被解析和转换，但生成的解析树会保存在后端内存中。

PostgreSQL 没有查询的全局缓存。这种架构的缺点显而易见：每个后端都必须解析所有传入的查询，即使同一个查询已经被另一
个后端解析过了。但也有一些好处。由于锁的存在，全局缓存很容易成为瓶颈。 一个客户端运行多个小而不同的查询（比如只有常
量变化的查询）会产生大量流量，并对整个实例的性能产生负面影响。在 PostgreSQL 中，查询是在本地解析的，因此不会影响
其他进程。

准备好的查询可以被参数化。下面是一个使用 SQL 命令的简单示例（虽然与协议级的准备不同，但最终效果是一样的）：

#+begin_src sql
  prepare plane(text) as
  select * from aircrafts where aircrafts_code=$1;
#+end_src

所有已命名的预处理语句都显示在 pg_prepared_statements 视图中：

#+begin_src sql
  select name, statement, parameter_types
  from pg_prepared_statments \gx
#+end_src

在这里找不到任何未命名的语句（使用扩展查询协议或 PL/pgSQL 的语句）。其他后端编写的语句也不会显示：因为无法访问其
他会话的内存。

参数绑定

在执行准备语句之前，必须绑定实际参数值。

#+begin_src sql
  execute plane('733');
#+end_src

在预处理语句中绑定参数，而不是将文字与查询字符串连接起来，其优点是绝对不可能发生 SQL 注入：绑定的参数值无法以任何
方式修改已构建的解析树。如果不使用预处理语句，要达到相同的安全级别，就必须仔细剔除从不可信来源接收的每个值。

规划和执行

在执行准备语句时，查询计划是根据实际参数值执行的，然后将计划传递给执行器。

不同的参数值可能意味着不同的最优计划，因此必须考虑到确切的参数值。例如，在查找价格昂贵的预订时，计划器会假设匹配的
行数不多，并使用索引扫描：

#+begin_src sql
  create index on bookings(total_amount);
  explain select * from bookings
  where total_amount > 1000000;
#+end_src

但如果所有预订都满足所提供的条件，就没有必要使用索引，因为必须扫描整个表：

#+begin_src sql
  explain select * from bookings where total_amount > 100;
#+end_src

在某些情况下，规划器可能会同时保留解析树和查询计划，以避免重复规划。这种计划不考虑参数值，因此称为通用计划（与基于
实际值的自定义计划相比）。服务器可以使用通用计划而不影响性能的一个明显例子是不带参数的查询。

参数化准备语句的前五次优化始终依赖于实际参数值；规划器根据这些值计算自定义计划的平均成本。从第六次执行开始，如果通
用计划的平均效率高于自定义计划（考虑到自定义计划每次都要重新创建），规划器将保留通用计划并继续使用，跳过优化阶段。

plane准备语句已经执行过一次。在接下来的三次执行之后，服务器仍然使用自定义计划--你可以从查询计划中的参数值看出这
一点：

#+begin_src sql
  execute plane('763');
  execute plane('773');
  explain execute plane('319');
#+end_src

执行第五次后，规划器切换到通用计划：通用计划与自定义计划没有区别，成本也相同，但后台可以一次生成，跳过优化阶段，从
而减少规划开销。现在，"explain"命令显示，参数是按位置而不是按值引用的：

我们不难想象，当最初的几个定制计划比通用计划更昂贵时，就会出现令人不快的情况；随后的计划本可以更有效率，但规划器根
本不会考虑这些计划。此外，它比较的是估算成本而不是实际成本，这也会导致计算错误。

不过，如果规划器出错，您可以覆盖自动决定，并通过相应设置自动计划缓存模式参数来选择通用计划或自定义计划：

#+begin_src sql
  set plan_cache_mode='force_custom_plan';
  explain execute plane('CN1');
#+end_src

其中，pg_prepared_statements 视图显示所选计划的统计数据：

#+begin_src sql
  select name, generic_plans, custom_plans
  from pg_prepared_statements;
#+end_src

取得成果

扩展查询协议允许分批而非一次性检索数据。SQL 游标具有几乎相同的效果（只是服务器需要做一些额外的工作，而且规划器会优
化获取第一个 cursor_tuple_fraction 行，而不是整个结果集）：

#+begin_src sql
  begin;
  declare cur cursor for
  select *
  from aircrafts
  order by aircraft_code;
  fetch 3 from cur;
  fetch 2 from cur;
  commit;
#+end_src

如果查询返回许多行，而客户端需要所有行，那么系统吞吐量在很大程度上取决于批量大小。批次中的记录越多，访问服务器和获取
响应所产生的通信开销就越少。但是，随着批量大小的增加，这些优势就变得不那么明显了：虽然逐条获取记录和批量获取 10 条
记录之间的差异可能很大，但如果比较 100 条和 1000 条记录的批量，差异就不那么明显了。

* 统计数据

** 基本的统计数据
基本的表相关数据存储在catalog目录的pg_class包含如下数据

+ 表中所有元组数(reltuples)
+ 表大小，以页为单位(relpages)
+ 在可见性中标记的页数量(relallvisible)


flights表的在pg_class中的数据
  
#+begin_src sql
  select reltuples, relpages, relallvisible
  from pg_class where relname='flights';
#+end_src

如果查询不附加任何过滤条件，则以 reltuples 值作为估计的Cardinality

#+begin_src sql
  explain select * from flights;
#+end_src

在手动和自动的表格分析过程中都会收集统计数据。此外，由于基本统计数据至关重要，因此在进行其他一些操作（VACCUM FULL
和 CLUSTER、CREATE INDEX 和 REINDEX）时也会计算这些数据，并在抽清理时进行完善。

为便于分析，将对 300×100 default_statistics_target 随机行进行采样。建立特定精度的统计数据所需的样本大小与分
析数据量的关系不大，因此不考虑表的大小。

取样行是从相同数量（300 × default_statistics_target）的随机页中抽取的。显然，如果表格本身较小，读取的页数可能
较少，被选中进行分析的行数也较少。

在大型表格中，统计数据收集并不包括所有行，因此估计值可能与实际值有偏差。这很正常：如果数据不断变化，统计数据无论如
何也不可能一直准确。通常一个数量级的精确度就足以选择一个合适的计划。

让我们创建一个禁用autovacuum的flights副本，以便控制自动分析的启动时间：

#+begin_src sql
  create table flights_copy(like flights)
  with (autovacuum_enable = false);
#+end_src

目前数据的表的统计

#+begin_src sql
  select reltuples, relpages, relallvisible
  from pg_class where relname='flights_copy';
#+end_src

reltuples = -1 的值用于区分尚未分析的表和没有任何行的真正空表。

在创建表格后，很可能会立即向表中插入一些行。因此，在不了解当前情况的情况下，规划器假设表包含 10 页：

#+begin_src sql
  explain select * from flights_copy;
#+end_src

行数是根据单行的大小估算的，在计划中显示为宽度。行宽通常是在分析过程中计算出的平均值，但由于尚未收集统计数据，这里
只是根据列数据类型进行近似估算。

现在，让我们从flights表中复制数据并进行分析：

#+begin_src sql
  insert into flights_copy select * from flights;
  analyze flights_copy;
#+end_src

收集的统计数据反映了实际行数（表的大小足够小，分析仪可以收集所有数据的统计数据）：

#+begin_src sql
  select reltuples, relpages, relallvisible
  from pg_class where relname='flights_copy';
#+end_src

relallvisible 值用于估算仅索引扫描的成本。该值由 VACUUM 更新：

#+begin_src sql
  vaccum flights_copy;
  select relallvisible from pg_class where relname='flights_copy';
#+end_src

现在，让我们在不更新统计信息的情况下将记录数增加一倍，并检查查询计划中的Cardinality估计值：

#+begin_src sql
  insert into flights_copy select * from flights;
  select count(*) from flights_copy;
  explain select * from flights_copy;
#+end_src

尽管 pg_class 数据已经过时，但估算结果仍然是准确的：

#+begin_src sql
  select reltuples, relpages
  from pg_class where relname='flights_copy';
#+end_src

由于文件大小比 relpages 增加了一倍，规划器会在假设数据密度不变的情况下调整估计的行数：

#+begin_src sql
  select reltuples *
  (pg_relation_size('flights_copy') / 8192) / relpages as tuples
  from pg_class where relname='flights_copy';
#+end_src

当然，这种调整不一定总能奏效（例如，如果我们删除一些行，估计结果将保持不变），但在某些情况下，它可以让规划器坚持到
重大变化触发下一次分析运行。

** NULL 值
虽然理论家们对 NULL 值嗤之以鼻，但它在关系数据库中仍然发挥着重要作用：它提供了一种方便的方法来反映一个未知或不存在
的值。

然而，特殊的价值需要特殊的处理方法。除了理论上的不一致性，我们还必须考虑多种实际挑战。常规的布尔逻辑被三值逻辑所取
代，因此 NOT IN 的表现出乎意料。不清楚 NULL 值应被视为大于还是小于常规值（因此在排序时使用了 NULLS FIRST 和
NULL LAST 子句）。聚合函数是否必须考虑 NULL 值，这一点也不太清楚。严格来说，NULL 值根本就不是值，因此规划器需要
额外的信息来处理它们。

除了在表级别收集最简单的基本统计信息外，分析器还收集关系中每一列的统计信息。这些数据存储在系统目录的 pg_statistic
表中，但也可以通过 pg_stats 视图访问，该视图以更方便的格式提供这些信息。

NULL值比例属于列级统计；在分析过程中计算得出，显示为 null_frac 属性。

例如，在搜索尚未起飞的航班时，我们可以依赖于其起飞时间未定义：

#+begin_src sql
  explain select * from flights where actual_departure is null;
#+end_src

为了估算结果，规划器会将总行数乘以 NULL 值的比例：

#+begin_src sql
  select round(reltuples * s.null_frac) as rows
  from pg_class
  join pg_stats s on s.tablename=relname
  where s.tablename='flights'
  and s.attname='actual_departure';
#+end_src

下面是实际行数：

#+begin_src sql
  select count(*) from flights where actual_departure is null;
#+end_src

** 不重复值
pg_stats 视图的 n_distinct 字段显示列中不同值的数量。

如果 n_distinct 为负数，其绝对值表示列中唯一值的比例，而不是实际的列数。例如，-1 表示所有列中的值都是唯一的，而
-3 则表示每个值平均出现在三行中。如果估计的唯一值数量超过总行数的 10%，分析器就会使用分数；在这种情况下，进一步的
数据更新不太可能改变这一比例。

[[./images/rWsuJI.png]]

如果预期数据分布均匀，则使用不同值的数量来代替。例如，在估算 "列 = 表达式 "条件的Cardinality数时，如果表达式
的确切值在规划阶段未知，规划器就会假设表达式可以以相同概率取任意列值：

#+begin_src sql
  explain select *
  from flights
  where depatrue_airport=(
  select airport_code from aiprots where city='Saint Petersburg'
  );
#+end_src

在这里，InitPlan 节点只执行一次，计算值将用于主计划。

#+begin_src sql
  select round(reltuples / s.n_distinct) as rows
  from pg_class
  join pg_stat s on s.tablename=relname
  where s.tablename='flights'
  and s.attname='departure_airport';
#+end_src

如果估计的不同值数量不正确（因为分析的行数有限），则可以在列级覆盖：

#+begin_src sql
  alter table ...
      alter column ...
      set (n_distinct = ...);
#+end_src

如果所有数据总是均匀分布，那么这些信息（加上最小值和最大值）就足够了。然而，对于非均匀分布（这在实践中更为常见），
这种估计是不准确的：

#+begin_src sql
  select min(cnt), round(avg(cnt)) avg, max(cnt)
  from (
  select depature_airport, count(*) cnt
  from flights
  group by depature_airport
  )t;
#+end_src

** most_common_vals值
如果数据分布不均匀，则会根据最常见值 (MVC) 及其频率的统计数据对估算进行微调。pg_stats 视图会在 most_common_vals
和 most_common_freqs 字段中分别显示这些数组。

以下是有关各类飞机的此类统计数据的示例：

[[./images/roYhLU.png]]

#+begin_src sql
  select most_common_vals as mcv,
  left(most_common_freqs::text, 60) || '...' as mcf
  from pg_stats
  where tablename='flights' and attname='aircraft_code' \gx
#+end_src

要估算 "列 = 值 "条件的选择性，只需在 most_common_vals 数组中找到该值，并从具有相同索引的 most_common_freqs
数组元素中提取其频率即可：

#+begin_src sql
  explain select * from flights where aircraft_code = '733';
#+end_src

#+begin_src sql
  select round(reltuples * s.most_common_freqs[
  array_position((s.most_common_vals::text::text[]), '733')
  ])
  from pg_class
  join pg_stat s on s.tablename=relname
  where s.tablename='flights'
  and s.attname='aircraft_code';
#+end_src

显然，这种估算将接近实际值：

#+begin_src sql
  select count(*) from flights where aircraft_code='733';
#+end_src

MCV 列表还用于估算不等式条件的选择性。例如，"列 < 值 "这样的条件要求分析器在 most_common_vals 中搜索所有小于目
标值的值，并求和 most_common_freqs 中列出的相应频率。

当独立值不多时，MCV 统计效果最佳。数组的最大大小由 default_statistics_target 参数定义，该参数还限制了为分析目
的而随机抽样的行数。

在某些情况下，可以增加默认参数值，从而扩大 MCV 列表并提高估算的准确性。您可以在列级别上这样做：

#+begin_src sql
  alter table ...
     alter column ...
     set statistics ...;
#+end_src

样本量也会增加，但仅限于指定的表格。

由于 MCV 数组存储的是实际值，因此可能会占用大量空间。为了控制 pg_statistic 的大小，避免给计划器增加无用功，大于
1 KB 的值将被排除在分析和统计之外。不过，由于这些大值很可能是唯一的，因此无论如何，它们都不可能进入
most_common_vals。

** 柱状图

如果差异值过多，无法存储在数组中，PostgreSQL 会使用直方图。在这种情况下，数值会分布在直方图的几个桶中。桶的数量也
受 default_statistics_target 参数的限制。

水桶宽度的选择方式是让每个水桶都能获得大致相同数量的数值（这一特性在图表中体现为大阴影矩形的面积相等）。包含在 MCV
列表中的值不计算在内。因此，每个水桶中数值的累积频率等于 $$ \frac{1}{number of buckets} $$

直方图以桶边界值数组的形式存储在 pg_stats 视图的 histogram_bounds 字段中：

#+begin_src sql
  select left(histogram_bounds::text, 60) || '...' as hist_bounds
  from pg_stats s
  where s.tablename='boarding_passes' and s.attname='seat_no';
#+end_src

结合 MCV 列表，直方图可用于估算大于和小于条件的选择性等操作。例如，我们来看看为后排签发的登机牌数量：

[[./images/HQf3Es.png]]

#+begin_src sql
  explain select * from boarding_passes where seat_no > '30B';
#+end_src

我有意选择了位于两个直方图桶边界上的座位号。

该条件的选择性将以 N 个桶的数量来估算，其中 N 是容纳满足条件的值（即位于指定值右侧的值）的桶的数量。还必须考虑到
MCV 不包括在直方图中。

顺便提一下，直方图中也不会出现 NULL 值，但 seat_no 列中无论如何也不会包含此类值：

#+begin_src sql
  select s.null_frac from pg_stats s
  where s.tablename='boarding_passes' and s.attname='seat_no';
#+end_src

首先，让我们找出满足条件的 MCV 分数：

#+begin_src sql
  select sum(s.most_common_freqs[
  array_position((s.most_common_vals::text::text[]), v)
  ])
  from pg_stats s, unnest(s.most_common_vals::text::[]) v
  where s.tablename='boarding_passes' and s.attname='seat_no'
  and v> '30B';
#+end_src

MCV 的总体份额（被柱状图忽略）为

#+begin_src sql
  select sum(s.most_common_freqs[
  array_position((s.most_common_vals::text::text[]), v)
  ])
  from pg_stats s, unnest(s.most_common_vals::text::text[]) v
  where s.tablename='boarding_passes' and s.attname='seat_no';
#+end_src

由于符合指定条件的值正好占 𝑁 个桶（可能有 100 个桶），我们可以得到以下估计值：

#+begin_src sql
  select round(reltuples *(
  0.21226657 -- MCV share
  +(1-0.67816657-0)*(51/100.0) -- histogram share
  ))
  from pg_class
  where relname='boarding_passes';
#+end_src

[[./images/RiqJUz.png]]

在非边界值的一般情况下，规划器采用线性插值，以考虑到包含目标值的桶的部分。

这是后排座椅的实际数量：

#+begin_src sql
  select count(*) from boarding_passes where seat_no>'30B';
#+end_src

随着 default_statistics_target 值的增加，估算精度可能会提高，但正如我们的示例所示，即使列中包含许多唯一值，直
方图与 MCV 列表相结合通常也能得到很好的结果：

#+begin_src sql
  select n_distinct from pg_stats
  where tablename='boarding_passes' and attname='seat_no';
#+end_src

只有当提高估算精度能带来更好的规划时，提高估算精度才有意义。不假思索地增加 default_statistics_target 值可能会
减慢规划和分析速度，却不会带来任何好处。也就是说，降低该参数值（降为零）可能会导致错误的计划选择，尽管这样做确实会
加快规划和分析速度。这种节省通常是不合理的。

** 非标量数据类型的统计数据

对于非标量数据类型，PostgreSQL 不仅可以收集值分布的统计数据，还可以收集用于构建这些值的元素分布的统计数据。当你查
询不符合第一正则表达式的列时，它可以提高规划的准确性。

+ most_common_elems 和 most_common_elem_freqs 数组显示最常见的元素列表及其使用频率。
  收集的这些统计数据将用于估算数组和 tsvector 数据类型操作的选择性。
+ elem_count_histogram 数组显示数值中不同元素数量的直方图。
  收集的这些数据仅用于估算阵列操作的选择性。
+ 对于范围类型，PostgreSQL 会为范围长度和范围的上下限建立分布直方图。这些直方图用于估计对这些类型进行各种操作的选
  择性，但 pg_stats 视图不会显示它们。

  还为多范围数据类型收集了类似的统计数据。


** 平均字段宽度

pg_stats 视图的 avg_width 字段显示了存储在列中的值的平均大小。当然，对于整数或 char(3) 等类型，这个大小总是相
同的，但对于长度可变的数据类型（如文本），每一列的大小可能会有很大差异：

#+begin_src sql
  select attname, avg_width from pg_stats
  where (tablename, attname) in (values
  ('tickets', 'passenger_name'), ('ticket_flights', 'fare_conditions')
  );
#+end_src

该统计量用于估算排序或散列等操作所需的内存量。

** 相关性

pg_stats 视图的相关性字段显示数据的物理顺序与比较操作定义的逻辑顺序之间的相关性。如果数值严格按照升序存储，其相关
性将接近 1；如果按照降序排列，其相关性将接近-1。磁盘上的数据分布越混乱，相关性就越接近零。

#+begin_src sql
  select attname, correlation
  from pg_stats where tablename='airports_data'
  order by abs(correlation) desc;
#+end_src

请注意，坐标列不收集此统计量：点类型未定义小于和大于运算符。

相关性用于估算索引扫描的成本。

** 表达式统计

只有当比较操作的左侧或右侧部分指向列本身，且不包含任何表达式时，才能使用列级统计。例如，规划器无法预测计算列的函数
会如何影响统计量，因此对于 "函数调用 = 常量 "这样的条件，选择性总是估计为 0.5%。

#+begin_src sql
  eplain select * from flights
  where extract(
  month from schedule_depature at time zone 'Europe/Moscow'
  )=1;
#+end_src

规划者对函数的语义一无所知，即使是标准函数也是如此。我们的常识表明，1 月份执行的航班约占航班总数的
$$ \frac{1}{12} $$，比预测值高出一个数量级。

为了改进估计，我们必须收集表达式统计数据，而不是依赖列级统计数据。有两种方法可以做到这一点。

扩展表达式统计

第一个选项是使用扩展表达式统计。默认情况下不会收集此类统计数据；必须通过运行 CREATE STATISITCS 命令手动创建相应
的数据库对象：

#+begin_src sql
  create statistics flights_expr on (extract(
  month from scheduled_depature at time zone 'Europe/Moscow'
  ))
  from flights;
#+end_src

一旦收集到数据，估算的准确性就会提高：

#+begin_src sql
  analyze flights;
  explain select * from flights
  where extract(
  month from scheduled_depature at time zone 'Europe/Moscow'
  )=1;
#+end_src

要应用收集的统计数据，查询必须以与 CREATE STATISTICS 命令完全相同的形式指定表达式。

可通过运行 ALTER STATISTICS 命令单独调整扩展统计数据的大小限制。例如

#+begin_src sql
  alter statistics flights_expr set statistics 42;
#+end_src

与扩展统计相关的所有元数据都存储在系统目录的 pg_statistic_ext 表中，而收集的数据本身则存储在名为
pg_statistic_ext_data 的单独表中。这种分离用于对敏感信息实施访问控制。

可在单独的视图中以更方便的格式显示特定用户可用的扩展表达式统计数据：

#+begin_src sql
  select left(expr, 50) || '...' as expr,
  null_frac, avg_width, n_distinct,
  most_comm_vals as mcv,
  left(most_comm_freqs::text, 50) || '...' as mcf,
  from pg_stats_ext_exprs
  where statistics_name='flights_expr' \gx
#+end_src

表达式索引统计

另一种改进Cardinality的方法是使用为表达式索引收集的特殊统计数据；这些统计数据会在创建此类索引时自动收集，就像
为表所做的那样。如果真的需要索引，这种方法会非常方便。

#+begin_src sql
  drop statistics flights_expr;
  create index on flights(extract(
  month from scheduled_departure at time zone 'Europe/Moscow'
  ));

  analyze flights;

  explain select * from flights
  where extract(
  month from scheduled_departure at time zone 'Europe/Moscow'
  )=1;
#+end_src

表达式索引统计信息的存储方式与表统计信息的存储方式相同。例如，在查询 pg_stats 时，将索引名称指定为 tablename，
就可以获得不同值的数量：

#+begin_src sql
  select n_distinct from pg_stats
  where tablename='flights_extract_idx';
#+end_src

您可以使用 ALTER INDEX 命令调整索引相关统计信息的准确性。如果不知道与索引表达式相对应的列名，则必须先找到它。例如

#+begin_src sql
  select attname from pg_attribute
  where attrelid='flgihts_extract_idx'::regclass;

  alter index flights_extract_idx
  alter column extract set statistics 42;
#+end_src

** 多元统计
还可以收集跨多个表列的多元统计。前提条件是，必须使用 CREATE STATISTICS 命令手动创建相应的扩展统计量。

PostgreSQL 实现了三种多元统计。

列之间的功能依赖关系

如果一列中的数值（完全或部分）依赖于另一列中的数值，而筛选条件又包括这两列，那么Cardnality将被低估。

让我们考虑一个有两个过滤条件的查询：

#+begin_src sql
  select count(*) from flights
  where flight_no='PG0007' and departure_airport='VKO';
#+end_src

其价值被大大低估了：

#+begin_src sql
  explain select * from flights
  where flight_no='PG0007' and departure_airport='VKO';
#+end_src

这是一个众所周知的相关谓词问题。规划器假定谓词之间互不相关，因此整体选择性是根据逻辑 AND 组合的过滤条件选择性的乘
积来估算的。上面的计划清楚地说明了这个问题：一旦位图堆扫描节点根据 departure_airport 列上的条件对结果进行过滤，
位图索引扫描节点对 flight_no 列上条件的估计值就会大大降低。

然而，我们知道机场是由航班号明确定义的：第二个条件实际上是多余的（当然，除非机场名称有误）。在这种情况下，我们可以
通过对功能依赖性进行扩展统计来改进估算。

让我们对两列之间的功能依赖性进行扩展统计：

#+begin_src sql
  create statistics flights_dep(dependencies)
  on flight_no, departure_airport from flights;
  #+end_src

下一次分析运行将收集这一统计数据，从而改进估算结果：

#+begin_src sql
  analyze flights;
  explain select * from flights
  where flight_no='PG0007'
  and departure_airport='VKO';
#+end_src

收集到的统计数据存储在系统目录中，可以像这样访问：

#+begin_src sql
  select dependencies
  from pg_stats_ext where statistics_name='flights_dep';
#+end_src

这里的 2 和 5 是存储在 pg_attribute 表中的列号，而相应的值则定义了功能依赖程度：从 0（无依赖）到 1（第二列中的
值完全依赖于第一列中的值）。

多元差异值数量

对存储在不同列中的值的唯一组合数进行统计，可改进对多列执行 GROUP BY 操作的Cardinality估计。

例如，这里估计的出发机场和到达机场的可能对数是机场总数的平方；但实际值要小得多，因为并非所有对数都有直达航班：

#+begin_src sql
  select count(*)
  from (
  select distinct departure_airport, arrival_airport from flights
  ) t;
#+end_src

#+begin_src sql
  explain select distinct departure_airport, arrival_airport
  from flights;
#+end_src

让我们定义并收集一个关于不同值的扩展统计量：

#+begin_src sql
  create statistics flights_nd(ndistinct)
  on departure_airport, arrival_airport from flights;
  analyze flights;
#+end_src

Cardinality估计值就会提高

#+begin_src sql
  explain select distinct departure_airport, arrival_airport
  from flights;
#+end_src

您可以在系统目录中查看收集到的统计数据：

#+begin_src sql
  select n_distinct
  from pg_stats_ext where statistics_name='flights_nd';
#+end_src

多元 MCV 列表

如果数值的分布不均匀，那么仅仅依靠功能依赖性可能是不够的，因为估算的准确性在很大程度上取决于特定的数值对。例如，规划
器低估了从谢列梅捷沃机场起飞的波音 737 航班数量：

#+begin_src sql
  select count(*) from flights
  where departure_airport='SVO' and aircraft_code='733';
#+end_src

#+begin_src sql
  explain select * from flights
  where departure_airport='SVO' and aircraft_code='733';
#+end_src

在这种情况下，可以通过收集多元 MCV 列表的统计数据来改进估算：

#+begin_src sql
  create statistics flights_mcv(mcv)
  on departure_airport, aircraft_code from flights;
  analyze flights;
#+end_src

新的Cardinality估算更加准确

#+begin_src sql
  explain select * from flights
  where departure_airport='SVO' and aircraft_code='733';
#+end_src

为获得这一估计值，规划器依赖于系统目录中存储的频率值：

#+begin_src sql
  select values, frequency
  from pg_statistic_ext stx
  join pg_statistic_ext_data stxd on std.oid=stxd.stdoid,
  pg_mcv_list_items(stxdmcv) m
  where stxname='flights_mcv'
  and values='{SVO, 733}';
#+end_src

与普通 MCV 列表一样，多变量列表可容纳 100 个 default_statistics_target 值（如果在列级别也设置了该参数，则使
用其最大值）。

如果需要，还可以更改列表的大小，就像扩展表达式统计一样：

#+begin_src sql
  alter statistics ... set statistics ...;
#+end_src

在所有这些示例中，我只使用了两列，但您也可以收集更多列的多元统计数据。

要在一个对象中结合多种类型的统计信息，可以在其定义中提供这些类型的逗号分隔列表。如果没有指定类型，PostgreSQL 将为
指定列收集所有可能类型的统计数据。

除实际列名外，多元统计还可以使用任意表达式，就像表达式统计一样。


* 表访问方法

** 可插拔式存储引擎

PostgreSQL 所使用的数据布局并不是唯一可行的，也不是适用于所有负载类型的最佳 布局。根据可扩展性的理念，PostgreSQL
允许创建和插入各种表访问方法（可插拔存储引擎），但目前只有一种方法是开箱即用的：

#+begin_src sql
  select amname, amhandler from pg_am where amtype='t';
#+end_src

创建表格时，可以指定要使用的引擎（CREATE TABLE ... USING）；否则，将使用堆 default_table_access_method 参
数中列出的默认引擎。

为使 PostgreSQL 内核能以相同方式与各种引擎协同工作，表访问方法必须实现一个特殊接口。amhandler 列中指定的函数会
返回接口结构，其中包含核心所需的所有信息。

所有表格访问方法都可以使用以下核心组件：

+ 事务管理器，包括 ACID 和快照隔离支持
+ 缓冲区管理
+ I/O子系统
+ TOAST
+ 优化器和执行器
+ 索引支持

这些部件始终由存储引擎支配，即使没有全部使用它们。

反过来，存储引擎也是这样定义的

+ 元组格式和数据结构
+ 表扫描实现和代价估计
+ insert,update,delete的实现和锁操作
+ 可见性规则
+ 自动回收和分析处理

从历史上看，PostgreSQL 使用的是单一的内置数据存储，没有任何适当的编程接口，因此现在很难设计出既考虑到标准引擎的所
有特性又不干扰其他方法的好方法。

#+begin_comment
例如，目前还不清楚如何处理 WAL。新的访问方法可能需要记录它们自己的操作，而内核并不知道这些操作。现有的通用 WAL 机
制通常是个糟糕的选择，因为它会带来过多的开销。你可以添加另一个接口来处理新类型的 WAL 条目，但这样一来，崩溃恢复将依
赖于外部代码，这是非常不可取的。目前唯一可行的解决方案就是为每个特定引擎的内核打补丁。
#+end_comment

因此，我没有刻意严格区分表访问方法和核心。本书前几部分所描述的许多特性在形式上都属于堆访问方法，而不是核心本身。这种
方法很可能一直是PostgreSQL的终极标准引擎，而其他方法将填补各自的空白，以应对特定负载类型的挑战。

在目前正在开发的所有新型存储引擎中，我想提及以下几种：

+ Zheap 的目的是防止表格臃肿。它实现了就地行更新，并将与 MVCC 相关的历史数据移至单独的撤销存储区。这种引擎对涉及
  频繁数据更新的负载非常有用。

  对于 Oracle 用户来说，Zheap 体系结构似乎并不陌生，不过它也有一些细微差别（例如，索引访问方法的接口不允许创建具
  有自己版本的索引）。
+ Zedstore 实现列式存储，这在 OLAP 查询中可能是最有效的。

  存储数据的结构是元组 ID 的 B 树；每一列都存储在与主 B 树相关联的自己的 B 树中。将来有可能在一棵 B 树中存储多
  列数据，从而实现混合存储。

** 顺序扫描

存储引擎定义表数据的物理布局，并提供访问表数据的方法。唯一支持的方法是顺序扫描，即全文读取表fork的文件（或多个文
件）。在每个读取页面中，都会检查每个元组的可见性；不满足查询要求的元组会被过滤掉。

[[./images/lCbMtr.png]]

扫描进程会经过缓冲缓存；为确保大表不会剔除有用数据，会使用一个小尺寸的缓冲环。其他正在扫描同一张表的进程会加入这个
缓冲环，从而避免了额外的磁盘读取；这种扫描被称为同步扫描。因此，扫描不必总是从文件的起始位置开始。

顺序扫描是读取整个表或其中最佳部分的最有效方法。换句话说，当选择性较低时，顺序扫描的价值最大。(如果选择性高，即查询
只需选择几行，则最好使用索引）。

代码估算

在查询执行计划中，顺序扫描由 Seq Scan 节点表示：

#+begin_src sql
  explain select *
  from flights;
#+end_src

估计行数作为基本统计数据的一部分提供：

#+begin_src sql
  select reltuples from pg_class where relname='flights';
#+end_src

在估算成本时，优化器会考虑以下两个因素：磁盘 I/O 和 CPU 资源。

I/O 成本的计算方法是，将表中的页数与读取单个页面的成本相乘，假定页面是按顺序读取的。当缓冲区管理器请求读取一个页面
时，操作系统实际上会从磁盘读取更多数据，因此在操作系统缓存中很可能会出现多个后续页面。因此，使用顺序扫描读取单个页
面的成本（规划器估计为 seq_page_cost）低于随机访问成本（由 random_page_cost 值定义）。

默认设置对 HDD 运行良好；如果使用 SSD，则应大幅降低 random_page_cost 值（seq_page_cost 参数通常保持不变，作
为参考值）。由于这些参数之间的最佳比例取决于硬件，因此通常在表空间级别进行设置（ALTER TABLESPACE ... SET）。

#+begin_src sql
  select relpages,
  current_setting('seq_page_cost') as seq_page_cost,
  relpages * current_setting('seq_page_cost')::real as total
  from pg_class where relname='flights';
#+end_src

这些计算清楚地显示了不及时抽真空导致表膨胀的后果：表的fork越大，需要扫描的页数就越多，而不管其中包含的活图组数有
多少。

CPU 资源估算包括处理每个元组的成本（规划器估算为  cpu_tuple_cost）:

#+begin_src sql
  select reltuples,
  current_setting('cpu_tuple_cost') as cpu_tuple_cost,
  reltuples * current_setting('cpu_tuple_cost')::real as total
  from pg_class where relname='flights';
#+end_src

这两个估算值的总和就是计划的总成本。启动成本为零，因为顺序扫描没有先决条件。

如果需要对扫描的表进行过滤，则应用的过滤条件会出现在 Seq Scan 节点过滤部分的计划中。估计的行数取决于这些条件的选
择性，而成本估计包括相关的计算费用。

EXPLAIN ANALYZE 命令同时显示实际返回的行数和已过滤掉的行数：

#+begin_src sql
  explain (analyze, timing off, summary off)
  select * from flights
  where status='Scheduled';
#+end_src

让我们来看看一个使用聚合的更复杂的执行计划：

#+begin_src sql
  explain select count(*) from seats;
#+end_src

该计划由两个节点组成：计算count函数的上层节点（Aggregate）从扫描表的下层节点（Sequ Scan）提取数据。

聚合节点的启动成本包括聚合本身：如果不从下层节点获取所有记录，就不可能返回第一行（在本例中是唯一的一行）。聚合成本
是根据每个输入行的条件操作执行成本（按 cpu_operator_cost 估算）估算的：

#+begin_src sql
  select reltuples,
  current_setting('cpu_operator_cost') as cpu_operator_cost,
  round((
  reltuples * current_setting('cpu_operator_cost')::real
  )::numeric, 2) as cpu_cost
  from pg_class where relname='seats';
#+end_src

收到的估算值将加到 Seq Scan 节点的总成本中。

聚合节点的总成本还包括处理要返回的记录的成本，估计为 cpu_tuple_cost：

#+begin_src sql
  with t(cput_cost) as (
  select round((
  reltuples * current_setting('cpu_operator_cost')::real
  )::numeric, 2)
  from pg_class where relname='seats'
  )
  select 21.39 + t.cput_cost as startup_cost,
  round((
  21.39+t.cpu_cost+
  1*current_setting('cpu_tuple_cost')::real
  )::numeric, 2) as total_cost
  from t;
#+end_src

[[./images/1lJUGc.png]]

** 并行计划

PostgreSQL 支持并行查询执行。执行查询的领导进程（通过 postmaster）会产生多个工作进程，同时执行计划的同一个并行
部分。执行查询的领导进程（通过 postmaster）会催生多个工作进程，这些工作进程会同时执行计划中的同一个并行部分，并将
结果传递给领导进程，领导进程会将这些结果放在 Gather2 节点中。在不接受数据时，领导进程也可以参与计划并行部分的执行

如果需要，可以关闭 parallel_leader_participation 参数，禁止领导者进程参与并行计划的执行。

[[./images/ZKyoaD.png]]

当然，启动这些进程并在它们之间发送数据并不是免费的，因此到目前为止，并非所有查询都应并行化。

此外，即使允许并行执行，也并非计划的所有部分都能同时处理。有些操作只能由领导者以顺序模式单独执行。

#+begin_comment
 PostgreSQL 不支持并行计划执行的另一种方法，即由多个工作进程执行数据处理，这些工作进程实际上形成了一条流水线（粗
 略地说，每个计划节点都由一个单独的进程执行）；PostgreSQL 开发人员认为这种机制效率低下。
#+end_comment

** 并行顺序扫描

Parallel Seq Scan 节点是为并行处理而设计的节点之一，它可执行并行顺序扫描。

这个名字听起来有点争议（到底是顺序扫描还是并行扫描？如果我们看一下文件访问，表页是按照简单顺序扫描的顺序读取的。不
过，这一操作是由多个并发进程执行的。为了避免重复扫描同一个页面，执行器通过共享内存同步这些进程。

这里的一个微妙之处在于，操作系统无法获得典型的顺序扫描的全貌，相反，它看到的是几个执行随机读取的进程。因此，通常能
加快顺序扫描速度的数据预取变得毫无用处。为了尽量减少这种令人不快的影响，PostgreSQL 为每个进程分配的读取页面不只是
一个，而是多个连续的页面。

因此，并行扫描的意义不大，因为通常的读取成本会因进程间的数据传输而进一步增加。不过，如果工作者对获取的行进行任何后
处理（如聚合），总执行时间可能会大大缩短。

代价估算

让我们来看看一个在大型表格上执行聚合的简单查询。执行计划是并行化的：

#+begin_src sql
  explain select count(*) from bookings;
#+end_src

Gather 下面的所有节点都属于计划的并行部分。它们由每个工作进程执行（这里计划了两个工作进程），也可能由领导进程执行
（除非通过 parallel_leader_participation 参数关闭了该功能）。Gather 节点本身及其上方的所有节点构成计划的顺序
部分，由领导进程单独执行。

Parallel Seq Scan 节点表示并行堆扫描。行数字段显示单个进程要处理的估计平均行数。总的来说，执行必须由三个进程（一
个领导进程和两个工作进程）完成，但领导进程处理的行数较少：随着工作进程数量的增加，领导进程的份额也会变小。在这种特殊
情况下，系数为 2.4

#+begin_src sql
  select reltuples::numeric, round(reltuples / 2.4) as per_process
  from pg_class where relname='bookings';
#+end_src

并行顺序扫描成本的计算方法与顺序扫描类似。接收到的值较小，因为每个进程处理的行数较少；I/O 部分全部包括在内，因为仍
需逐页读取整个表：

#+begin_src sql
  select round((
  relpages * current_setting('seq_page_cost')::real +
  reltuples / 2.4 * current_setting('cpu_tuple_cost')::real
  )::numeric, 2)
  from pg_class where relname='bookings';
#+end_src

接下来，"Partial Aggregate"节点对获取的数据进行聚合；在这种特殊情况下，它会计算行数。

聚合成本按常规方式估算，并添加到表格扫描的成本估算中：

#+begin_src sql
  with t(startup_cost)
  as (
  select 22243.29 + round((
  reltuples / 2.4 * current_setting('cpu_operator_cost')::real
  )::numeric, 2)
  from pg_class
  where relname='bookings'
  )
  select startup_cost,
  startup_cost+round((
  1*current_setting('cpu_tuple_cost')::real
  )::numeric, 2) as total_cost
  from t;
#+end_src

下一个节点（Gather）由领导进程执行。该节点负责启动工作者并收集他们返回的数据。

为便于规划，启动进程（无论其数量多少）的成本估算由 parallel_setup_cost 参数定义，而进程间每行传输的成本则由
parallel_tuple_cost 估算。

在此示例中，启动成本（用于启动进程的成本）占主导地位；此值已添加到部分聚合节点的 启动成本中。总成本中还包括传输两行
的成本；该值将计入 "Partial Aggregate"节点的总成本：

#+begin_src sql
  select
  24442.36+round(
  current_setting('parallel_setup_cost')::numeric,
  2) as setup_cost,
  24442.37+round(
  current_setting('parallel_setup_cost')::numeric+
  2*current_setting('parallel_tuple_cost')::numeric,
  2) as total_cost;
#+end_src

最后但并非最不重要的是，"Finalize Aggregate"节点汇总 "Gather "节点从并行进程中收到的所有部分结果。

最终聚合的估算与其他聚合一样。它的启动成本基于聚合三条记录的成本；这个值会加到聚合的总成本中（因为计算结果需要所有
记录）。最终聚合的总成本还包括返回一条记录的成本。

#+begin_src sql
  with t(startup_cost) as (
  select 25442.57+round((
  3*current_setting('cpu_operator_cost')::real
  )::numeric, 2)
  from pg_class where relname='bookings'
  )
  select startup_cost,
  startup_cost+round((
  1*current_setting('cpu_tuple_cost')::real
  )::numeric, 2) as total_cost
  from t;
#+end_src

成本估算之间的依赖关系取决于节点是否需要先积累数据，然后再将结果传递给上级节点。聚合节点在获得所有输入行之前无法返
回结果，因此其启动成本基于下级节点的总成本。相反，聚合节点会在获取记录后立即开始向上游发送记录。因此，该操作的启动
成本取决于下层节点的启动成本，而总成本则基于下层节点的总成本。

下面是依赖关系图：

[[./images/hQJyfP.png]]

** 并行执行的限制

后端workers进程的数量

进程数量由三个参数组成的层次结构控制。同时运行的后台工作者的最大数量由 max_worker_processes 值定义。

然而，并行查询执行并不是唯一需要后台工作者的操作。例如，它们还参与逻辑复制，并可用于扩展。专门为并行计划执行分配的
进程数量受限于 max_parallel_workers 值。

其中，最多 max_parallel_workers_per_gather 进程可为一个领导者服务。

这些参数值的选择取决于以下因素：

+ 软件能力：系统必须有专门用于并行执行的空闲内核。
+ 容量：数据库必须包含大型表格。
+ 典型负载：必须有可能受益于并行执行的查询。

满足这些标准的通常是 OLAP 系统，而不是 OLTP 系统。

如果估计要读取的堆数据量不超过 min_parallel_table_scan_size 值，规划器根本不会考虑并行执行。

除非在 parallel_workers 存储参数中明确指定了特定表的进程数，否则进程数将按以下公式计算：

$$ 1+logs_{3}{(\frac{table size}{min_parallel_table_scan_size})} $$

这意味着表每增长三倍，PostgreSQL 就会多分配一个并行工作者进行处理。默认设置下的数据如下：

| table,MB | number of processes |
|----------+---------------------|
|        8 |                   1 |
|       24 |                   2 |
|       72 |                   3 |
|      216 |                   4 |
|      648 |                   5 |
|     1944 |                   6 |

无论如何，并行工作者的数量不能超过 max_parallel_workers_per_gather 参数定义的上限。

如果我们查询一个 19MB 的小表，将只计划并启动一个 Worker：

#+begin_src sql
  explain (analyze, costs off, timing off, summary off)
  select count(*) from flights;
#+end_src
  
在一个 105MB 的表上进行查询，只能获得两个进程，因为它达到了最大并行工作进程数的限制：

#+begin_src sql
  explain (analyze, costs off, timing off, summary off)
  select count(*) from bookings;
#+end_src

如果取消这一限制，我们将得到估计的三个过程：

#+begin_src sql
  alter system set max_parallel_workers_per_gather=4;
  select pg_reload_conf();
  explain (analyze, costs off, timing off, summary off)
  select count(*) from bookings;
#+end_src

如果查询执行期间空闲的插槽数量少于计划值，则只会启动可用的workers数量。

让我们把并行进程的总数限制在 5 个，同时运行两个查询：

#+begin_src sql
  alter system set max_parallel_workers=5;
  select pg_reload_conf();
  explain (analyze, costs off, timing off, summary off)
  select count(*) from bookings;
  explain (analyze, costs off, timing off,  summary off)
  select count(*) from bookings;
#+end_src


虽然在这两种情况下都预计有三个进程，但其中一个查询只获得了两个时段。

让我们恢复默认设置：

#+begin_src sql
  alter system reset all;
  select pg_reload_conf();
#+end_src

不可并行查询

并非所有查询都可以并行化。特别是，并行计划不能用于以下查询类型：

+ 修改或锁定数据的查询（UPDATE、DELETE、SELECT FOR UPDATE 等）。
  此限制不适用于以下命令中的子查询：
  CREATE TABLE AS, SELECT INTO, CREATE MATERIALIZED VIEW
  REFRESH MATERIALIZED VIEW
  不过，在所有这些情况下，行插入仍按顺序进行。
+ 可暂停的查询。它适用于在游标内运行的查询，包括 PL/pgSQL 中的 FOR 循环。
+ 调用 PARALLEL UNSAFE 函数的程序。默认情况下，这些都是用户定义的函数和一些标准函数。您可以通过查询系统目录获得
  不安全函数的完整列表：
  #+begin_src sql
    select * from pg_proc where proparallel='u';
  #+end_src
+ 如果从并行化查询中调用了函数内的查询（以避免worker数量的递增），则应调用这些函数。

其中一些限制可能会在未来的 PostgreSQL 版本中被移除。例如，在 Serializable 隔离级别并行查询的功能已经存在。

#+begin_comment
  目前正在开发使用 INSERT 和 COPY 等命令并行插入记录的功能。
#+end_comment

查询未并行化可能有几个原因：

+ 这种查询完全不支持并行化。
+ 服务器配置禁止使用并行计划（例如，由于强制规定的表大小限制）。
+ 并行计划比顺序计划更昂贵。

要检查查询是否可以并行化，可以暂时关闭 force_parallel_mode 参数。这样，规划器就会尽可能建立并行计划：

#+begin_src sql
  explain select * from flights;
#+end_src

#+begin_src sql
  set force_parallel_mode=on;
  explain select * from flights;
#+end_src

并行限制查询

计划的并行部分越大，可能实现的性能提升就越多。不过，某些操作只能由领导进程严格按顺序执行，尽管它们并不影响并行化。
换句话说，它们不能出现在聚集节点下面的计划树中。

不可扩展的子查询。不可扩展子查询最明显的例子就是扫描 CTE 结果（在计划中由 CTE 扫描节点表示）：

#+begin_src sql
  explain (costs off)
  with t as materialized(
  select * from flights
  )
  select count(*) from t;
#+end_src

如果 CTE 没有实体化，则计划不包含 CTE SCAN节点，因此此限制不适用。

不过，请注意，如果 CTE 本身的计算成本较低，也可以在并行模式下进行计算：

#+begin_src sql
  explain (costs off)
  with t as materialized(
  select count(*) from flights;
  )
  select * from t;
#+end_src

下图中的 SubPlan 节点下显示了另一个不可扩展子查询的示例：

#+begin_src sql
  explain (costs off)
  select * from flights f
  where f.scheduled_depature > ( -- SubPlan
  select min(f2.scheduled_depature)
  from flights f2
  where f2.aircraft_code=f.aircraft_code
  );
#+end_src

前两行表示主查询的计划：按顺序扫描航班表，并根据提供的过滤器检查每一行。过滤器条件包括一个子查询；该子查询的计划从
第三行开始。因此，SubPlan 节点会被执行多次，在这种情况下，通过顺序扫描获取的每一条记录都会被执行一次。

该计划的上层 Seq Scan 节点不能参与并行执行，因为它依赖于 SubPlan 节点返回的数据。

最后，这里还有一个由 InitPlan 节点表示的不可扩展子查询：

#+begin_src sql
  explain (costs off)
  select * from flights f
  where f.scheduled_depature > ( -- SubPlan
  select min(f2.scheduled_depature)
  from flights f2
  where exists ( -- InitPlan
  select * from ticket_flights tf
  where tf.flight_id=f.flight_id
  )
  );
#+end_src

[[./images/KIPsEQ.png]]

与 SubPlan 节点不同，InitPlan 只评估一次（在本例中，每次执行 SubPlan 2 节点时评估一次）。

InitPlan 的父节点不能参与并行执行（但接收 InitPlan 评估结果的节点可以，如本示例）。

临时表。临时表不支持并行扫描，因为只能由创建临时表的进程访问。它们的页面是在本地缓冲缓存中处理的。要让多个进程都能
访问本地缓冲区，就需要像共享缓冲区那样的锁定机制，这将使其其他优点变得不那么突出。

#+begin_src sql
  create temporary table flights_tmp as select * from flights;
  explain (costs off)
  select count(*) from flights_tmp;
#+end_src

并行限制函数。定义为 PARALLEL RESTRICTED 的函数只允许在计划的顺序部分使用。您可以通过运行以下查询从系统目录中获
取此类函数的列表：

#+begin_src sql
  select * from pg_proc where proparallel='r';
#+end_src

只有在完全了解所有影响并仔细研究了所有施加的限制后，才能将您的功能标记为 PARALLEL RESTRICTED（更不用说
PARALLEL SAFE）。



* 索引访问方法





























* hash索引
** 概览
散列索引提供了通过特定索引键快速查找元组ID(TID)的能力。粗略地说，它只是一个存储在磁盘上的哈希表。散列索引所支持的
唯一操作是通过平等条件进行搜索。

当一个值被插入到一个索引中时，索引键的哈希函数被计算出来。在PostgreSQL中，哈希函数返回32位或64位的整数；这
些值的几个最低位被用作相应的桶的编号。键的TID和哈希代码被添加到选择的桶中。密钥本身并不存储在索引中，因为处理小的
固定长度的值更方便。

一个索引的哈希表是动态扩展的。集群的最小数量是两个。随着索引元组数量的增加，其中一个桶会被拆成两个。这个操作多用了一
个哈希代码的位，所以元素只在分割后的两个桶之间重新分配；哈希表的其他桶的组成保持不变。

索引搜索操作会计算索引键的哈希函数和相应的桶号。在所有桶的内容中，搜索将只返回那些与键的哈希代码相对应的TID。由于
桶中的元素是按键的哈希代码排序的，二进制搜索可以相当有效地返回匹配的TID。

由于键没有存储在哈希表中，索引访问方法可能因为哈希碰撞而返回多余的TID。因此，索引引擎必须重新检查由访问方法获取的
所有结果。由于同样的原因，不支持仅有索引的扫描。
** 页面布局

与普通的哈希表不同，哈希索引是存储在磁盘上的。因此，所有的数据必须被排列成页，最好是使索引操作（搜索、插入、删除）需
要访问尽可能少的页。

一个哈希索引有以下四种页面

+ 元页－提供索引的目录
+ 桶页－索引的主要页面
+ 溢出页－当前主桶页不能容纳所有的元素时使用的附加页
+ 位图页－包含位数组的页，用于跟踪已经释放并可重复使用的溢出页

可以使用pageinspect扩展来查看索引页面

创建一个空的表

#+begin_src sql
  create extension pageinspect;
  create table t(n integer);
  analyze t;
  create index on t using hash(n);
#+end_src

对表进行了分析，所以创建的索引将具有尽可能小的尺寸；否则，将根据表包含10个页面的假设来选择桶的数量。索引包含四
个页面：元页面、两个桶页面和一个位图页面（一次性创建，供将来使用）：

#+begin_src sql
  select page, hash_page_type(get_raw_page('t_n_idx', page))
  from generate_series(0, 3) page;
#+end_src

[[./images/hySCnE.png]]

元页包含所有关于索引的控制信息。我们目前只对几个值感兴趣：

#+begin_src sql
  select ntuples, ffactor, maxbucket
  from hash_metapage_info(get_raw_page('t_n_idx', 0));
#+end_src

每个桶的估计行数显示在ffactor字段。这个值是根据块的大小和fillfactor存储参数值计算的。通过绝对均匀的数据分布
和没有哈希碰撞，你可以使用一个更高的fillfactor值，但在现实生活中的数据库，它增加了页面溢出的风险。

对于散列索引来说，最糟糕的情况是数据分布有很大的偏斜，当一个键重复多次时。由于散列函数将返回一个相同的值，所有的数据
都将被放入同一个桶中，增加桶的数量将无济于事。

现在索引是空的，如ntuples字段所示。让我们通过插入多条具有相同索引键值的行来引起一个桶状页溢出。一个溢出的页面出
现在索引中：


#+begin_src sql
  insert into t(n)
  select 0 from generate_series(1, 500);
  select page, hash_page_type(get_raw_page('t_n_idx', page))
  from generate_series(0, 4) page;
#+end_src

[[./images/RSwyo0.png]]

对所有页面的综合统计显示，桶0是空的，而所有的值都被放入桶1：其中一些位于主页面，而那些不适合它的值可以在溢出页面
找到。

#+begin_src sql
  select page, live_items, free_size, hasho_bucket
  from (values (1), (2), (4)) p(page),
  hash_page_stats(get_raw_page('t_n_idx', page));
#+end_src

很明显，如果一个相同的桶的元素分布在几个页面上，性能会受到影响。如果数据分布均匀，散列索引显示出最好的结果。

现在让我们来看看一个桶是如何被分割的。当索引中的行数超过可用桶的估计ffactor值时，它就会发生。这里我们有两个桶，
ffactor是307，所以当第615行被插入到索引中时，它就会发生：

#+begin_src sql
  select ntuples, ffactor, maxbucket, ovflpoint
  from hash_metapage_info(get_raw_page('t_n_idx', 0));
  insert into t(n)
  select n from generate_series(1, 115) n;
  select nutples, ffactor, maxbucket, ovflpoint
  from hash_metapage_info(get_raw_page('t_n_idx', 0));
#+end_src

最大bucket值已经增加到两个：现在我们有三个bucket，编号从0到2。 但是，尽管我们只增加了一个bucket，页面
的数量却增加了一倍：

#+begin_src sql
  select page, hash_page_type(get_raw_page('t_n_idx', page))
  from generate_series(0, 6) page;
#+end_src

[[./images/tNA8od.png]]

其中一个新页面被桶2使用，而另一个保持空闲，一旦出现就会被桶3使用。



#+begin_src sql
  select page, live_items, free_size, hasho_bucket
  from (values(1), (2), (4), (5)) p(page),
  hash_page_stats(get_raw_page('t_n_idx', page));
#+end_src

因此，从操作系统的角度来看，哈希指数是呈井喷式增长的，尽管从逻辑的角度来看，哈希表显示的是逐步增长。

为了在一定程度上平衡这种增长，并避免一次分配太多的页面，从第十次增加开始，页面将分四批平均分配，而不是一次全部分配。

元页的另外两个字段，实际上是位掩码，提供了关于bucket地址的细节：

#+begin_src sql
  select maxbucket, highmask::bit(4), lowmask::bit(4)
  from hash_metapage_info(get_raw_page('t_n_idx' , 0));
#+end_src

一个桶号是由对应于高掩码的哈希码位定义的。但如果收到的桶号不存在（超过了maxbucket），就会取低掩码位。1 在这种特
殊情况下，我们取两个最低位，这样就有了0到3的值；但如果我们得到3，我们就只取一个最低位，也就是用桶1而不是
桶3。

每次大小翻倍时，新的bucket页被分配为一个连续的chunk，而溢出页和位图页根据需要被插入这些片段之间。元页在备用数
组中保留了插入每个块的页数，这让我们有机会根据桶的数量用简单的算术来计算其主页的数量。

在这个特殊的案例中，第一次增加后插入了两个页面（一个位图页和一个溢出页），但第二次增加后还没有发生新的增加：

#+begin_src sql
  select spares[2], spares[3]
  from hash_metapage_info(get_raw_page('t_n_idx', 0));
#+end_src

元页还存储了一个指向位图页的指针数组：

#+begin_src sql
  select mapp[1]
  from hash_metapage_info(get_raw_page('t_n_idx', 0));
#+end_src

[[./images/xr92tr.png]]

当指向死元组的指针被移除时，索引页内的空间被释放出来。这发生在修剪页面的过程中（这是由试图将一个元素插入到一个完全
填充的页面中所引发的）1，或者是在进行常规回收的时候。

然而，散列索引不能缩减：一旦分配，索引页将不会返回到操作系统。主页面被永久地分配给它们的桶，即使它们根本不包含任何
元素；被清除的溢出页面在位图中被跟踪，可以被重新使用（可能被另一个桶使用）。减少索引的物理尺寸的唯一方法是使用
REINDEX或VACCUM FULL命令重建它

查询计划中没有显示索引类型：

#+begin_src sql
  create index on flights using hash(flight_no);
  explain (costs off)
  select * from flights where flight_no = 'PG0001';
#+end_src

** 操作类

在Postgre 10之前，哈希索引是没有记录的，也就是说，它们既没有对故障的保护，也没有复制，因此，不建议使用它们。但
即使如此，它们也有自己的价值。事情是这样的，散列算法被广泛使用（特别是执行散列连接和分组），系统必须知道哪种散列函数
可以用于某种数据类型。然而，这种对应关系不是静态的：它不能被一劳永逸地定义，因为Postgre 10允许在运行中添加新的数
据类型。因此，它是由散列索引和特定数据类型的运算器类来维护的。散列函数本身由该类的支持函数来表示：

#+begin_src sql
  select opfname as opfamily_name,
  amproc::regproc as opfamily_procedure
  from pg_am am
  join pg_opfamily opf on opfmethod=am.oid
  join pg_amproc amproc on amprocfamily=opf.oid
  where amname='hash'
  and amprocnum=1
  order by opfamily_name, opfamily_procedure;
#+end_src

这些函数返回32位整数。尽管它们没有被记录下来，但它们可以用来计算相应类型的值的哈希代码。

例如，text_ops系列使用hashtext函数：

#+begin_src sql
  select hashtext('one'), hashtext('two');
#+end_src

散列索引的运算符类只提供等于运算符：

#+begin_src sql
  select opfname as opfamily_name
  left(amopopr::regoperator::text, 20) as opfamily_operator
  from pg_am am
  join pg_opfamily opf on opfmethod=am.oid
  join pg_amop amop on amopfamily=opf.oid
  where amname='hash'
  order by opfamily_name, opfamily_operator;
#+end_src


** 属性

让我们看一下哈希访问方法赋予系统的索引级属性

访问方法属性

#+begin_src sql
  select a.amname, p.name, pg_indexam_has_property(a.oid, p.name)
  from pg_am a, unnest(array[
  'can_order', 'can_unique', 'can_multi_col',
  'can_exclude', 'can_include'
  ])p(name)
  where a.amname = 'hash';
#+end_src

很明显，哈希索引不能用于行排序：哈希函数或多或少地随机混合了数据。

唯一性约束也不被支持。然而，哈希索引可以执行排除性约束，由于唯一支持的函数是等于，这种排除性达到了唯一性的含义：

#+begin_src sql
  alter table aircrafts_data
  add constraint unique_range exclude using hash(range with =);
  insert into aircrafts_data
  values('744', '{"ru":"Boeing 747-400"}', 11100);
#+end_src

多列索引和额外的INCLUDE列也不被支持。

索引级属性

#+begin_src sql
  select p.name, pg_index_has_property('flights_flight_no_idx', p.name)
  from unnest(array[
  'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'
  ]) p(name);
#+end_src

散列索引同时支持常规的索引扫描和位图扫描。

不支持通过哈希索引进行表的聚类。这很符合逻辑，因为很难想象为什么可能需要根据哈希函数值对堆数据进行物理排序。

列级属性

#+begin_src sql
  select p.name,
  pg_index_column_has_property('flights_flight_no_idx', 1, p.name)
  from unnest(arrary[
  'asc', 'desc', 'nulls_first', 'nulls_last', 'orderable',
  'distance_orderable', 'returnable', 'search_array', 'search_nulls'
  ]) p(name);
#+end_src

由于散列函数不保留值的顺序，所有与排序有关的属性都不适用于散列索引。

散列索引不能参与仅有索引的扫描，因为它不存储索引键，需要堆访问。

散列索引不支持NULL值，因为等于操作对它们不适用。

对数组中的元素的搜索也没有实现。



* B-tree索引

** 概览
B树（实现为btree访问方法）是一种数据结构，它可以使你从树的根部往下走，快速找到树的叶子节点中的所需元素。为了使
搜索路径能够被明确地确定，所有的树元素必须是有序的。B树是为有序数数据类型设计的，其数值可以被比较和排序。
以下是代码上建立索引的示意图，内部节点是水平的矩形；叶子节点是垂直排列的。

[[./images/1nM4AH.png]]

每个树节点包含几个元素，这些元素由一个索引键和一个指针组成。内部节点元素引用下一级的节点；叶子节点元素引用堆图元
（图示中没有显示这些引用）。

B树具有以下重要特性：
+ 它们是平衡的，这意味着一棵树的所有叶子结点都位于相同的深度。因此，它们保证所有数值的搜索时间相等。
+ 在这个过程中，每个节点都有大量的分支，也就是说，每个节点都包含许多元素，通常是数百个（图中显示的是三元素节点，只
  是为了清晰起见）。因此，B树的深度总是很小，即使是非常大的表。
  #+begin_comment
  我们不能绝对肯定地说这种结构的名称中的字母B代表什么。平衡型和灌木型都同样适合。令人惊讶的是，你经常可以看到它被
  解释为二进制，这当然是不正确的。
  #+end_comment
+ 索引中的数据是按升序或降序排序的，在每个节点内和同一级别的所有节点中都是如此。对等节点被绑定到一个双向的列表中，因
  此可以通过简单地扫描列表来获得一个有序的数据集，而不必每次都从根部开始。


** 搜索和插入
*** 等值搜索
让我们来看看我们如何通过条件 "indexedcolumn = expression "在树中搜索一个值。我们将尝试找到KJA机场

搜索从根节点开始，访问方法必须确定要下降到哪个子节点。它选择Ki键，对于Ki⩽表达式<Ki+1是满足的。

根节点包含键AER和OVB。 条件AER⩽ KJA < OVB成立，所以我们需要下降到有AER键的元素所引用的子节点。

[[./images/jQgP3H.png]]


这个过程是递归重复的，直到我们到达包含所需元组ID的叶节点。在这种特殊情况下，子节点满足条件DME⩽ KJA < KZN，
所以我们必须下降到有DME键的元素所引用的叶节点。

你可以注意到，树的内部节点中最左边的键是多余的：要选择根的子节点，只要满足条件KJA < OVB就可以了。B树并不存储这
样的键，所以在接下来的插图中，我将把相应的元素留空。

叶子节点中的所需元素可以通过二进制搜索快速找到。

然而，搜索过程并不像它看起来那么简单。必须考虑到，索引中数据的排序顺序可以是升序，如上图所示，也可以是降序。即使是
一个唯一的索引也可以有几个匹配的值，而所有这些值都必须被返回。此外，可能会有很多重复的数据，以至于它们不适合一个节
点，所以邻近的叶子节点也要被处理。

由于一个索引可以包含非唯一的值，将其顺序称为非降序而不是升序（非升序而不是降序）会更准确。但我将坚持使用一个更简单
的术语。此外，元组ID是索引键的一部分，这让我们认为索引条目是唯一的，即使这些值实际上是相同的。

除此之外，在搜索过程中，其他进程可能会修改数据，页面可能会被分割成两个，树状结构可能会改变。所有的算法都是为了尽可能
地减少这些并发操作之间的争夺，避免过多的锁，但我们在这里不打算讨论这些技术问题。

*** 不等值搜索
如果按条件 "索引列⩽表达式"（或 "索引列⩾表达式"）进行搜索，我们必须首先在索引中搜索满足平等条件的值，然后按要求的
方向遍历其叶子节点，直到到达树的末端。

这张图说明了搜索小于或等于DME的机场代码。

[[./images/V61w6w.png]]

对于小于和大于运算符，过程是相同的，只是必须排除第一个发现的值。

*** 范围搜索

当按范围 "expression1 ⩽ 索引列 ⩽ expression2 "搜索时，我们必须首先找到expression1，然后沿正确方向遍历叶
子节点，直到找到expression2。这张图说明了在LED和ROV之间（含）搜索的过程。

[[./images/wgP1TQ.png]]


*** 插入
新元素的插入位置是由键的顺序明确定义的。例如，如果你在表中插入RTW机场代码（萨拉托夫），新元素将出现在最后一个
叶子节点，在ROV和SGC之间。

但是，如果叶子节点没有足够的空间容纳新的元素呢？例如（假设一个节点最多可以容纳三个元素），如果我们插入TJM机场代
码，最后一个叶子节点将被过度填充。在这种情况下，节点会被分成两个，旧节点的一些元素被移到新节点中，而新的子节点的
指针会被添加到父节点中。很明显，父节点也会被填满。然后它也被分割成两个节点，以此类推。如果要拆分根节点，则在产生
的节点上方再创建一个节点，成为树的新根。在这种情况下，树的深度会增加一级。

在这个例子中，TJM机场代码的插入导致了两个节点的分裂；由此产生的新节点在下图中被突出显示。为了确保任何节点都能被
拆分，双向列表绑定了所有级别的节点，而不仅仅是最低级别的节点。

[[./images/Mkersy.png]]

所描述的插入和拆分程序保证了树保持平衡，由于一个节点可以容纳的元素数量通常相当大，所以树的深度很少增加。

问题是，一旦被分割，节点就不能被合并在一起，即使它们在清理回收后只包含很少的元素。这个限制不是与树形数据结构本身有
关，而是与它的Postgre实现有关。因此，如果在尝试插入时，节点被证明是满的，访问方法首先尝试修剪多余的数据，以清
除一些空间，避免额外的分裂。

** 页布局
B树的每个节点需要一个页面。该页的大小定义了节点的容量。

由于页面的分割，树的根部在不同的时间可以由不同的页面来代表。但是搜索算法必须总是从根开始扫描。它在零索引页（称为
metapage）中找到当前根页的ID。metapage还包含一些其他的元数据。

[[./images/Zikz1R.png]]

索引页中的数据布局与我们到目前为止所看到的有点不同。所有的页面，除了每一级的最右边，都包含一个额外的 "高键"，它保
证不比这一页的任何键小。在上图中，高键被突出显示。

让我们使用pageinspect扩展来看看一个建立在六位数的预订参考上的真实索引的一个页面。metapage列出了根页面ID和
树的深度（级别编号从叶子节点开始，以0为基础）：

#+begin_src sql
  select root, level
  from bt_metap('bookings_pkey')
#+end_src

存储在索引项中的键被显示为字节序列，这其实并不方便：

#+begin_src sql
  select data
  from bt_page_items('bookings_pkey', 290)
  where itemoffset=2;
#+end_src

为了破译这些值，我们将不得不写一个特设的函数。它不支持所有的平台，也可能对某些特定的场景不起作用，但它对本章的例子
来说是可行的：

#+begin_src sql
  create function data_to_text(data text)
  returns text
  as $$
  declare
  raw bytea := ('\x' || replace(data, ' ', ''))::bytea;
  pos integer := 0;
  len integer;
  res text := '';
  begin
  while(octet_length(raw) > pos)
  loop
  len := (get_byte(raw, pos)-3)/2;
  exit when len<=0;
  if pos>0 then
  res := res || ', ';
  end if;
  res := res || (
  select string_agg( chr(get_byte(raw, i)), '')
  from generate_series(pos+1, pos+len) i
  );
  pos := pos+len+1;
  end loop;
  return res;
  end;
  $$ LANGUAGE plpgsql
#+end_src

现在我们可以看一下根页面的内容了：

#+begin_src sql
  select itemoffset, ctid, data_to_text(data)
  from bt_page_items('bookings_pkey', 5135);
#+end_src

正如我所说的，第一个条目不包含键。ctid列提供了指向子页面的链接。

需要在booking查找E2D725.在这个例子中，必须选择19到页5135

#+begin_src sql
  select itemoffset, ctid, data_to_text(data)
  from bt_page_items('bookings_pkey', 5135);
#+end_src

本页的第一个条目包含了高调，这可能看起来有点出乎意料。从逻辑上讲，它应该被放在页面的末尾，但从实现的角度来看，把它
放在开头更方便，以避免每次页面内容发生变化时移动它。这里我们选择条目3，然后下到页面11919。

#+begin_src sql
  select itemoffset, ctid, data_to_text(data)
  from bt_page_items('bookings_pkey', 5133);
#+end_src

它是索引的一个叶子页。第一个条目是高键；所有其他条目都指向堆元组

这是bookings元组数据

#+begin_src sql
  select * from bookings
  where ctid='(11919, 77)'
#+end_src

这是bookings表中搜索，低层次执行的操作

#+begin_src sql
  explain (costs off)
  select * from bookings
  where book_ref='E2D725';
#+end_src

重复数据删除

非唯一的索引可能包含很多重复的键，这些键指向不同的堆元组。由于非唯一的键会出现不止一次，因此会占用很多空间，重复的
键会被折叠成一个索引条目，其中包含键和相应元组ID的列表。在某些情况下，这个程序（被称为重复数据删除）可以大大
减少索引的大小。

然而，由于MVCC的原因，唯一索引也可能包含重复的内容：一个索引会保留对表行所有版本的引用。HOT更新的机制可以帮助
你回收因引用过时的、通常是很短的行版本而导致的索引膨胀，但有时它可能并不适用。在这种情况下，重复数据删除可以争取一
些时间来删除多余的堆元组，并避免额外的页面分割。

为了避免在重复数据删除没有带来直接好处的情况下浪费资源，只有在叶子页没有足够的空间来容纳一个更多的元组时才会进行折
叠。然后页面修剪和重复数据删除可以释放一些空间，并防止不希望的页面分割。然而，如果重复的情况很少，你可以通过关闭
deduplicate_items存储参数来停用重复数据删除功能。

一些索引不支持重复数据删除。主要的限制是，键的相等必须通过简单的二进制比较它们的内部表示来检查。到目前为止，并非
所有的数据类型都可以通过这种方式进行比较。例如，浮点数（浮点数和双精度）对零有两种不同的表示。任意精度的数字
（numeric）可以用不同的尺度表示一个相同的数字，而jsonb类型可以使用这样的数字。如果你使用非确定性排序4，允许
相同的符号由不同的字节序列来表示，那么文本类型也不可能进行重复数据删除（标准排序是确定性的）。

此外，对于复合类型、范围和数组，以及用INCLUDE子句声明的索引，目前不支持重复数据删除。

要检查一个特定的索引是否可以使用重复数据删除，你可以看一下其metapage中的allequalimage字段：

#+begin_src sql 
  create index on tickets(book_ref);
  select allequalimage from bt_metap('tickets_book_ref_idx');
#+end_src

在这种情况下，重复数据删除得到了支持。而事实上，我们可以看到，其中一个叶子页既包含有单一元组ID( htid)的索引条目，
也包含有IDS(tids)的列表：

#+begin_src sql
  select itemoffset, htid, left(tids::text, 27) tids,
  data_to_text(data) as data
  from bt_page_items('tickets_book_ref_idx', 1)
  where itemoffset > 1;
#+end_src

内部索引条目的存储

重复数据删除能够在索引的叶子页中容纳更多的条目。但是，即使叶子页构成了索引的大部分，在内部页中进行的数据压缩以防止额
外的分裂也同样重要，因为搜索效率直接取决于树的深度。

内部索引条目包含索引键，但是它们的值只用来确定搜索时要下到的子树。在多列索引中，通常取第一个关键属性（或几个第一属
性）就足够了。其他属性可以被截断以节省页面空间。这种后缀截断发生在一个叶子页被分割，而内部页必须容纳一个新的指针的
时候。

#+begin_comment
  理论上，我们甚至可以更进一步，只保留属性的有意义的部分，例如，一行的前几个符号足以区分子树。但是现在还没有实现：
  一个索引条目要么包含整个属性，要么完全排除这个属性。
#+end_comment

例如，这里是建立在机票表上的索引的根页面的几个条目，包含booking引用的passenger_name列：

#+begin_src sql
  create index tickets_bref_name_idx
  on tickets(book_ref, passenger_name);
  select itemoffset, ctid, data_to_text(data)
  from bt_page_items('tickets_bref_name_idx', 229)
  where itemoffset between 8 and 13;
#+end_src

我们可以看到，一些索引条目没有第二个属性。

当然，叶子页必须保留所有的键属性和INCLUDE列值（如果有的话）。否则，就不可能进行仅有索引的扫描了。唯一的例外是
高键；它们可以被部分保留。

** 操作类

比较语义

除了散列值之外，系统还必须知道如何对各种类型的值进行排序，包括自定义的值。这对于排序、分组、合并连接和其他一些操作来
说是不可缺少的。而就像散列的情况一样，特定数据类型的比较运算符是由运算符类定义的。

操作符类允许我们从名称中抽象出来（如>、<、=），甚至可以提供几种方法来排列同一类型的值。

下面是必须在btree方法的任何运算符类中定义的强制性比较运算符（所示为bool_ops系列）：

#+begin_src sql
  select amopopr::regoperator as opfamily_operator,
  amopstrategy
  from pg_am am
  join pg_opfamily opf on opfmethod=am.oid
  join pg_amop amop on amopfamily=opf.oid
  where amname='btree' and opfname='bool_ops'
  order by amopstrategy;
#+end_src

这五个比较运算符中的每一个都对应于一个策略，这就定义了它们的语义：

1. 小于
2. 小于或等于
3. 等于
4. 大于或等于
5. 大于

一个B-树操作符类还包括几个支持函数。3 第一个必须返回1，如果它的第一个参数大于第二个参数，-1，如果它小于第二个参
数，0，如果参数相等。其他支持函数是可选的，但它们可以提高访问方法的性能。

为了更好地理解这一机制，我们可以定义一个具有非默认排序的新数据类型。文档中给出了一个关于复数的例子，1但它是用C语
言写的。幸运的是，B-树操作符类也可以用解释语言来实现，所以我将利用它，做一个尽可能简单的例子（即使是明知效率低下）。

让我们为信息单元定义一个新的复合类型：

#+begin_src sql
  create type capacity_unit as enum (
   'B', 'kB', 'MB','GB', 'TB','PB'
   );
   create type capacity as (
   amount integer,
   unit capacity_unit
   );
#+end_src

现在创建一个带有新类型的列的表，并用随机值填充它：

#+begin_src sql
  create table test as
  select ((random()*1023)::integer, u.unit)::capacity as cap
  from generate_series(1,100),
     unnest(enum_range(NULL::capacity_units)) as u(unit);
#+end_src

默认情况下，复合类型的值是按词汇表顺序排序的，这与这种特定情况下的自然顺序不一样：

#+begin_src sql
  select * from test order by cap;
#+end_src

现在让我们开始创建我们的操作者类。我们将首先定义一个将体积转换为字节的函数：

#+begin_src sql
  create function capacity_to_bytes(a capacity) returns numeric
  as $$
  select a.amount::numeric *
  1024::numeric ^ (array_position(enum_range(a.unit), a.unit) -1 );
  $$ language sql strict immutable;
  select capacity_to_bytes((1, kB)::capacity);
#+end_src

为该数据类型创建一个操作者函数

#+begin_src sql
  create function capacity_cmp(a capacity, b capacity)
  returns integer
  as $$
  select sign(capacity_to_bytes(a)-capacity_to_bytes(b));
  $$ language sql strict immutable;
#+end_src

现在，使用这个支持函数来定义比较运算符是很容易的。我故意使用奇特的名字来证明它们可以是任意的：

#+begin_src sql
  create function capacity_lt(a capacity, b capacity) returns boolean
  as $$
  begin
  return capacity_cmp(a,b)<0;
  end;
  $$ language plpgsql immutable strict;

  create operator #<# (
  leftarg=capacity,
  rightarg=capacity,
  function=capacity_lt
  );
#+end_src

其他四个运算符的定义与此类似。

#+begin_src sql
  create function capacity_le(a capacity, b capacity) returns boolean
  as $$
  begin
  return capacity_cmp(a,b) <= 0;
  end;
  $$  language plpgsql immutable strict;
  create operator #<=# (
  leftarg=capacity,
  rightarg=capacity,
  function=capacity_le
  );
#+end_src

#+begin_src sql
  create function capaity_eq(a capacity, b capacity) returns boolean
  as $$
  begin
  return capacity_cmp(a,b) = 0;
  end;
  $$ language plpgsql immutable strict;
  create operator #=#(
  leftarg=capacity,
  rightarg=capacity,
  function=capacity_eq,
  merges -- can be used in merge joins
  );
#+end_src

#+begin_src sql
  create function capacity_ge(a capacity, b capacity) returns boolean
  as $$
  begin
  return capacity_cmp(a, b) >= 0;
  end;
  $$ language plpgsql immutable strict;
  create operator #>=#(
  leftarg=capacity,
  rightarg=capacity,
  function=capacity_ge
  );
#+end_src

#+begin_src sql
  create function capacity_gt(a capacity, b capacity) returns boolean
  as $$
  begin
  return capacity_cmp(a,b) > 0;
  end;
  $$ language plpgsql immutable strict;
  create operator #>#(
  leftarg=capacity,
  rightarg=capacity,
  function=capacity_gt
  );
#+end_src

在这个阶段，我们已经可以进行比较:

#+begin_src sql
  select (1, 'MB')::capacity #># (512, 'KB')::capacity
#+end_src

一旦操作者类被创建，排序也将按预期开始工作：

#+begin_src sql
  create operator class capacity_ops
  default for type capacity -- to be used by default
  using btree as
  operator 1 #<#,
  operator 2 #<=#,
  operator 3 #=#,
  operator 4 #>=#,
  operator 5 #>#,
  function 1 capacity_cmp(capacity, capacity);
  select * from test order by cap;
#+end_src

当一个新的索引被创建时，我们的操作者类被默认使用，这个索引以正确的顺序返回结果：

#+begin_src sql
  create index on test(cap);
  select * from test where cap #<# (100, 'B')::capacity
  order by cap;
  explain (consts off) select *
  from test where cap #<# (100, 'B')::capacity order by cap;
#+end_src

在平等运算符声明中指定的MERGE子句可以对该数据类型进行合并连接。

多列索引和排序

让我们仔细看看多列索引的排序问题。

首先，在声明索引时，选择最佳的列序是非常重要的：页面内的数据排序将从第一列开始，然后转到第二列，以此类推。多列索引
只有在所提供的过滤条件从第一列开始跨越一个连续的序列时才能保证高效的搜索：第一列、前两列、第一列和第三列之间的范围，
等等。其他类型的条件只能用于过滤掉根据其他标准获取的多余的值。

下面是在tickets表上创建的索引的第一叶页中的索引条目顺序，其中包括tickets引用的乘客姓名：

#+begin_src sql
  select itemoffset, data_to_text(data)
  from bt_page_items('tickets_bref_name_idx', 1)
  where itemoffset > 1;
#+end_src

在这种情况下，只有通过订票信息和乘客姓名，或仅通过订票信息，才能有效地搜索到票。

#+begin_src sql
  explain (costs off) select *
  from tickets
  where book_ref = '000010';
#+end_src

#+begin_src sql
  explain (costs off) select *
  from tickets
  where book_ref='000010' and passenger_name='LYUDMILA BOGDANOVA';
#+end_src

但如果我们决定寻找一个乘客的名字，我们必须扫描所有的行：

#+begin_src sql
  explain (costs off) select *
  from tickets
  where passenger_name='LYUDMILA BOGDANOVA';
#+end_src

即使规划器选择执行索引扫描，所有的索引条目仍然要被遍历。不幸的是，规划器不会显示条件实际上只用于过滤结果。

#+begin_comment
如果第一列没有太多不同的值v1, v2, ... vn，那么在相应的子树上执行几次就会有好处，实际上就是用一系列的搜索条件
"col2 = value "来代替一次搜索：col1 = v1 and col2 = value col1 = v2 and col2 = value ⋯ col1 = vn
and col2 = value 这种类型的索引访问被称为跳过扫描，但是它还没有实现。
#+end_comment

反之亦然，如果在乘客姓名和订票号码上创建索引，它将更适合通过乘客姓名单独查询，或者同时查询乘客姓名和订票参考：

#+begin_src sql
  create index tickets_name_bref_idx
  on tickets(passenger_name, book_ref);
  select itemoffset, data_to_text(data)
  from bt_page_items('tickets_name_bref_idx', 1)
  where itemoffset > 1;
  explain (costs off) select * from tickets
  where passenger_name='LYUDMILA BOGDANOVA';
#+end_src

除了列的顺序之外，在创建新的索引时，你还应该注意排序顺序。默认情况下，数值是按升序排序的（ASC），但如果需要的话，你
可以把它倒过来（DESC）。如果一个索引是在单列上建立的，这并不重要，因为它可以在任何方向上被扫描。但是在一个多列索引
中，顺序就变得很重要了。

我们新创建的索引可以用来检索按两列升序或降序排序的数据：

#+begin_src sql
  explain(costs off) select *
  from tickets
  order by passenger_name, book_ref;
  explain (costs off) select *
  from tickets order by passenger_name desc, book_ref desc;
#+end_src

但是，如果需要同时按一列升序排序和按另一列降序排序，这个索引就不能马上返回数据。在这种情况下，索引提供的是部分排序
的数据，必须按第二个属性进一步排序：

#+begin_src sql
  explain (costs off) select *
  from tickets order by passenger_name asc, book_ref desc;
#+end_src

NULL值的位置也会影响到使用索引进行排序的能力。默认情况下，NULL值在排序时被认为比普通值 "大"，也就是说，如果排序
顺序是升序，它们就位于树的右侧，如果排序顺序是降序，它们就位于左侧。NULL值的位置可以通过NULL LAST和NULL
FIRST子句来改变。

在下一个例子中，索引不满足ORDER BY子句，所以必须对结果进行排序：

#+begin_src sql
  explain (costs off) select *
  form tickets order by passenger_name nulls first, book_ref desc;
#+end_src

但是，如果我们创建一个遵循所需顺序的索引，它将被使用：

#+begin_src sql
  create index tickets_name_bref_idx2
  on tickets(passenger_name nulls first, book_ref desc);
  explain (costs off) select *
  from tickets order by passenger_name nulls first, book_ref desc;
#+end_src

** 属性
让我们来看看B树的接口属性。

访问方法属性

#+begin_src sql
  select a.amname, p.name, pg_indexam_has_property(a.oid, p.name)
  from pg_am a, unnest(array[
  'can_order', 'can_unique', 'can_multi_col',
  'can_exclude', 'can_include']) p(name)
  where a.amname='btree';
#+end_src

B树可以对数据进行排序并确保其唯一性。它是唯一具有这种特性的访问方法。

许多访问方法都支持多列索引，但由于B树中的值是有序的，所以你必须密切注意索引中各列的顺序。

从形式上看，支持排除式约束，但它们仅限于平等条件，这使得它们类似于唯一约束。使用成熟的唯一约束更为可取。

B树索引也可以用额外的不参与搜索的INCLUDE列来扩展。

索引级的属性

#+begin_src sql
  select p.name, pg_index_has_property('flights_pkey', p.name)
  from unnest(array[
  'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'
  ])p(name)
#+end_src

B树索引可用于集群化。

索引扫描和位图扫描都被支持。由于叶子页被绑定成一个双向的列表，一个索引也可以被反向遍历，这将导致反向的排序顺序：

#+begin_src sql
  explain (costs off) select *
  from bookings order by book_ref desc;
#+end_src

列级属性

#+begin_src sql
  select p.name
  pg_index_column_has_property('flight_pkey', 1, p.name)
  from unnest(array[
  'asc', 'desc', 'nulls_first', 'nulls_last', 'orderable',
  'distance_orderable', 'returnable', 'search_array', 'search_nulls'
  ])p(name);
#+end_src

ORDERABLE 属性表示存储在 B 树中的数据是有序的，而前四个属性（ASC 和 DESC、NULLS FIRST 和 NULLS LAST）定义
了特定列中的实际顺序。在这个例子中，列值是按升序排序的，NULL值列在最后。

SEARCH NULLS 属性指示是否可以搜索 NULL 值。

B树不支持排序运算符（DISTANCE ORDERABLE），尽管已经试图实现它们。

B-树支持在一个数组中搜索多个元素（SEARCH ARRAY属性），并且可以在不访问堆的情况下返回结果数据（RETURNABLE）。



* GiST

** 概要
GiST(广义搜索树)是一种访问方法，它实际上是平衡搜索树的广义化，适用于支持值的相对定位的数据类型。B-树的适用性仅限于
允许比较操作的序数类型（但为此类类型提供的支持极为高效）。至于GiST，它的操作符类允许为数据在树中的分布定义任意的标
准。GiST索引可以容纳空间数据的R树、集合的RD树以及任何数据类型（包括文本和图像）的签名树。

得益于可扩展性，你可以通过实现索引引擎的接口，在 PostgreSQL 中从头开始创建一个新的访问方法。然而，除了设计索引逻
辑，您还必须定义页面布局、有效的锁定策略和WAL支持。这一切都需要强大的编程能力和大量的实施工作。GiST简化了这一任
务，解决了所有底层技术问题，并为搜索算法提供了基础。要在新的数据类型中使用GiST方法，只需添加一个新的运算符类，其
中包括十几个支持函数。与为B树提供的琐碎的操作符类不同，这样的类包含了大部分的索引逻辑。在这方面，GiST可以被视为
构建新访问方法的框架。

一般来说，属于叶子节点的每个条目（叶子条目）都包含一个谓词（逻辑条件）和一个堆元组 ID。索引关键字必须满足谓词的要
求；至于关键字本身是否属于该条目，则无关紧要。

内叶中的每个条目（内条目）也包含一个谓词和一个指向子节点的引用；子子树的所有索引数据都必须满足这个谓词。换句话说，内
部条目的谓词是其所有子条目谓词的联合。GiST的这一重要特性为B树的简单排序提供了服务。

GiST树搜索依赖于一致性函数，这是运算符类定义的支持函数之一。

一致性函数在索引条目上调用，以确定该条目谓词是否与搜索条件（"索引列运算符表达式"）"一致"。对于内条目，它显示我们是
否必须进入相应的子树；对于叶条目，它检查其索引键是否满足条件。

搜索从根节点开始，这是典型的树形搜索。一致性函数决定了哪些子节点必须遍历，哪些可以跳过。与B树不同，GiST索引可能
有多个这样的节点。由一致性函数选择的叶节点条目将作为结果返回。

搜索总是深度优先的：算法试图尽快进入叶页。因此，它可以立即开始返回结果，如果用户只需要得到前几行，这就非常有意义了。

要在 GiST树中插入一个新值，不可能使用一致性函数，因为我们需要选择一个节点作为下行节点。

就像在 B 树中一样，被选中的节点可能没有空闲空间，从而导致拆分。这一操作还需要两个函数，其中一个在新旧节点之间分配条
目；另一个形成两个谓词的联合，以更新父节点的谓词。

随着新值的不断增加，现有的谓词也在不断扩充，通常只有在拆分页面或重建整个索引时才会缩小谓词的范围。因此，GiST索引的
频繁更新会导致其性能下降。

由于所有这些理论讨论可能显得过于空泛，而且确切的逻辑大多取决于特定的运算符类，因此我将提供几个具体的例子。

** 积分R树

第一个示例涉及平面上点（或其他几何图形）的索引。常规的B树不能用于这种数据类型，因为没有为点定义比较运算符。显然，
我们可以自己实现这样的运算符，但几何图形需要索引支持完全不同的运算。我将只介绍其中的两种：搜索特定区域内的对象和最近
邻搜索。

B-树在平面上绘制矩形，这些矩形必须覆盖所有索引点。索引项存储了边界框，谓词可定义如下：点位于此边界框内。

B型树的根节点包含几个大矩形（也可能重叠）。子节点包含与其父节点相匹配的较小矩形，它们共同覆盖所有底层点。

叶节点应包含索引点本身，但GiST要求所有条目具有相同的数据类型；因此，叶条目也用矩形表示，矩形被简单地简化为点。

为了更直观地展示这种结构，让我们来看一看在机场坐标上建立的三层 B 树。在这个示例中，我将演示数据库中的机场表扩展到了
五千行。此外，我还降低了填充因子值，以使树更深；默认值将为我们提供一个单层树。

#+begin_src sql
  create table airports_big as
  select * from airports_big;
  copy airports_big from
  '/home/student/internals/airports/extra_airports.copy';
  create index airports_gist_idx on airports_big
  using gist(coordinates) with (fillfactor=10);
#+end_src

在上层，所有的点都包含在几个（部分重叠的）边界框中：

[[./images/kKZ3ML.png]]

在下一层，大矩形被分割成小矩形：

[[./images/Poc8It.png]]


最后，在树的内层，每个边界框包含的点数与单页所能容纳的点数相同：

[[./images/zboFkn.png]]


该索引使用point_ops运算符类，这是唯一可用的点运算符类。

矩形和任何其他几何图形都可以用同样的方式进行索引，但索引必须存储对象的边界框，而不是对象本身。

页面布局

可以使用pageinspect扩展来研究GiST页面。

与B树索引不同，GiST没有元页，零页总是树的根。如果根页面被拆分，旧的根页面将被移至一个单独的页面，而新的根页面将
取而代之。

以下是根页面的内容：

#+begin_src sql
  select ctid, keys
  from gist_page_items(
   get_raw_page('airports_gist_idx', 0), 'airports_gist_idx'
   );
#+end_src


这四行对应于第一幅图中上层的四个矩形。遗憾的是，这里的键是以点的形式显示的（这对叶页来说是合理的），而不是以矩形的形
式显示的（这对内页来说更合理）。但我们可以随时获取原始数据并自行解释。

#+begin_comment
  要提取更详细的信息，可以使用gevel扩展，它不包含在标准的PostgreSQL发行版中。
#+end_comment

操作类

下面的查询返回实现树的搜索和插入操作逻辑的支持函数列表：

#+begin_src sql
  select amprocnum, amproc::regproc
  from pg_am am
  join pg_opclass opc on opcmethod=am.oid
  join pg_amproc amop on amprocfamily=opcfamily
  where amname='gist'
  and opcname='point_ops'
  order by amprocnum;
#+end_src


上面已经列出了必须具备的功能：

1. 在搜索过程中用于遍历树的一致性函数
2. union函数合并矩形
3. penalty函数，用于在插入条目时选择下行子树
4. picksplit函数，用于在页面分割后在新页面之间分配条目
5. same函数检查两个键是否相等

point_ops运算符类包括以下运算符：

#+begin_src sql
  select amopopr::regoperator, amopstrategy as st, oprcode::regproc,
  left(obj_description(opr.oid, 'pg_operator'), 19) descrioption
  from pg_am am
  join pg_opclass opc on opcmethod = am.oid
  join pg_amop amop on amopfamily = opcfaimly
  join pg_operator opr on opr.oid = amopopr
  where amname = 'gist'
  and opcname = 'point_ops'
  order by amopstrategy;
#+end_src

运算符名称通常不会告诉我们太多关于运算符的语义，因此该查询也会显示底层函数的名称及其描述。所有运算符都以这种或那种
方式处理几何图形的相对位置（左侧、右侧、上方、下方、包含、被包含）以及它们之间的距离。

与 B 树相比，GiST 提供了更多的策略。有些策略编号是几种索引1 的共用编号，而有些则是通过公式计算得出的（例如，28、
48和68实际上代表了同一种策略：矩形、多边形和圆形都包含这种策略）。此外，GiST还支持一些过时的运算符名称
（<<| 和 |>>）。

操作符类可能只实现部分可用策略。例如，点的运算器类不支持包含策略，但是在为具有可测量面积的几何体定义的类（box_ops、
poly_ops 和 circle_ops）中可以使用包含策略。

容器元素的搜索

一个典型的查询可以通过索引加快速度，返回指定区域内的所有点。

例如，我们可以找到所有距离莫斯科市中心1度以内的机场：

#+begin_src sql
  select airport_code, airport_name->>'en'
  from airports_big
  where coordinates <@ '<(37.622513, 55.753220), 1.0>'::circle;

  expalin (costs off) select airport_code
  from airports_big
  where coordinates <@ '<(37.622513, 55.753220), 1.0>'::circle;
#+end_src

我们可以通过下图中的一个微不足道的例子来详细了解这个运算符：

[[./images/h0GqE7.png]]


如果以这种方式选择边界框，索引结构将如下所示：

[[./images/anbOi7.png]]

包含运算符 <@ 确定特定点是否位于指定的矩形内。如果索引项的矩形与该矩形有公共点，则该操作符的一致性函数返回 "yes"。
这意味着，对于叶节点条目（存储简化为点的矩形），该函数确定点是否包含在指定的矩形中。

例如，让我们找出矩形(1,2)-(4,7)的内点，如下图所示：

[[./images/Hka1hm.png]]


[[./images/F4qXea.png]]


搜索从根节点开始。边界框与(0,0)-(3,4)重叠，但与(5,3)-(9,9)不重叠。这意味着我们不必进入第二个子树。

在下一层，边界框与 (0,3)-(3,4) 重叠，并触及 (0,0)-(3,2) ，因此我们必须检查这两个子树。

一旦我们到达叶节点，我们只需遍历其包含的所有点，并返回满足一致性函数的点。

B树搜索总是选择一个子节点。然而，GiST搜索可能需要扫描多个子树，特别是当它们的边界框重叠时。

最近邻搜索

索引支持的大多数操作符（如前面示例中的=或<@）通常被称为搜索操作符，因为它们定义了查询中的搜索条件。这些操作符是谓词，
也就是说，它们返回一个逻辑值。

但是还有一组排序操作符，它们返回参数之间的距离。此类操作符在ORDER BY子句中使用，通常由具有DISTANCE ORDERABLE
属性的索引支持，它能够快速查找指定数量的近邻。这种类型的搜索被称为k-NN或k-近邻搜索。

例如，我们可以找到离科斯特罗马最近的10个机场：

#+begin_src sql
  select airport_code, airport_name->>'en'
  from airports_big
  order by coordinates <-> '(40.926780, 57.767943)'::point
  limit 10;

  explain (costs off) select airport_code
  from airports_big
  order by coordinates <-> '(40.926780, 57.767943)'::point
  limit 5;
#+end_src

由于索引扫描会逐个返回结果，并且可以随时停止，因此可以很快找到几个首值。

#+begin_comment
 如果没有索引支持，很难实现高效搜索。我们必须找到特定区域内出现的所有点，然后逐步扩大该区域，直到返回所需的结果数。
 这将需要多次索引扫描，更不用说选择原始区域大小及其增量的问题了。
#+end_comment

您可以在系统目录中看到运算符类型（"s "表示搜索，"o "表示排序运算符）：

#+begin_src sql 
        select amopopr::regoperator, amoppurpose, amopstrategy 
      from pg_am am 
      join pg_opclass opc on opcmethod=am.oid 
      join pg_amop amop on amopfamily=opcfamily 
      where amname='gist' 
      and opcname='point_ops'  
  order by amopstrategy;
#+end_src

为了支持这种查询，操作符类必须定义一个额外的支持函数：它是距离函数，在索引条目上调用它来计算从存储在这个条目中的值
到其他值的距离。

对于表示索引值的叶元素，该函数必须返回与该值的距离。在点的情况下，它是一个常规的欧几里得距离，等于
#+begin_export latex
\sqrt((x_{2}-x_{1})^{2}-(y_{2}-y_{1})^{2}
#+end_export

对于一个内部元素，函数必须返回它与其子叶元素之间所有可能距离的最小值。由于扫描所有子元素的成本很高，函数可以乐观地
低估距离（牺牲效率），但绝不能返回更大的值--这会影响搜索的正确性。

因此，对于由边界框表示的内部元素，点到点的距离可以从常规数学意义上理解：要么是点到矩形之间的最小距离，要么如果点在
矩形内部则距离为零。这个值很容易计算，不需要遍历矩形的所有子点，并且保证不大于到这些点中任何一点的距离。

让我们考虑搜索点(6,8)的三个近邻的算法：

[[./images/AO2UAo.png]]


搜索从根节点开始，根节点包含两个边界框。从指定点到矩形 从指定点到矩形(0,0)-(3,4)的距离为到矩形角(3.4)的距离，等于
5.o。到(5,3)-(9,9)的距离为 0.0。这里所有的值都四舍五入到小数点后第一位；这样的精度在本例中就足够了。对于本例来说，
这样的精度已经足够了）。

我们再次选择右侧子树，进入包含三个点的叶节点：距离为 2.0 的 (6,6)，距离为 2.2 的 (8,9)，以及距离为 3.2 的
(9,7)。

[[./images/xXOaiY.png]]

因此，我们得到了前两个点：(6,6) 和 (8,9)。但是到这个节点的第三个点的距离大于到矩形(5,3)-(8,5)的距离。

因此，现在我们必须进入包含两个点的左侧子节点。到点 (8,5) 的距离是 3.6，而到点 (5,3) 的距离是 5.1。事实证明，前
一个子节点中的点(9,7)比左侧子树的任何一个节点都更靠近点(6,8)，因此我们可以将其作为第三个结果返回。

[[./images/UenfK8.png]]

本例说明了内部条目的距离函数必须满足的要求。由于矩形(5,3)-(8,5)的距离减小（从3.6减小到3.0），多扫描了一个节
点，因此搜索效率下降；但是算法本身仍然是正确的。

插入操作

当一个新的键插入到B树中时，该键所使用的节点由penalty函数决定：边界框的大小必须尽可能小。

例如，点(4,7)将被添加到矩形(5,3)-(9,9)中，因为其面积仅增加 6 个单位，而矩形(0,0)-(3,4)的面积将增加 12 个单位。
在下（叶）层，按照同样的逻辑，该点将添加到矩形(6,6)-(9,9)中。
[[./images/sdy7bb.png]]


假设一个页面最多包含三个元素，则必须将其一分为二，并在新的页面之间分配元素。在这个例子中，结果似乎是显而易见的，但在
一般情况下，数据分配任务并不那么简单。首先，picksplit函数试图最小化边界框之间的重叠，目的是获得更小的矩形和页面之
间点的均匀分布。

[[./images/QpjdZO.png]]


排他性约束

GiST索引也可用于排除约束中。

排除约束保证任意两个堆元组的指定字段在某些操作符的意义上不相互匹配。必须满足以下条件：

+ 排除约束必须由索引方法（CAN Exclude属性）支持。
+ 操作符必须属于该索引方法的操作符类。
+ 运算符必须是交换的，即 "a运算符b = b运算符a "的条件必须为真。

对于上面提到的哈希树和 btree 访问方法，唯一合适的操作符是 equal to。它实际上将一个排除约束变成了唯一约束，这并不
是特别有用。

GiST方法有两种更适用的策略：

+ 重叠：&&运算符
+ djacency: -|- 运算符 (为区间定义)

为了尝试一下，让我们创建一个约束条件，禁止机场之间距离太近。这个条件可以表述为：以机场坐标为圆心的特定半径的圆不能
重叠：

#+begin_src sql
  alter table aiports_data add exclude
  using gist (circle(coordinates, 0.2) with &&);
  insert into aiports_data(
  aiport_code, airport_name, city, coordinates, timezone
  ) values (
   'ZIA', '{}', '{"en":"Moscow"}', point(38.1517, 55.5533),
   'Europe/Moscow'
   );
#+end_src

当定义排除约束时，会自动添加一个用于执行该约束的索引。这里是在表达式上建立的GiST索引。

让我们来看一个更复杂的例子。假设我们需要允许机场之间的距离很近，但前提是它们必须属于同一个城市。一个可能的解决方案
是定义一个新的完整性约束，它可以表述如下：如果圆的中心位于机场坐标处，并且相应的城市名称不同，则禁止存在圆相交(&&)
的行对(!=)。

由于没有文本数据类型的操作符类，尝试创建这样的约束会导致错误：

#+begin_src sql
  alter table aiports_data
  drop constraint aiports_data_circle_excl;
  alter table aiports_data add exclude using gist(
  circle(coordinates, 0.2) with && ,
          (city->>'en') with !=
          );
#+end_src

然而，GiST 确实提供了诸如 strictly left of、 strictly right of 和 same 等策略，这些策略也可以应用于常规的
序数数据类型，例如数字或文本字符串。btree_gist扩展专门用于实现GiST对通常用于B树的操作的支持：

#+begin_src sql
  create extension btree_gist;
  alter table aiports_data add exclude using gist(
   circle(coordinates, 0.2) with &&,
       (city->>'en') with !=
       );
#+end_src

约束条件已创建。现在我们不能添加属于同名城镇的茹科夫斯基机场，因为莫斯科机场太近了：

#+begin_src sql
  insert into aiports_data(
  airport_code, aiport_name, city, coordinates, timezone
  )values (
  'ZIA', '{}', '{"en":"Zhukovskey"}', point(38.1517, 55.5533),
  'Europe/Moscow');
#+end_src

但是，如果我们指定莫斯科为该机场的城市，我们就可以做到这一点：

#+begin_src sql
  insert into aiports_data(
  aiport_code, aiport_name, city, coodrinates, timezone
  ) values (
  'ZIA', '{}', '{"en":"Moscow"}', point(38.1517, 55.5533),
  'Europe/Moscow');
#+end_src

需要记住的是，尽管 GiST 支持大于、小于和等于操作，但 B 树在这方面的效率要高得多，尤其是在访问数值范围时。因此，只
有当GiST索引确实是出于其他合理的原因时，使用上面的btree_gist扩展才是有意义的。

属性

访问方法属性，GiST方法有如下属性

#+begin_src sql
  select a.amname, p.name, pg_indexam_has_property(a.oid, p.name)
  from pg_am a, unnest(array[
  'can_order', 'can_unique', 'can_multi_col',
  'can_exclude', 'can_include'
  ]) p(name)
  where a.amname='gist';
#+end_src

不支持唯一约束和排序。

GiST索引可以创建额外的INCLUDE列

我们知道，我们可以在多个列上建立索引，也可以在完整性约束中使用索引。

索引级属性 这些属性在索引级别上定义：

#+begin_src sql
  select p.name, pg_index_has_property('airports_gist_idx', p.name)
  from unnest(array[
  'clusterable', 'index_scan', 'bitmap_scan', 'backward_scan'
  ])p(name);
#+end_src

GiST索引可用于聚类。

在数据检索方法方面，支持常规（逐行）索引扫描和位图扫描。但是，不允许对GiST索引进行后向扫描。

列级属性

#+begin_src sql
  select p.name,
  pg_index_column_has_property('airports_gist_idx', 1, p.name)
  from unnest(array[
  'orderable', 'search_array', 'search_nulls'
  ])p(name);
#+end_src

禁用所有与排序相关的属性。

NULL值是允许的，但是GiST在处理NULL值时并不有效。我们假定NULL值不会增加边界框；这样的值会被插入到随机子
树中，因此必须在整棵树中进行搜索。

然而，有几个列级属性确实取决于特定的操作符类：

#+begin_src sql
  select p.name,
  pg_index_column_has_property('airports_gist_idx', 1, p.name)
  from unnest(array[
  'returnable', 'distance_orderable'
  ])p(name);
#+end_src

由于叶节点保留完整的索引键，因此允许只对索引进行扫描。

如上所述，该操作符类为近邻搜索提供了距离操作符。到NULL值的距离被认为是NULL；这样的值最后返回（类似于B树中的
NULLS LAST子句）。

但是，对于范围类型（表示线段，即线性几何图形，而不是等值几何图形），没有距离运算符，因此对于为此类类型建立的索引，
该属性是不同的：

#+begin_src sql
  create table reservations(during tsrange);
  create index on reservations using gist(during)

  select p.name,
  pg_index_column_has_property('reservations_during_idx', 1, p.name)
  from unnest(array[
  'returnable', 'distance_orderable'
  ]) p(name);
#+end_src

** 用于全文搜索的RD树

关于全文搜索

全文搜索的目的是从所提供的文档集中选择与搜索查询相匹配的文档。

要进行搜索，需要将文档转换为 tsvector 类型，其中包含词目及其在文档中的位置。词目是转换成适合搜索的格式的单词。默
认情况下，所有单词都被归一化为小写，并截去词尾：

#+begin_src sql
  set default_text_search_config = english;
  select to_tsvector(
   'No one can tell me, nobody knows, ' ||
   'Where the wind comes from, where the wind goes.'
   );
#+end_src

所谓的停顿词（如 "the "或 "from"）会被过滤掉：因为它们出现的频率太高，搜索无法返回任何有意义的结果。当然，所有这
些转换都是可配置的。

搜索查询用另一种类型来表示：tsquery。任何查询都包括一个或多个由逻辑连接词绑定的词素：& (AND)、| (OR)、! (NOT)。
您还可以使用括号定义运算符优先级。

#+begin_src sql
  select to_tsquery('wind & (comes | goes)');
#+end_src

用于全文搜索的唯一操作符是匹配操作符 @@：

#+begin_src sql
  select amopopr::regoperator, oprcode::regproc, amopstrategy
  from pg_am am
  join pg_opclass opc on opcmethod=am.oid
  join pg_amop amop on amopfamily=opcfamily
  join pg_operator opr on opr.oid=amopopr
  where amname='gist'
  and opcname='tsvector_ops'
  order by amoostrategy;
#+end_src

该操作符决定文档是否满足查询要求。下面是一个示例：

#+begin_src sql
  select to_tsvector('Where the wind comes from, where the wind goes')
  @@ to_tsvector('wind & coming');
#+end_src

这绝不是对全文搜索的详尽描述，但这些信息应足以让我们了解索引编制的基本原理。

为了快速工作，全文搜索必须有索引支持。由于被索引的不是文档本身，而是 tsvector 值，因此你有两种选择：要么在表达式
上建立索引并执行类型转换，要么为 tsvector 类型添加一个单独的列并为该列建立索引。第一种方法的好处是不会浪费任何空间
来存储 tsvector 值，而实际上并不需要这些值。但它比第二种方法慢，因为索引引擎必须重新检查访问方法返回的所有堆元组。
这意味着，每重新检查一行，tsvector 值都要重新计算一次，而我们很快就会看到，GiST会重新检查所有的行。

让我们举一个简单的例子。我们要创建一个双列表：第一列存储文档，第二列存储 tsvector 值。我们可以使用触发器来更新第二
列 ，但更方便的做法是简单地将这一列声明为已生成：

#+begin_src sql
  create table ts(
   doc text,
   doc_tsv tsvecotr generated always as (
    to_tsvector('pg_catalog.english', doc)
    )stored
    );
    create index ts_gist_idx on ts
    using gist(doc_tsv);
#+end_src

#+begin_comment
 在上面的示例中，我使用了带有单参数的 to_tsvector 函数，并设置了 english default_text_search_config 参数
 来定义全文搜索配置。这个函数的波动类别是 STABLE，因为它隐式地依赖于参数值。但在这里，我应用了另一种明确定义配置
 的变量；这种变量是 IMMUTABLE 的，可以在生成表达式中使用。
#+end_comment

插入一些行
#+begin_src sql
  insert into ts(doc)
  values
  ('Old MacDonald had a farm'),
  ('And on his farm he had some cows'),
  ('Here a moo, there a moo'),
  ('Everywhere a moo moo'),
  ('Old Macdonald had a farm'),
  ('And on his farm he had some chicks'),
  ('Here a cluck, there a cluck'),
  ('Everywhere a cluck cluck'),
  ('Old Macdonald had a farm'),
  ('And on his farm he had some pigs'),
  ('here an oink, there an oink'),
  ('Everywhere an oink oink')
  return doc_tsv;
#+end_src

因此，R 树对于索引文档毫无用处，因为边界框的概念对它们毫无意义。因此，我们使用了 RD 树（俄罗斯娃娃）的改进版。这种
树使用边界集来代替边界框，即包含其子集所有元素的集合。在全文检索中，这种集合包含文档的词目，但在一般情况下，边界集合
可以是任意的。

在索引条目中表示边界集有几种方法。最简单的方法是枚举集合的所有元素。

它可能是这样的

[[./images/vzJXK6.png]]

要找到满足 DOC_TSV @@ TO_TSQUERY('cow')条件的文档，我们需要深入到已知子条目包含 "cow"词素的节点。

这种表示法的问题显而易见。文档中的词目数量可能非常庞大，而页面大小却有限。即使每份特定的文档单独来看没有太多不同的
词目，但在树的上层，它们的联合集仍然可能太大。

[[./images/juU0GL.png]]

全文搜索使用另一种解决方案，即更紧凑的签名树。使用过 Bloom 过滤器的人应该对这种方法不陌生。

每个词素都可以用它的签名来表示：一个特定长度的比特串，其中只有一个比特被置 1。

文档的签名是对该文档中所有词条的签名进行比特 OR 运算的结果。

| Suppose we have        | chick                              | 1000000 |
| assigned the following | cluck                              | 0001000 |
| signatures to our      | cow                                | 0000010 |
| lexemes:               | everywher                          | 0010000 |
|                        | farm                               | 0000100 |
|                        | macdonald                          | 0100000 |
|                        | moo                                | 0000100 |
|                        | oink                               | 0000010 |
|                        | old                                | 0000001 |
|                        | pig                                | 0010000 |
| Then the documents'    | Old MacDonald had a farm           | 0100101 |
| signatures will be as  | And on his farm he had some cows   | 0000110 |
| follows:               | Here a moo, there a moo            | 0000100 |
|                        | Everywhere a moo moo               | 0010100 |
|                        | And on his farm he had some chicks | 1000100 |
|                        | Here a cluck, there a cluck        | 0001000 |
|                        | Everywhere a cluck cluck           | 0011000 |
|                        | And on his farm he had some pigs   | 0010100 |
|                        | Here an oink, there an oink        | 0000010 |
|                        | Everywhere an oink oink            | 0010010 |

索引树可以这样表示

[[./images/5hTISQ.png]]

这种方法的优点显而易见：索引条目大小相同，而且相当小，因此索引相当紧凑。但也有一些缺点。首先，不可能只执行索引扫描，
因为索引不再存储索引键，每个返回的 TID 都必须由表重新检查。准确性也会受到影响：索引可能会返回许多误报，这些误报必
须在重新检查时过滤掉。

让我们再来看看 DIC_TSV @@ TO_TSQYERT(‘cows')条件。查询签名的计算方法与文档签名的计算方法相同；在本例中，查询
签名等于 0000010。一致性函数1必须找到所有在签名中设置了相同位的子节点：

[[./images/cj13sM.png]]

与前面的例子相比，由于存在假阳性命中，这里需要扫描更多的节点。由于签名的容量是有限的，所以在一个大的词集中必然会有
一些词具有相同的签名。在本例中，"cow "和 "oink "就是这样的词素。这意味着同一个签名可以匹配不同的文档；在这里，查
询的签名对应三个文档。

假阳性会降低索引的效率，但丝毫不会影响索引的正确性：因为假阴性是可以保证排除的，所以所需的值不会被漏掉。

显然，签名的大小实际上更大。默认情况下，签名大小为 124 字节（992bits），因此发生碰撞的概率远低于本例。如果需要，
可以使用操作符类参数将签名大小进一步增加到 2000 字节左右：

#+begin_src sql
  create index ... using gist(column tsvector_ops(siglen = size ));
#+end_src

此外，如果值足够小（比页面的十六分之一小一点，一个标准页面大约需要 500 字节），tsvector_ops 运算符类保存在索引叶
页中的是 tsvector 值本身，而不是它们的签名。

为了了解索引是如何在真实数据上运行的，我们可以使用 pgsql-hackers 邮件列表存档。它包含 356125 封电子邮件及其发
送日期、主题、作者姓名和正文。

让我们添加一个 tsvector 类型的列并建立一个索引。在这里，我将三个值（主题、作者和正文）合并为一个向量，以显示文件
可以动态生成，而不必存储在单列中。

#+begin_src sql
  alter table mail_messages add column tsv tsvector
  generated always as ( to_tsvector(
  'pg_catalog.english', subject || ' ' || author || ' ' || body_plain
  )) stored;
#+end_src

#+begin_src sql
  create index mail_gist_idx on mail_message using gist(tsv);
  select pg_size_pertty(pg_relation_size('mail_gist_idx'));
#+end_src

在填充列的过程中，由于词的大小，一定数量的大词被过滤掉了。但索引一旦准备就绪，就可以用于搜索查询：

#+begin_src sql
  explain (analyze, costs off, timing off, summary off)
  select *
  from mail_messages
  where tsv @@ to_tsquery('magic & value');
#+end_src

除了满足条件的 898 条记录外，访问方法还返回了 7859 条记录，这些记录将在之后的重新检查中被过滤掉。如果我们增加签名
容量，准确性（以及索引效率）将会提高，但索引大小也会增加：

#+begin_src sql
  drop index mail_message_tsv_idx;
  create index on mail_messages
  using gist(tsv tsvector_ops(siglen=248));

  select pg_size_pretty(pg_relation_size('mail_message_tsv_idx'));

  explain (analyze, costs off, timing off, summary off)
  select *
  from mail_message
  where tsv @@ to_tsquery('magic & value');
#+end_src

属性

我已经介绍了访问方法属性，其中大部分属性对所有操作符类都是一样的。但以下两个列级属性值得一提：

#+begin_src sql
  select p.name
  pg_index_column_has_property('mail_messages_tsv_idx', 1, p.name)
  from unnest(array[
      'returnable', 'distance_orderable'
      ])p(name);
#+end_src

现在只扫描索引是不可能的，因为无法从其签名恢复原始值。在这种特殊情况下完全没问题：tsvector 值只用于搜索，而我们需
要检索文档本身。

tsvector_ops类的排序运算符也没有定义。

** 其他数据类型

我只考虑了两个最突出的例子。它们表明，尽管 GiST 方法是基于平衡树的，但由于不同运算符类中有不同的支持函数实现，它可
以用于各种数据类型。当我们谈论 GiST 索引时，我们必须始终指定操作符类，因为它对索引属性至关重要。

以下是 GiST 访问方法目前支持的其他几种数据类型。

1. 几何数据类型
   除了点之外，GiST 还可以索引其他几何对象：矩形、圆形、多边形。所有这些对象都可以用它们的边界框来表示。
   cube扩展添加了表示多维立方体的同名数据类型。它们使用带有相应维度边框的 R 树进行索引。
2. 范围类型
   PostgreSQL 提供了几种内置的数字和时间范围类型，如 int4range 和 tstzrange。自定义范围类型可使用 CREATE
   TYPE AS RANGE 命令来定义。
   GiST 通过 range_ops 运算符类支持任何范围类型，包括标准范围和自定义范围。对于索引，采用的是一维 R 树：在这种
   情况下，边界框会转换为边界段。
   也支持多范围类型；它们依赖于 multirange_ops 类。边界范围包括作为多范围值一部分的所有范围。
   seg 扩展为区间提供了同名数据类型，其边界定义特别精确。它不被视为区间类型，但实际上是，因此它的索引方式与区间类
   型完全相同。
3. 序数类型
   让我们再次回顾一下 btree_gist 扩展：它为 GiST 方法提供了操作符类，以支持各种序数数据类型，这些数据类型通常由
   B 树建立索引。当其中一列的数据类型不受 B 树支持时，此类操作符类可用于建立多列索引。
4. 网络地址类型
   inet 数据类型内置 GiST 支持，可通过 inet_ops 操作符类实现。
5. 整型数组
   intarray 扩展扩展了整数数组的功能，为其添加了 GiST 支持。有两类操作符。对于小数组，你可以使用 gist__int_ops
   它实现了 RD 树，在索引项中完整地表示了键。对于大型数组，可以使用基于 gist__bigint_ops 运算符类的 RD-tree
   来实现更紧凑但不太精确的签名。
   #+begin_comment
      运算符类名称中多余的下划线属于基本类型数组的名称。例如，除了更常用的 int4[] 符号外，整数数组还可以表示为
      _int4。但没有 _int 和 _bigint 类型。
   #+end_comment
6. Ltree
   ltree 扩展为带有标签的树状结构添加了同名数据类型。GiST 支持通过签名 RD-trees 提供，RD-trees 使用
   gist_ltree_ops 运算符类处理 ltree 值，使用 gist__ltree_ops 运算符类处理 ltree 类型的数组。
7. 键值对存储
   hstore 扩展提供了用于存储键值对的 hstore 数据类型。gist_hstore_ops 运算符类基于签名 RD 树实现了索引支持。
8. Trigrams
   pg_trgm 扩展添加了 gist_trgm_ops 类，该类实现了对比较文本字符串和通配符搜索的索引支持。

* GIN

** 简介
根据其作者的解释，GIN 代表的是一种烈性和不畏艰险的精神，而不是一种酒精饮料。但也有一种正式的解释：这个缩写被扩展为
"Generalized Inverted Index"。

GIN 访问方法专为表示由独立元素组成的非原子值的数据类型而设计（例如，在全文检索中，文档由词条组成）。GiST 将值作为
一个整体进行索引，而 GIN 则不同，它只对其元素进行索引；每个元素都会映射到包含它的所有值。

我们可以把这种方法比作一本书的索引，它包括所有重要的术语，并列出所有提到这些术语的页面。为了方便使用，索引必须按字
母顺序排列，否则就无法快速浏览。类似地，GIN 依靠的是复合值的所有元素都可以排序这一事实；它的主要数据结构是 B 树。

GIN 元素树的实现没有普通 B 树那么复杂：它被设计为包含多次重复的小元素集。

这一假设得出了两个重要结论：
+ 一个元素只能在索引中存储一次。
  每个元素都被映射到一个 TID 列表中，该列表被称为 "posting list"。如果列表较短，则会与元素一起存储；较长的列表
  会被移到单独的过posting tree中，posting tree实际上是一棵 B 树。就像元素树一样，发布列表也是排序的；从用
  户的角度来看，这并不重要，但有助于加快数据访问速度和减少索引大小。

+ 删除树上的元素毫无意义。
  即使某个元素的 TID 列表是空的，同一元素也有可能作为其他值的一部分再次出现。

因此，索引是一棵元素树，其叶条目与平面列表或 TID 树绑定。

与 GiST 和 SP-GiST 访问方法一样，GIN 也可以通过操作符类的简化接口来索引各种数据类型。此类操作符通常会检查索引的
复合值是否与特定元素集相匹配（就像 @@ 操作符会检查文档是否满足全文搜索查询一样）。

要为特定数据类型建立索引，GIN 方法必须能够将复合值分割成元素，对这些元素进行排序，并检查找到的值是否满足查询要求。
这些操作由操作符类的支持函数实现。

** 全文搜索索引

GIN 主要用于加速全文检索，因此我将继续举例说明 GiST 索引。 正如你所猜测的，这里的复合值就是文档，而这些值的元素就
是词条。

让我们在 "Old MacDonald "表上建立一个 GIN 索引：

#+begin_src sql
  create index ts_gin_dx on ts using gin(doc_tsv);
#+end_src

该指数的可能结构如下所示。与前面的插图不同，这里我提供了实际的 TID 值（灰色背景显示），因为它们对于理解算法非常重
要。这些值表明堆元组具有以下 ID：

#+begin_src sql
  select ctid, * from ts;
#+end_src

[[./images/tLm8IB.png]]

请注意这里与普通 B 树索引的一些不同之处。B 树内部节点最左边的键是空的，因为它们实际上是多余的；而在 GIN 索引中，
它们根本不会被存储。因此，对子节点的引用也会被转移。两种索引都使用高键，但在 GIN 索引中，高键位于其合法的最右侧位
置。B 树中的同级节点被绑定到一个双向列表中；而 GIN 使用的是单向列表，因为树的遍历始终只有一个方向。

在这个理论性的例子中，除了 "farm "这个词素外，所有的posting list都适合常规页面。这个词素在多达六个文档中出现，
因此其 ID 被移到了一个单独的树中。

页面布局

GIN 页面布局与 B 树非常相似。我们可以使用 pageinspect 扩展来窥探索引。让我们在存储 pgsql-hackers 邮件列表电
子邮件的表上创建一个 GIN 索引：

#+begin_src sql
  create index mail_gin_idx on mail_messages using gin(tsv);
#+end_src

零页（元页）包含基本统计数据，如元素和其他类型页面的数量：

#+begin_src sql
  select * from gin_metapage_info(get_raw_page('mail_gin_idx', 0));
#+end_src

GIN 使用索引页的特殊空间；例如，该空间存储定义页类型的位：

#+begin_src sql
  select flags, count(*)
  from generate_series(0, 22956) as p,
  gin_page_opaque_info(get_raw_page('mail_gin_idx', p))
  group by flags
  order by 2;
#+end_src

带有元属性的页面当然是元页面。带 data 属性的页面属于posting list，而不带 data 属性的页面则与元素树有关。叶子
页面带有 leaf 属性。

在下一个示例中，另一个 pageinspect 函数会返回存储在树的叶子页中的 TID 信息。这样一棵树的每个条目实际上都是一个
小的 TID 列表，而不是单个 TID：

#+begin_src sql
  select left(tids::text, 60) || '...' tids
  from gin_leafpage_items(get_raw_page('mail_gin_idx', 24));
#+end_src

posting list是有序的，因此可以对其进行压缩（这也是同名属性的由来）。它们存储的不是 6 字节的 TID，而是与前一个
值的差值，差值用可变的字节数表示：1 差值越小，数据占用的空间就越小。

操作类

下面是 GIN 运算符类的支持函数列表：

#+begin_src sql
  select amprocnum, amproc::regproc
  from pg_am am
  join pg_opclass opc on opcmethod=am.oid
  join pg_amproc amop on amprocfamily=opfamily
  where amname='gin'
  and opcname='tsvector_ops'
  order by amprocnum;
#+end_src

第一个支持函数比较两个元素（本例中为两个词素）。如果词条是由 B-tree 支持的常规 SQL 类型表示的，那么 GIN 会自动
使用 B-tree 运算符类中定义的比较运算符。

第五个（可选）函数用于部分搜索，检查索引元素是否与搜索关键字部分匹配。在这种特殊情况下，部分搜索包括通过前缀搜索词
目。例如，查询 "c: *"对应于所有以字母 "c "开头的词目。

第二个函数从文档中提取词条，而第三个函数则从搜索查询中提取词条。使用不同的函数是合理的，因为至少文档和查询是由不同
的数据类型表示的，即 tsvector 和 tsquery。此外，搜索查询的函数决定了搜索的执行方式。如果查询要求文档包含特定词
素，那么搜索将仅限于至少包含查询中指定的一个词素的文档。如果没有这样的条件（例如，如果您需要不包含特定词素的文档），
则必须扫描所有文档--当然，这样做的成本要高得多。

#+begin_comment
 如果查询包含任何其他搜索关键字，则首先按这些关键字扫描索引，然后重新检查这些中间结果。因此，无需全面扫描索引。
#+end_comment

第四和第六个函数是一致性函数，用于确定找到的文档是否满足搜索查询。作为输入，第四个函数获取查询中指定的词目在文档中
出现的确切信息。第六个函数是在不确定的情况下运行的，可以在尚不清楚某些词目是否出现在文档中时调用。操作符类不必同时
实现这两个功能：只需提供其中一个即可，但在这种情况下搜索效率可能会受到影响。

tsvector_ops 运算符类只支持一种与搜索查询匹配文档的运算符： @@，它也包含在 GiST 运算符类中。

搜索

让我们来看看 "everywhere | oink "查询的搜索算法，其中两个词项由 OR 运算符连接。首先，支持函数从 tsquery 类型
的搜索字符串中提取词条 "everywher "和 "oink"（搜索键）。

由于查询要求包含特定的词目，因此至少包含一个查询关键字的文档的 TID 会被绑定到一个列表中。为此，与每个搜索关键字相
对应的 TID 会在词条树中被搜索，并被添加到一个公共列表中。索引中存储的所有 TID 都是有序的，因此可以将多个有序的
TID 流合并为一个索引。

请注意，此时键是否通过 AND、OR 或任何其他运算符组合并不重要：搜索引擎处理的是键列表，对搜索查询语义一无所知。

[[./images/T7HZte.png]]

找到的每个与文档相对应的 TID 都要经过一致性函数的检查。正是该功能解释了搜索查询，并只留下满足查询的 TID（或至少可
能满足查询的 TID，且必须由表格重新检查）。

在这种特殊情况下，一致性函数会保留所有 TID：

| TID   | "everywher" | "oink" | consistency function |
|-------+-------------+--------+----------------------|
| (0,4) | ✓           | -      | ✓                    |
| (1,4) | ✓           | -      | ✓                    |
| (2,3) | -           | ✓      | ✓                    |
| (2,4) | ✓           | ✓      | ✓                    |
|-------+-------------+--------+----------------------|

搜索查询可以包含前缀，而不是普通词素。如果应用程序用户在搜索字段中输入单词的首字母，并希望立即得到搜索结果，那么前
缀就非常有用。例如，"pig： *查询将匹配所有包含以 "pig "开头的词素的文档：在这里，我们得到了 "pigs"，如果老麦克
唐纳在他的农场里饲养了鸽子，我们也会得到 "pigeons"。

这种部分搜索使用一个特殊的支持函数将索引词目与搜索关键字进行匹配；除了前缀匹配外，该函数还可以实现其他部分搜索逻辑。

常用词和罕见词

如果搜索的词条在文档中多次出现，创建的 TID 列表就会变得很长，这当然是低效的。幸运的是，如果查询中也包含一些罕见词
目，这种情况通常可以避免。

让我们来看看 "farm & cluck "查询。cluck "词素出现了两次，而 "farm "词素则出现了六次。我们没有将这两个词素同等
对待，也没有根据它们建立完整的 TID 列表，而是将罕见的 "cluck "词素视为必选词素，而将出现频率较高的 "farm "词素
视为可选词素，因为很明显（考虑到查询语义），带有 "farm "词素的文档只有同时包含 "cluck "词素才能满足查询。

因此，我们通过索引扫描确定了第一个包含 "cluck "的文档，其 TID 为 (1,3)。然后，我们必须找出该文档是否也包含
"farm "词素，但可以跳过所有 TID 小于 (1,3) 的文档。由于频繁出现的词素很可能与许多 TID 相对应，它们很有可能被存
储在单独的树中，因此有些页面也可以跳过。在本例中，在 "farm "词条树中的搜索从 (1,3) 开始。

随后的强制词素值将重复这一过程。

显然，这种优化也可以应用于涉及两个以上词性的更复杂搜索场景。该算法按词目出现频率的顺序排列词目，并将其逐一添加到必
选词目列表中，当剩余词目无法保证文档满足查询要求时，算法就会停止。

例如，我们来看看查询 "farm & ( cluck | chick )"。出现频率最低的词素是 "chick"；它被立即添加到必选词素列表中。
为了检查其他词素是否可以被视为可选词素，一致性函数对必选词素取 false，对所有其他词素取 true。函数返回 true AND
(true OR false) = true，这意味着其余词素是 "自给自足 "的，它们中至少有一个必须成为必选词素。

下一个出现频率最低的词素（"cluck"）被添加到列表中，现在一致性函数返回 true AND (false OR false) = false。
因此，"chick "和 "cluck "词素成为必选词素，而 "farm "仍然是可选词素。

[[./images/rc63ga.png]]


posting list 的长度为三，因为必填词目出现了三次：

| TID   | "chick" | "cluck" | "farm" | consistency function |
|-------+---------+---------+--------+----------------------|
| (1,2) | ✓       | -       | ✓      | ✓                    |
| (1,3) | -       | ✓       | -      | -                    |
| (1,4) | -       | ✓       | -      | -                    |
|-------+---------+---------+--------+----------------------|

因此，如果知道词素频率，就可以用最有效的方式合并词素树，从罕见词素开始，跳过那些肯定是多余的频繁词素的页面范围。这
样可以减少调用一致性函数的次数。

为了确保这种优化确实有效，让我们查询一下 pgsql-hackers 档案。我们需要指定两个词目，一个常见词目和一个罕见词目：

#+begin_src sql
  select word, ndoc
  from ts_stat('select tsv frm mail_messages')
  where word in ('wrote', 'tattoo');
#+end_src

事实证明，确实存在一份同时包含这两份内容的文件：

#+begin_src sql
  \timing on
  select count(*) from mail_messages
  where tsv @@ to_tsquery('wrote & tattoo');
#+end_src

该查询的执行速度几乎与搜索单词 "tattoo "一样快：

#+begin_src sql
  select count(*) from mail_messages
  where tsv @@ to_tsquery('tatoo')'
#+end_src

但是，如果我们要找的是 "wrote "这个单词，搜索时间会更长：

#+begin_src sql
  select count(*) from mail_messages
  where tsv @@ to_tsquery('wrote');
#+end_src

插入

GIN 索引不能包含重复内容；如果要添加的元素已经存在于索引中，其 TID 只需添加到已存在元素的posting list或树状
结构中即可。posting list是索引条目中的一部分，它不能占用页面中过多的空间，因此如果超出了分配的空间，列表就会变
成树状。

当一个新元素（或新 TID）被添加到树中时，可能会出现页面溢出；在这种情况下，页面会被一分为二，元素会在它们之间重新
分配。

但是，每个文档通常都包含许多需要索引的词目。因此，即使我们只创建或修改了一个文档，索引树仍然会经历大量的修改。这
就是 GIN 更新相当缓慢的原因。

下图显示了在表中插入 TID 为 (4,1) 的 "Everywhere clucks, moos, and oinks "行后树的状态。扩展了 "cluk"、
"moo "和 "oink "的词条列表；"everywher "的词条列表超过了最大值，被分割成一棵独立的树。

不过，如果一次更新索引以纳入与多个文档相关的更改，那么与连续更改相比，总工作量可能会减少，因为这些文档可能包含一些
共同词目。

这一优化由 fastupdate 存储参数控制。延迟的索引更新会累积到一个无序的待定列表中，该列表实际存储在元素树之外的单独
列表页中。该列表的最大大小由 4MB gin_pending_list_limit 参数或同名索引存储参数确定。

[[./images/vqGXad.png]]

默认情况下，这种延迟更新是启用的，但你应该记住，它们会减慢搜索速度：除了树本身，还必须扫描整个无序词条列表。此外，
插入时间的可预测性也会降低，因为任何变化都可能导致溢出，从而引发昂贵的合并过程。在索引抽真空过程中，合并也可以异步
进行，这在一定程度上缩短了合并时间。

创建新索引 时，元素也是分批添加的，而不是逐个添加，因为这样速度太慢。所有更改不会被保存到磁盘上的无序列表中，而是累
积到一个 64MB 的 maintenance_work_mem 内存块中，一旦该内存块没有可用空间，就会被转移到索引中。为这一操作分配的
内存越多，建立索引的速度就越快。

本章提供的示例证明，就搜索精度而言，GIN 优于 GiST 签名树。因此，GIN 通常被用于全文检索。不过，如果数据更新频繁，
GIN 更新速度慢的问题可能会使 GiST 更受青睐。

限制结果集大小

GIN 访问方法总是以位图的形式返回结果，不可能逐个获取 TID。换句话说，支持 BITMAP SCAN 属性，但不支持 INDEX SCAN
属性。

造成这种限制的原因是延迟更新的无序列表。在索引访问的情况下，该列表会被扫描以建立一个位图，然后用树的数据更新该位图。
如果在搜索过程中无序列表与树合并（作为索引更新的结果或在抽真空过程中），一个相同的值可能会返回两次，这是不可接受的。
但对于位图来说，这不会造成任何问题：相同的位只会被设置两次。

因此，使用 LIMIT 子句和 GIN 索引的效率并不高，因为位图仍需全部建立，这在总成本中占了相当大的比例：

#+begin_src sql
  expalin select * from mail_messages
  where tsv @@ to_tsquery('hacker')
  limit 1000;
#+end_src

因此，GIN 方法提供了一个特殊功能，可以限制索引扫描所返回结果的数量。该限制由 gin_fuzzy_search_limit 参数施加，
默认情况下该参数处于关闭状态。如果启用该参数，索引访问方法将随机跳过某些值，以获得大致指定的行数（因此被称为 "模
糊"）：

#+begin_src sql
  set gin_fuzzy_search_limit=1000;
  select count(*)
  from mail_messages
  where tsv @@ to_tsquery('hacker');
#+end_src

#+begin_src sql
  select count(*)
  from mail_messages
  where tsv @@ to_tsquery('hacker');
#+end_src


#+begin_src sql
  reset gin_fuzzy_search_limig;
#+end_src

请注意，这些查询中没有 LIMIT 子句。这是使用索引扫描和堆扫描获取不同数据的唯一合法方式。规划器对 GIN 索引的这种行
为一无所知，因此在估算成本时不会考虑该参数值。

属性

gin 访问方法的所有属性在所有级别上都是相同的，它们并不依赖于特定的操作符类别。

访问方法属性

#+begin_src sql
  select a.amname, p.name, pg_indexam_has_property(a.oid, p.name)
  from pg_am a, unnest(array[
  'can_order', 'can_unique', 'can_multi_col',
  'can_exclude', 'can_include'
  ])p(name)
  where a.amname='gin';
#+end_src

GIN 既不支持排序，也不支持唯一约束。

支持多列索引，但值得一提的是，其列的顺序并不重要。与普通的 B 树不同，多列 GIN 索引不存储复合键，而是用相应的列编
号扩展独立的元素。

由于 INDEX SCAN 属性不可用，因此无法支持排除限制。

GIN 不支持额外的 INCLUDE 列。这类列在这里没有太大意义，因为几乎不可能使用 GIN 索引作为覆盖：它只包含索引值的单独
元素，而索引值本身存储在表中。

索引级属性

#+begin_src sql
  select p.name, pg_index_has_property('mail_gin_idx', p.name)
  from unnest(array[
  'clusterable', 'index_scan', 'bitmap_scan', 'backwoard_scan'
  ])p(name);
#+end_src

不支持逐个获取结果：索引访问总是返回一个位图。

出于同样的原因，使用 GIN 索引对表重新排序也是没有意义的：位图总是与表中数据的物理布局相对应，不管它是什么。

不支持后向扫描：该功能适用于常规索引扫描，不适用于位图扫描。

列级属性

#+begin_src sql
  select p.name,
  pg_index_column_has_property('mail_gin_idx', 1, p.name)
  from unnest(array[
  'orderable', 'search_array', 'search_nulls',
  'returnable', 'distance_orderable'
  ])p(name);
#+end_src

列级属性都不可用：既不能排序（原因显而易见），也不能使用索引作为覆盖（因为文档本身并不存储在索引中）。也不支持 NULL
（对于非原子类型的元素没有意义）。

GIN的限制和RUM指数

尽管 GIN 功能强大，但它仍无法解决全文检索的所有难题。尽管tsvector 类型确实能指示词目位置，但这些信息并不能进入
索引。因此，GIN 无法用于加快短语搜索，因为短语搜索需要考虑词素的邻近性。此外，搜索引擎通常按照相关性（不管这个词是
什么意思）来返回结果，由于 GIN 不支持排序运算符，因此唯一的解决方案就是为每一行结果计算排序函数，这当然会非常慢。

RUM 访问方法（它的名字让我们怀疑开发人员对 GIN 真正含义的诚意）解决了这些缺点。

这种访问方法是作为扩展提供的；您可以从 PGDG 软件库中下载相应的软件包，或者获取源代码本身。

RUM 基于 GIN，但两者有两个主要区别。首先，RUM 不提供延迟更新，因此除了位图扫描外，它还支持常规的索引扫描，并实现
了排序操作符。其次，RUM 索引键可以扩展附加信息。这一功能在某种程度上类似于 INCLUDE 列，但在这里，附加信息被绑定
到特定的键上。在全文搜索中，RUM 运算符类可将词素出现映射到其在文档中的位置，从而加快短语搜索和结果排序。

这种方法的缺点是更新速度较慢，索引规模较大。此外，由于RUM访问方法是作为扩展提供的，它依赖于通用 WAL 机制，这比
内置日志更慢，而且会产生更大的 WAL。

** Trigrams

pg_trgm 扩展可以通过比较重合的三个字母序列（trigrams）的数量来评估词语的相似性。单词相似性可与全文搜索一起使用，
即使要搜索的单词在输入时有错别字，也能返回一些结果。

gin_trgm_ops 运算符类实现了文本字符串索引。为了找出文本值中的元素，它提取各种三字母子串，而不是单词或词目（只考
虑字母和数字，其他字符忽略不计）。在索引中，三字符串表示为整数。需要注意的是，对于UTF-8 编码中需要 2 至 4 个字节
的非拉丁字符，这种表示法无法解码原始符号。

#+begin_src sql
  create extension pg_trgm;
  select unnest(show_trgm('macdonald')),
         unnest(show_trgm('McDonald'));
#+end_src

该类支持精确和模糊比较字符串和单词的操作符。

#+begin_src sql
  select amopopr::regoperator, oprcode::regproc
  from pg_am am
  join pg_opclass opc on opcmethod=am.oid
  join pg_amop amop on amopfamily=opcfamily
  join pg_operator opr on opr.oid=amopopr
  where amname='gin'
  and opcname='gin_trgm_ops'
  order by amopstrategy;
#+end_src

为了进行模糊比较，我们可以将字符串之间的距离定义为共同卦数与查询字符串中卦数之比。但如前所述，GIN 不支持排序操作
符，因此该类中的所有操作符都必须是布尔型的。因此，对于执行模糊比较策略的 %、%> 和 %>> 操作符，如果计算出的距离不
超过定义的阈值，一致性函数就会返回 true。

对于 = 和 LIKE 操作符，一致性函数要求值包含查询字符串中的所有三元组。根据正则表达式匹配文档需要进行更复杂的检查。

无论如何，trigram搜索总是模糊的，搜索结果必须重新检查。

**  索引数组

GIN 还支持数组数据类型。通过在数组元素上建立 GIN 索引，可以快速确定一个数组是否与另一个数组重叠或包含在另一个数组
中：

#+begin_src sql
  select amopopr::regoperator, oprcode::regproc, amopstrategy
  from pg_am am
  join pg_opclass opc on opcmethod=am.oid
  join pg_amop amop on amopfamily=opcfamily
  join pg_operator opr on opr.oid=amopopr
  where amname='gin'
  and opcname='array_ops'
  order by amopstrategy;
#+end_src

以演示数据库中显示航班信息的路线视图为例。Days_of_week 列是一个数组，包含一周中执行航班的天数。要建立索引，我们首
先要将视图实体化：

#+begin_src sql
  create table routes_tbl as select * from routes;
  create index on routes_tbl using gin(days_of_week);
#+end_src

让我们使用创建的索引来选择在周二、周四和周日起飞的航班。我关闭了顺序扫描；否则，计划器不会为这样一个小表使用索引：

#+begin_src sql
  set enable_seqscan=off;
  explain (costs off) select * from routes_tbl
  where days_of_week=array[2,4,7];
#+end_src

原来，这样的航班共有 11 次：

#+begin_src sql
  select flight_no, departure_airport, arrival_aiport,
  days_of_week
  from routes_tbl
  where days_of_week = array[2,4,7];
#+end_src

建立的索引只包含七个元素：从 1 到 7 的整数，代表一周的天数。

查询的执行与之前展示的全文检索非常相似。在这种特殊情况下，搜索查询由普通数组而非特殊数据类型表示；我们假定索引数组
必须包含所有指定元素。这里的一个重要区别是，相等条件还要求索引数组不包含其他元素。一致性函数通过策略编号知道了这一
要求，但它无法验证是否存在不需要的元素，因此它要求索引引擎通过表格重新检查结果：

#+begin_src sql
  explain (analyze, costs off, timing off, summary off)
  select * from routes_tbl
  where days_of_week=array[2,4,7];
#+end_src

使用附加列扩展 GIN 索引可能很有用。例如，为了搜索每周二、周四和周日从莫斯科起飞的航班，索引缺少 departure_city
列。但常规标量数据类型没有操作符类：

#+begin_src sql
  create index on routes_tbl using gin(days_of_week, departure_city);
#+end_src

这种情况可以通过 btree_gin 扩展来解决。它添加了 GIN 运算符类，通过将标量值表示为包含单个元素的复合值来模拟常规的
B 树处理。

#+begin_src sql
  create extension btree_gin;
  create index on routes_tbl using gin(days_of_week, departure_city);
  explain (costs off)
  select * from routes_tbl
  where days_of_week = array[2,4,7]
  and departure_city='Moscow';
  reset enable_seqscan;
#+end_src

关于 btree_gist 的评论也适用于 btree_gin：在进行比较操作时，B 树的效率要高得多，因此只有在真正需要 GIN 索引时，
才有必要使用 btree_gin 扩展。例如，通过小于或小于等于条件进行的搜索可以在 B 树中通过后向扫描执行，但在 GIN 中则
不行。


** 索引JSON

jsonb_ops 操作符类是默认的操作符类。原始 JSON 文档中的所有键、值和数组元素都会转换成索引项。它能加快检查是否包含
JSON 值 (@>)、是否存在键 (?、?| 和 ?&) 或是否与 JSON 路径匹配 (@? 和 @@) 的查询速度：

#+begin_src sql
  select amopopr::regoperator, oprcode::regproc, amopstrategy
  from pg_am am
  join pg_opclass opc on opcmethod=am.oid
  join pg_amop amop on amopfamily=opcfamily
  join pg_operator opr on opr.oid=amopopr
  where amname='gin'
  and opcname='jsonb_ops'
  order by amopstrategy;
#+end_src

让我们把路由视图中的几行转换成 JSON 格式：

#+begin_src sql
  create table routes_jsonb as
  select to_jsonb(t) route
  from (
  select departure_aiport_name, arrival_airport_name, days_of_week
  from routes
  order by flight_no
  limit 4
  );
  select ctid, jsonb_pretty(route) from routes_jsonb;
#+end_src

#+begin_src sql
  create index on routes_jsonb using gin(route);
#+end_src

创建的索引如下所示：

[[./images/AWQKPK.png]]

让我们考虑一个带条件路由 @> '{"days_of_week"： [6]}'，它会选择包含指定路径（即周六执行的航班）的 JSON 文档。

支持函数从搜索查询的 JSON 值中提取搜索键： 支持函数从搜索查询的值中提取搜索关键字："days_of_week "和 "6"。
对于包含策略，该函数要求所有搜索键都可用，但结果仍需由表格重新检查：从索引的角度来看，指定路径也可以对应于类似
{"days_of_week"： [2], "foo"： [6]}.

jsonb_path_ops操作类

第二个类名为 jsonb_path_ops，包含的操作符较少：

#+begin_src sql
  select amopopr::regoperator, oprcode::regproc, amopstrategy
  from pg_am am
  join pg_opclass opc on opcmethod=am.oid
  join pg_amop amop on amopfamily=opcfamily
  join pg_operator opr on opr.oid=amopopr
  where amname='gin'
  and opcname='jsonb_path_ops'
  order by amopstrategy;
#+end_src

如果使用该类，索引将包含从文档根指向所有值和所有数组元素的路径，而不是孤立的 JSON 片段。这将使搜索更精确、更高效，
但对于使用独立键而非路径表示的参数的操作，速度不会提高。

由于路径可能相当长，因此真正被索引的不是路径本身，而是其哈希值。

让我们使用该操作符类为同一个表创建一个索引：

#+begin_src sql
  create index on routes_jsonb using gin(route jsonb_path_ops);
#+end_src

创建的索引可以用以下树形结构表示：

[[./images/iE6aBN.png]]


当执行具有相同条件路由 @> '{"days_of_week"： [6]}"时，支持函数提取的是整个路径 "days_of_week, 6"，而不是其
单独的组成部分。这样就能立即在元素树中找到两个匹配文档的 TID。

显然，这些条目将由一致性函数进行检查，然后由索引引擎重新检查（例如，排除哈希碰撞）。但是，通过树进行搜索的效率要高
得多，因此，如果 jsonb_path_ops 类的运算符提供的索引支持足以满足查询的需要，那么选择 jsonb_path_ops 类是有道
理的。


** 索引其他数据类型

还通过扩展为以下数据类型提供 GIN 支持：

+ 整型数组intarray 扩展为整数数组添加了 gin__int_ops 操作符类。它与标准的 array_ops 运算符类非常相似，但它支持
  匹配运算符 @@，可将文档与搜索查询进行匹配。
+ 键值存储 hstore 扩展实现了键值对的存储，并提供了 gin_hstore_ops 运算符类。键和值都有索引。
+ JSON查询语言 外部 jsquery 扩展为 JSON 提供了自己的查询语言和 GIN 索引支持。

#+begin_comment
 在采用 SQL:2016 标准并在 PostgreSQL 中实现 SQL/JSON 查询语言后，标准内置功能似乎是更好的选择。
#+end_comment





